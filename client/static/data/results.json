{
	"basic": {
		"repos_name": "ossf/scorecard",
		"license": "Apache License 2.0",
		"language": "GoLang, Makefile"
	},
	"contributors": {
		"num_of_contributor": 59,
		"contributors": [
			{
				"contributor_name": "naveensrinivasan",
				"historical_contributions": 275
			},
			{
				"1.contributor_name": "azeemshaikh38",
				"2.historical_contributions": 223
			},
			{
				"1.contributor_name": "dependabot[bot]",
				"2.historical_contributions": 215
			},
			{
				"1.contributor_name": "laurentsimon",
				"2.historical_contributions": 198
			},
			{
				"1.contributor_name": "inferno-chromium",
				"2.historical_contributions": 93
			},
			{
				"1.contributor_name": "chrismcgehee",
				"2.historical_contributions": 58
			},
			{
				"1.contributor_name": "dlorenc",
				"2.historical_contributions": 49
			},
			{
				"1.contributor_name": "kimsterv",
				"2.historical_contributions": 18
			},
			{
				"1.contributor_name": "david-a-wheeler",
				"2.historical_contributions": 13
			},
			{
				"1.contributor_name": "evverx",
				"2.historical_contributions": 13
			},
			{
				"1.contributor_name": "olivekl",
				"2.historical_contributions": 12
			},
			{
				"1.contributor_name": "asraa",
				"2.historical_contributions": 12
			},
			{
				"1.contributor_name": "justaugustus",
				"2.historical_contributions": 11
			},
			{
				"1.contributor_name": "oliverchang",
				"2.historical_contributions": 7
			},
			{
				"1.contributor_name": "azeemsgoogle",
				"2.historical_contributions": 6
			},
			{
				"1.contributor_name": "moorereason",
				"2.historical_contributions": 6
			},
			{
				"1.contributor_name": "cpanato",
				"2.historical_contributions": 5
			},
			{
				"1.contributor_name": "nanikjava",
				"2.historical_contributions": 4
			},
			{
				"1.contributor_name": "nathannaveen",
				"2.historical_contributions": 4
			},
			{
				"1.contributor_name": "loosebazooka",
				"2.historical_contributions": 3
			},
			{
				"1.contributor_name": "developer-guy",
				"2.historical_contributions": 3
			},
			{
				"1.contributor_name": "r0mdau",
				"2.historical_contributions": 3
			},
			{
				"1.contributor_name": "neilnaveen",
				"2.historical_contributions": 3
			},
			{
				"1.contributor_name": "raboof",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "chair6",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "tom-vanbraband-sonarsource",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "varunsh-coder",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "notanton",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "behnazh-w",
				"2.historical_contributions": 2
			},
			{
				"1.contributor_name": "dota17",
				"2.historical_contributions": 2
			}
		]
	},
	"dependency": {
		"num_of_dependency": 1020,
		"dependencies": [
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go",
				"3.dependency_version": "0.100.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/bigquery",
				"3.dependency_version": "1.30.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/compute",
				"3.dependency_version": "1.5.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/iam",
				"3.dependency_version": "0.3.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/monitoring",
				"3.dependency_version": "1.4.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/pubsub",
				"3.dependency_version": "1.19.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/storage",
				"3.dependency_version": "1.21.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/trace",
				"3.dependency_version": "1.2.0"
			},
			{
				"1.dependency_name": "/census-ecosystem/opencensus-go-exporter-stackdriver",
				"2.dependency_token": "contrib.go.opencensus.io/exporter/stackdriver",
				"3.dependency_version": "0.13.11"
			},
			{
				"1.dependency_name": "/acomagu/bufpipe",
				"2.dependency_token": "github.com/acomagu/bufpipe",
				"3.dependency_version": "1.0.3"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go",
				"2.dependency_token": "github.com/aws/aws-sdk-go",
				"3.dependency_version": "1.43.31"
			},
			{
				"1.dependency_name": "/bombsimon/logrusr",
				"2.dependency_token": "github.com/bombsimon/logrusr/v2",
				"3.dependency_version": "2.0.1"
			},
			{
				"1.dependency_name": "/bradleyfalzon/ghinstallation",
				"2.dependency_token": "github.com/bradleyfalzon/ghinstallation/v2",
				"3.dependency_version": "2.0.4"
			},
			{
				"1.dependency_name": "/caarlos0/env",
				"2.dependency_token": "github.com/caarlos0/env/v6",
				"3.dependency_version": "6.9.1"
			},
			{
				"1.dependency_name": "/census-instrumentation/opencensus-proto",
				"2.dependency_token": "github.com/census-instrumentation/opencensus-proto",
				"3.dependency_version": "0.3.0"
			},
			{
				"1.dependency_name": "/containerd/stargz-snapshotter",
				"2.dependency_token": "github.com/containerd/stargz-snapshotter/estargz",
				"3.dependency_version": "0.0.0-20210622060536-734e95fb86be"
			},
			{
				"1.dependency_name": "/containerd/typeurl",
				"2.dependency_token": "github.com/containerd/typeurl",
				"3.dependency_version": "0.10.1"
			},
			{
				"1.dependency_name": "/docker/cli",
				"2.dependency_token": "github.com/docker/cli",
				"3.dependency_version": "1.0.2"
			},
			{
				"1.dependency_name": "/distribution/distribution",
				"2.dependency_token": "github.com/docker/distribution",
				"3.dependency_version": "20.10.12+incompatible"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go",
				"3.dependency_version": "2.7.1+incompatible"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/bigquery",
				"3.dependency_version": "v0.100.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/compute",
				"3.dependency_version": "v1.30.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/datacatalog",
				"3.dependency_version": "v1.5.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/iam",
				"3.dependency_version": "v1.3.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/monitoring",
				"3.dependency_version": "v0.3.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/pubsub",
				"3.dependency_version": "v1.4.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/storage",
				"3.dependency_version": "v1.19.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/trace",
				"3.dependency_version": "v1.21.0"
			},
			{
				"1.dependency_name": "/census-ecosystem/opencensus-go-exporter-stackdriver",
				"2.dependency_token": "contrib.go.opencensus.io/exporter/stackdriver",
				"3.dependency_version": "v1.2.0"
			},
			{
				"1.dependency_name": "/acomagu/bufpipe",
				"2.dependency_token": "github.com/acomagu/bufpipe",
				"3.dependency_version": "v0.13.11"
			},
			{
				"1.dependency_name": "/anmitsu/go-shlex",
				"2.dependency_token": "github.com/anmitsu/go-shlex",
				"3.dependency_version": "v1.0.3"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go",
				"2.dependency_token": "github.com/aws/aws-sdk-go",
				"3.dependency_version": "v0.0.0-20161002113705-648efa622239"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2",
				"3.dependency_version": "v0.0.0-20160902184237-e75332964ef5"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2/aws/protocol/eventstream",
				"3.dependency_version": "v1.43.31"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2/config",
				"3.dependency_version": "v1.16.2"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2/credentials",
				"3.dependency_version": "v1.4.1"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2/feature/ec2/imds",
				"3.dependency_version": "v1.15.3"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go-v2",
				"2.dependency_token": "github.com/aws/aws-sdk-go-v2/feature/s3/manager",
				"3.dependency_version": "v1.11.2"
			},
			{
				"1.dependency_name": "/leighmcculloch/gochecknoglobals",
				"2.dependency_token": "4d63.com/gochecknoglobals",
				"3.dependency_version": "v1.12.3"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go",
				"3.dependency_version": "v1.11.3"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/compute",
				"3.dependency_version": "0.1.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/iam",
				"3.dependency_version": "0.100.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/kms",
				"3.dependency_version": "1.4.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/storage",
				"3.dependency_version": "0.2.0"
			},
			{
				"1.dependency_name": "/acomagu/bufpipe",
				"2.dependency_token": "github.com/acomagu/bufpipe",
				"3.dependency_version": "1.1.0"
			},
			{
				"1.dependency_name": "/alecthomas/jsonschema",
				"2.dependency_token": "github.com/alecthomas/jsonschema",
				"3.dependency_version": "1.21.0"
			},
			{
				"1.dependency_name": "/AlekSi/pointer",
				"2.dependency_token": "github.com/AlekSi/pointer",
				"3.dependency_version": "0.15.1"
			},
			{
				"1.dependency_name": "/alessio/shellescape",
				"2.dependency_token": "github.com/alessio/shellescape",
				"3.dependency_version": "1.0.3"
			},
			{
				"1.dependency_name": "/alexkohler/prealloc",
				"2.dependency_token": "github.com/alexkohler/prealloc",
				"3.dependency_version": "0.0.0-20211209230136-e2b41affa5c1"
			},
			{
				"1.dependency_name": "/Antonboom/errname",
				"2.dependency_token": "github.com/Antonboom/errname",
				"3.dependency_version": "1.2.0"
			},
			{
				"1.dependency_name": "/Antonboom/nilnil",
				"2.dependency_token": "github.com/Antonboom/nilnil",
				"3.dependency_version": "1.4.1"
			},
			{
				"1.dependency_name": "/apex/log",
				"2.dependency_token": "github.com/apex/log",
				"3.dependency_version": "1.0.0"
			},
			{
				"1.dependency_name": "/asaskevich/govalidator",
				"2.dependency_token": "github.com/asaskevich/govalidator",
				"3.dependency_version": "0.1.5"
			},
			{
				"1.dependency_name": "/ashanbrown/forbidigo",
				"2.dependency_token": "github.com/ashanbrown/forbidigo",
				"3.dependency_version": "0.1.0"
			},
			{
				"1.dependency_name": "/ashanbrown/makezero",
				"2.dependency_token": "github.com/ashanbrown/makezero",
				"3.dependency_version": "1.9.0"
			},
			{
				"1.dependency_name": "/atc0005/go-teams-notify",
				"2.dependency_token": "github.com/atc0005/go-teams-notify/v2",
				"3.dependency_version": "0.0.0-20210307081110-f21760c49a8d"
			},
			{
				"1.dependency_name": "/aws/aws-sdk-go",
				"2.dependency_token": "github.com/aws/aws-sdk-go",
				"3.dependency_version": "1.3.0"
			},
			{
				"1.dependency_name": "/leighmcculloch/gochecknoglobals",
				"2.dependency_token": "4d63.com/gochecknoglobals",
				"3.dependency_version": "1.1.1"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go",
				"3.dependency_version": "2.6.1"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/compute",
				"3.dependency_version": "1.42.43"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/iam",
				"3.dependency_version": "v0.1.0"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/kms",
				"3.dependency_version": "v0.100.2"
			},
			{
				"1.dependency_name": "/googleapis/google-cloud-go",
				"2.dependency_token": "cloud.google.com/go/storage",
				"3.dependency_version": "v1.4.0"
			},
			{
				"1.dependency_name": "/acomagu/bufpipe",
				"2.dependency_token": "github.com/acomagu/bufpipe",
				"3.dependency_version": "v0.2.0"
			},
			{
				"1.dependency_name": "/alecthomas/jsonschema",
				"2.dependency_token": "github.com/alecthomas/jsonschema",
				"3.dependency_version": "v1.1.0"
			},
			{
				"1.dependency_name": "/AlekSi/pointer",
				"2.dependency_token": "github.com/AlekSi/pointer",
				"3.dependency_version": "v1.21.0"
			},
			{
				"1.dependency_name": "/alessio/shellescape",
				"2.dependency_token": "github.com/alessio/shellescape",
				"3.dependency_version": "v0.15.1"
			},
			{
				"1.dependency_name": "/alexkohler/prealloc",
				"2.dependency_token": "github.com/alexkohler/prealloc",
				"3.dependency_version": "v1.0.3"
			},
			{
				"1.dependency_name": "/anmitsu/go-shlex",
				"2.dependency_token": "github.com/anmitsu/go-shlex",
				"3.dependency_version": "v0.0.0-20211209230136-e2b41affa5c1"
			},
			{
				"1.dependency_name": "/Antonboom/errname",
				"2.dependency_token": "github.com/Antonboom/errname",
				"3.dependency_version": "v1.2.0"
			},
			{
				"1.dependency_name": "/Antonboom/nilnil",
				"2.dependency_token": "github.com/Antonboom/nilnil",
				"3.dependency_version": "v1.4.1"
			},
			{
				"1.dependency_name": "/apex/log",
				"2.dependency_token": "github.com/apex/log",
				"3.dependency_version": "v1.0.0"
			},
			{
				"1.dependency_name": "/asaskevich/govalidator",
				"2.dependency_token": "github.com/asaskevich/govalidator",
				"3.dependency_version": "v0.0.0-20161002113705-648efa622239"
			},
			{
				"1.dependency_name": "/ashanbrown/forbidigo",
				"2.dependency_token": "github.com/ashanbrown/forbidigo",
				"3.dependency_version": "v0.1.5"
			},
			{
				"1.dependency_name": "/ashanbrown/makezero",
				"2.dependency_token": "github.com/ashanbrown/makezero",
				"3.dependency_version": "v0.1.0"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "v1.9.0"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/analyze",
				"3.dependency_version": "v0.0.0-20160902184237-e75332964ef5"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/autobuild",
				"3.dependency_version": "v0.0.0-20210307081110-f21760c49a8d"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/init",
				"3.dependency_version": "v1.3.0"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "v1.1.1"
			},
			{
				"1.dependency_name": "/actions/cache",
				"2.dependency_token": "actions/cache",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/arduino/setup-protoc",
				"2.dependency_token": "arduino/setup-protoc",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "9b0655f430fba8c7001d4e38f8d4306db5c6e0ab"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "136d96b4aee02b1f0de3ba493b1d47135042d9c0"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/crazy-max/ghaction-import-gpg",
				"2.dependency_token": "crazy-max/ghaction-import-gpg",
				"3.dependency_version": "f6164bd8c8acb4a71fb2791a8b6c4024ff038dab"
			},
			{
				"1.dependency_name": "/goreleaser/goreleaser-action",
				"2.dependency_token": "goreleaser/goreleaser-action",
				"3.dependency_version": "64c0c85d18e984422218383b81c52f8b077404d3"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/actions/cache",
				"2.dependency_token": "actions/cache",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "f6164bd8c8acb4a71fb2791a8b6c4024ff038dab"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "4d58d49bfefed583addec96996588e8bc4b306b8"
			},
			{
				"1.dependency_name": "/codecov/codecov-action",
				"2.dependency_token": "codecov/codecov-action",
				"3.dependency_version": "b953231f81b8dfd023c58e0854a721e35037f28b"
			},
			{
				"1.dependency_name": "/nick-fields/retry",
				"2.dependency_token": "nick-invision/retry",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/peter-evans/create-or-update-comment",
				"2.dependency_token": "peter-evans/create-or-update-comment",
				"3.dependency_version": "136d96b4aee02b1f0de3ba493b1d47135042d9c0"
			},
			{
				"1.dependency_name": "/peter-evans/find-comment",
				"2.dependency_token": "peter-evans/find-comment",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "f6164bd8c8acb4a71fb2791a8b6c4024ff038dab"
			},
			{
				"1.dependency_name": "/actions/cache",
				"2.dependency_token": "actions/cache",
				"3.dependency_version": "e3c560433a6cc60aec8812599b7844a7b4fa0d71"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "7f8f3d9f0f62fe5925341be21c2e8314fd4f7c7c"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "c9fcb64660bc90ec1cc535646af190c992007c32"
			},
			{
				"1.dependency_name": "/arduino/setup-protoc",
				"2.dependency_token": "arduino/setup-protoc",
				"3.dependency_version": "1769778a0c5bd330272d749d12c036d65e70d39d"
			},
			{
				"1.dependency_name": "/codecov/codecov-action",
				"2.dependency_token": "codecov/codecov-action",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/nick-fields/retry",
				"2.dependency_token": "nick-invision/retry",
				"3.dependency_version": "136d96b4aee02b1f0de3ba493b1d47135042d9c0"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/peter-evans/slash-command-dispatch",
				"2.dependency_token": "peter-evans/slash-command-dispatch",
				"3.dependency_version": "f6164bd8c8acb4a71fb2791a8b6c4024ff038dab"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "64c0c85d18e984422218383b81c52f8b077404d3"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "e3c560433a6cc60aec8812599b7844a7b4fa0d71"
			},
			{
				"1.dependency_name": "/actions/upload-artifact",
				"2.dependency_token": "actions/upload-artifact",
				"3.dependency_version": "7f8f3d9f0f62fe5925341be21c2e8314fd4f7c7c"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/upload-sarif",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/ossf/scorecard-action",
				"2.dependency_token": "ossf/scorecard-action",
				"3.dependency_version": "2afb49dbaafaba8005860648bf7fc178637aca0d"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/actions/stale",
				"2.dependency_token": "actions/stale",
				"3.dependency_version": "a12a3943b4bdde767164f792f33f40b04645d846"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "6673cd052c4cd6fcf4b4e6e60ea986c889389535"
			},
			{
				"1.dependency_name": "/kubernetes-sigs/kubebuilder-release-tools",
				"2.dependency_token": "kubernetes-sigs/kubebuilder-release-tools",
				"3.dependency_version": "f5d822707ee6e8fb81b04a5c0040b736da22e587"
			},
			{
				"1.dependency_name": "/step-security/harden-runner",
				"2.dependency_token": "step-security/harden-runner",
				"3.dependency_version": "c1aec4ac820532bab364f02a81873c555a0ba3a1"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/analyze",
				"3.dependency_version": "3cc123766321e9f15a6676375c154ccffb12a358"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/autobuild",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/init",
				"3.dependency_version": "4777888c377a26956f1831d5b9207eea1fa3bf29"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "bdb12b622a910dfdc99a31fdfe6f45a16bc287a4"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "ec3a7ce113134d7a93b817d10a8272cb61118579"
			},
			{
				"1.dependency_name": "/crazy-max/ghaction-import-gpg",
				"2.dependency_token": "crazy-max/ghaction-import-gpg",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/goreleaser/goreleaser-action",
				"2.dependency_token": "goreleaser/goreleaser-action",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/actions/cache",
				"2.dependency_token": "actions/cache",
				"3.dependency_version": "b7dd4a6f2c343e29a9ab8e181b2f540816f28bd7"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "ec3a7ce113134d7a93b817d10a8272cb61118579"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "424fc82d43fa5a37540bae62709ddcc23d9520d4"
			},
			{
				"1.dependency_name": "/codecov/codecov-action",
				"2.dependency_token": "codecov/codecov-action",
				"3.dependency_version": "cb4264d3319acaa2bea23d51ef67f80b4f775013"
			},
			{
				"1.dependency_name": "/nick-fields/retry",
				"2.dependency_token": "nick-invision/retry",
				"3.dependency_version": "79d4afbba1b4eff8b9a98e3d2e58c4dbaf094e2b"
			},
			{
				"1.dependency_name": "/peter-evans/create-or-update-comment",
				"2.dependency_token": "peter-evans/create-or-update-comment",
				"3.dependency_version": "937d24475381cd9c75ae6db12cb4e79714b926ed"
			},
			{
				"1.dependency_name": "/peter-evans/find-comment",
				"2.dependency_token": "peter-evans/find-comment",
				"3.dependency_version": "ec3a7ce113134d7a93b817d10a8272cb61118579"
			},
			{
				"1.dependency_name": "/actions/cache",
				"2.dependency_token": "actions/cache",
				"3.dependency_version": "424fc82d43fa5a37540bae62709ddcc23d9520d4"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "f32b3a3741e1053eb607407145bc9619351dc93b"
			},
			{
				"1.dependency_name": "/actions/setup-go",
				"2.dependency_token": "actions/setup-go",
				"3.dependency_version": "7f8f3d9f0f62fe5925341be21c2e8314fd4f7c7c"
			},
			{
				"1.dependency_name": "/arduino/setup-protoc",
				"2.dependency_token": "arduino/setup-protoc",
				"3.dependency_version": "a35cf36e5301d70b76f316e867e7788a55a31dae"
			},
			{
				"1.dependency_name": "/codecov/codecov-action",
				"2.dependency_token": "codecov/codecov-action",
				"3.dependency_version": "d2dae40ed151c634e4189471272b57e76ec19ba8"
			},
			{
				"1.dependency_name": "/peter-evans/slash-command-dispatch",
				"2.dependency_token": "peter-evans/slash-command-dispatch",
				"3.dependency_version": "937d24475381cd9c75ae6db12cb4e79714b926ed"
			},
			{
				"1.dependency_name": "/actions/checkout",
				"2.dependency_token": "actions/checkout",
				"3.dependency_version": "ec3a7ce113134d7a93b817d10a8272cb61118579"
			},
			{
				"1.dependency_name": "/actions/upload-artifact",
				"2.dependency_token": "actions/upload-artifact",
				"3.dependency_version": "424fc82d43fa5a37540bae62709ddcc23d9520d4"
			},
			{
				"1.dependency_name": "/github/codeql-action",
				"2.dependency_token": "github/codeql-action/upload-sarif",
				"3.dependency_version": "64c0c85d18e984422218383b81c52f8b077404d3"
			},
			{
				"1.dependency_name": "/ossf/scorecard-action",
				"2.dependency_token": "ossf/scorecard-action",
				"3.dependency_version": "f32b3a3741e1053eb607407145bc9619351dc93b"
			},
			{
				"1.dependency_name": "/actions/stale",
				"2.dependency_token": "actions/stale",
				"3.dependency_version": "40877f718dce0101edfc7aea2b3800cc192f9ed5"
			},
			{
				"1.dependency_name": "/kubernetes-sigs/kubebuilder-release-tools",
				"2.dependency_token": "kubernetes-sigs/kubebuilder-release-tools",
				"3.dependency_version": "ec3a7ce113134d7a93b817d10a8272cb61118579"
			}
		]
	},
	"issue": {
		"num_of_issue": "182",
		"issues": [
			{
				"issue_serial": "/ossf/scorecard/issues/1828",
				"issue_name": [
					"Feature - Add a check for dependency review action "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThe dependency review action lets you proactively block pull requests that introduce dependencies with known vulnerabilities.\nGitHub introduced https://github.com/actions/dependency-review-action https://github.blog/2022-04-06-prevent-introduction-known-vulnerabilities-into-your-code/\n      ",
				"issue_comment": "\nThe dependency review action lets you proactively block pull requests that introduce dependencies with known vulnerabilities. GitHub introduced   "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1826",
				"issue_name": [
					"Concerns about metrics from @ljharb"
				],
				"issue_label": [
					"bug",
					"needs discussion",
					"question",
					"wishlist"
				],
				"issue_content": "\n          From @ljharb in https://openssf.slack.com/archives/C0235AR8N2C/p1649348249786279:\n\nhi! i have some concerns about some of the metrics used in the scorecard. for example:\n\n\"binary-artifacts\" likely doesn't often apply to ecosystems that don't compile (JS, php, ruby, python)\nmore than one contributor is nice, but the existence of a lone contributor is usually because nobody else wants to help (that's why i'm the sole maintainer of over 10% of npm's traffic - not because i don't want help, but because nobody offers it). penalizing projects for something that's not in their control doesn't seem like it'll help security.\nsimilarly, on a 1-maintainer project, it shouldn't matter if changes are landed via a PR or not, because there's nobody to do code review.\nPinned-Dependencies: the best practice for an npm project is to never pin dependencies, and to always use a ^ semver range. apps should always pin deps with a lockfile, and thus pinning deps in package.json provides no value (because transitive deps exist)\ndoes SAST take into account eslint usage? a linter is a static analysis tool just like codeql, depending on how it's configured.\npublishing from CI is not something i'd call a best practice, at least in the npm ecosystem - it's impossible to do it with 2FA, since using an authentication token is just a single factor.\nSigned-Releases: what's the benefit of signing releases in an ecosystem where the majority of users (or their tooling) don't verify them by default? Measuring whether releases are signed, before making that actually a beneficial action, seems like putting the cart before the horse.\nFuzzing: given that CI logs are not persistent (GHA deletes them in 30 days, i believe, and travis deletes them on a rerun) a fuzzing tool that doesn't eternally persist previously failed input seems like it doesn't add much value. Why is this an objectively cross-ecosystem best practice?\nDependency-Update-Tool: for renovate, does it check the project's owner's .github repo for renovate config as well? i use this, altho it's not that common to do so.\n\nI'd love to chat more about this 🙂 metrics that add value are great, but simply assigning a number to something brings a risk of unintended consequences/perverse effects, like incentivizing me to make a sock puppet account solely to code review all my changes, to satisfy the \"2 contributor\" requirement.\n\n\n@ossf/scorecard-maintainers -- Some of these points are familiar, so wherever possible, let's reference existing discussions/issues/PRs. I just want to make sure we capture all of @ljharb's questions as a first step! :)\n      ",
				"issue_comment": "From   in  : hi! i have some concerns about some of the metrics used in the scorecard. for example: \n \n \n \n \n \n \n \n \n \n I'd love to chat more about this   metrics that add value are great, but simply assigning a number to something brings a risk of unintended consequences/perverse effects, like incentivizing me to make a sock puppet account solely to code review all my changes, to satisfy the \"2 contributor\" requirement.  -- Some of these points are familiar, so wherever possible, let's reference existing discussions/issues/PRs. I just want to make sure we capture all of  's questions as a first step! :)  fyi. From   in  : hi! i have some concerns about some of the metrics used in the scorecard. for example: \n \n \n \n \n The check does not ask users to pin their dependencies in the manifest. It only verifies that there's a lock file when it's used in CI, so you get \"reproducible\" installation and don't suddenly fall victim to \"npm color\" attacks if someone pushed a malicious package. If there's a false positive you found, can you provide a link? \n \n We want to improve this check   I envisage adding support for commands as well \n \n \n yes, we'd like to have checks be more ecosystem-aware. This is on our radar, but needs work. \n \n /cc  I'd love to chat more about this   metrics that add value are great, but simply assigning a number to something brings a risk of unintended consequences/perverse effects, like incentivizing me to make a sock puppet account solely to code review all my changes, to satisfy the \"2 contributor\" requirement. good point.  -- Some of these points are familiar, so wherever possible, let's reference existing discussions/issues/PRs. I just want to make sure we capture all of  's questions as a first step! :) It only verifies that there's a lock file when it's used in CI, so you get \"reproducible\" installation and don't suddenly fall victim to \"npm color\" attacks if someone pushed a malicious package I don't think this is actually a best practice - I think that a package   fall victim to the same attacks and bugs that their users will on a fresh installation, and that encouraging lockfile use in packages actively harms users by hiding problems from maintainers. The false positive imo is that this is considered at all. I don't entirely follow. How would a maintainer of package X control how users will use their package? Even if the maintainers of package X want to protect themselves in their CI tests (I think they should), they would not be able to control what their users will do or when installing X, right? Speaking for the npm ecosystem, most lockfiles are \"dev only\", which means that a project with a lockfile has a pinned graph, but their consumers have   pinned. There's also one lockfile format that   published, and that   respected by consumers - but it's considered highly user-hostile to use this, because it prevents the consumer from deduping, and from seamlessly updating transitive deps. The belief I'm professing here is that the CI on a public project isn't worth protecting in this fashion in general terms, but it's   less important than ensuring that maintainers are notified as early as possible about issues in their dep graph. With a lockfile, the project will continue passing CI even as users who newly install it get bugs or security vulnerabilities - without one, the project's success or failure matches that of the users, and the problems can be fixed much more rapidly. The belief I'm professing here is that the CI on a public project isn't worth protecting in this fashion in general terms, but it's   less important than ensuring that maintainers are notified as early as possible about issues in their dep graph. With a lockfile, the project will continue passing CI even as users who newly install it get bugs or security vulnerabilities - without one, the project's success or failure matches that of the users, and the problems can be fixed much more rapidly. There are two different classes of problems: \n \n \n Right - but a dev-only lockfile (the almost exclusive kind used in the npm ecosystem) does not pin dependencies  , only for the maintainers. The best practice in the npm ecosystem is to   use   on dependencies and never, ever pin them - that's always only the job of the top-level app (with a lockfile). I'm not fully versed on log4j, but wasn't the issue with log4j a problem because of a ton of applications that   pinned, and thus when the long-standing vulnerability was discovered, they weren't able to upgrade to the fixed version? Right - but a dev-only lockfile (the almost exclusive kind used in the npm ecosystem) does not pin dependencies  , only for the maintainers. The best practice in the npm ecosystem is to   use   on dependencies and never, ever pin them - that's always only the job of the top-level app (with a lockfile). I think we're saying the same thing... The scorecard check does this, i.e., checks that the final top-level app uses a lock file. If someone tests an API in their CI, scorecard will also check they've added a lockfile for the CI check, but not the overall project. I'm not fully versed on log4j, but wasn't the issue with log4j a problem because of a ton of applications that   pinned, and thus when the long-standing vulnerability was discovered, they weren't able to upgrade to the fixed version? log4j was an example of how remediation takes time and limiting the blast radius helps. \nThe npm color package attack is one that affected may people due to lack of pinning. Would the changes been malicious, it would have created a lot of work for remediation. The scorecard runs on projects tho - a published project isn't a top-level app, generally (altho sometimes it can be)."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1824",
				"issue_name": [
					"Write a Scorecards Mission and Vision"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Overview\nAs proposed in the 4/24/2022 biweekly meeting, a mission and vision statement will help guide present and future contributors. Our goal is to bring shared direction across contributors and help potential contributors determine if they'd like to become a collaborator.\nFirst drafts of both a mission and vision statement are written below. To get to a final draft, we will work with group members who’re interested in writing these statements and bring them before our working group for final approval. Once approved, we will bring them to the larger OpenSSF org.\nInitial Drafts\nScorecards Mission Draft - v0.1\nAssess the software development security posture of open-source projects.\nScorecards Vision Draft - v0.1\nWith Scorecards, open-source projects’ secure development practices become accessible and understandable to consumers.\nVolunteers Wanted\nPlease tag yourself if you're interested in helping write these statements. Volunteers to far:\n\n@azeemshaikh38\n@brianrussell2\n@jeffmendoza\n@justaugustus\n@laurentsimon\n@naveensrinivasan\n\n      ",
				"issue_comment": "Overview As proposed in the 4/24/2022 biweekly meeting, a mission and vision statement will help guide present and future contributors. Our goal is to bring shared direction across contributors and help potential contributors determine if they'd like to become a collaborator. First drafts of both a mission and vision statement are written below. To get to a final draft, we will work with group members who’re interested in writing these statements and bring them before our working group for final approval. Once approved, we will bring them to the larger OpenSSF org. Initial Drafts Scorecards Mission Draft - v0.1 Assess the software development security posture of open-source projects. Scorecards Vision Draft - v0.1 With Scorecards, open-source projects’ secure development practices become accessible and understandable to consumers. Volunteers Wanted Please tag yourself if you're interested in helping write these statements. Volunteers to far: \n \n \n \n \n \n \n Edited to include myself in the list. Thanks "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1817",
				"issue_name": [
					"add 'workflow_run' as dangerous trigger"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScorecard checks: 'unsafe checkout' and 'secrets used in dangerous workflows' test only for the 'pull_request_target' trigger.\n'workflow_run' trigger suffers from the same security issues as 'pull_request_target' and should be detected as well.\nDescribe the solution you'd like\nAdd detection for the 'workflow_run' trigger when running 'unsafe checkout' and 'secrets in workflow' checks.\nThe Problem\n'workflow_run' trigger may be triggered by a forked pull request. Since anyone can fork and create a pull request for a public repository, code/data originating from the pull request cannot be trusted.\nAdditional context\nA blog post I've published explaining the issue in detail: link\n      ",
				"issue_comment": "\nScorecard checks: 'unsafe checkout' and 'secrets used in dangerous workflows' test only for the 'pull_request_target' trigger. \n'workflow_run' trigger suffers from the same security issues as 'pull_request_target' and should be detected as well. \nAdd detection for the 'workflow_run' trigger when running 'unsafe checkout' and 'secrets in workflow' checks. \n'workflow_run' trigger may be triggered by a forked pull request. Since anyone can fork and create a pull request for a public repository, code/data originating from the pull request cannot be trusted. \nA blog post I've published explaining the issue in detail:  Suggested implementation:  Thanks!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1815",
				"issue_name": [
					"Binary Artifacts should allow well-known artifacts, if best practices are followed"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\n\nThe gradle-wrapper.jar is a binary blob of executable code that is checked into nearly 2.8 Million GitHub Repositories.\n\nThis is the intended use of this jar\nhttps://github.com/marketplace/actions/gradle-wrapper-validation\nhttps://docs.gradle.org/current/userguide/gradle_wrapper.html#wrapper_checksum_verification\nDescribe the solution you'd like\nBinary artifacts knows about gradle-wrapper.jar and allows it if the repo has the Wrapper Validation Action installed (or another community-approved mitigation best practice).\nDescribe alternatives you've considered\nAn alternative could be fully user configurable, like providing the name and checksum (https://gradle.org/release-checksums/).  The public scorecard results wouldn't have this, but user-run scorecard checks could exempt certain binaries.\n      ",
				"issue_comment": "The gradle-wrapper.jar is a binary blob of executable code that is checked into nearly  . This is the intended use of this jar \n \nBinary artifacts knows about gradle-wrapper.jar and allows it if the repo has the Wrapper Validation Action installed (or another community-approved mitigation best practice). \nAn alternative could be fully user configurable, like providing the name and checksum ( ).  The public scorecard results wouldn't have this, but user-run scorecard checks could exempt certain binaries. As I noted in  \nsupporting an .artifact-ignore could be very useful (like .gitignore). Discussion in the sync meeting was against the proposed solution, and for the described alternative above. That works for me as well."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1807",
				"issue_name": [
					"ossf/scorecard should detect unpinned dependencies via chocolatey installer "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nossf/scorecard should detect unpinned dependencies via chocolatey installer.\nDescribe the solution you'd like\nossf/scorecard should detect unpinned dependencies via chocolatey installer.\nDescribe alternatives you've considered\nNone.\nAdditional context\nExample:\n    - name: Set up OpenCppCoverage and add to PATH\n      id: setup_opencppcoverage\n      run: |\n        choco install OpenCppCoverage -y\n        echo \"C:\\Program Files\\OpenCppCoverage\" >> $env:GITHUB_PATH\n\nThis should detect an unpinned dependency.\nI think this should require something like:\nchoco install -y --requirechecksum=true --checksum=2295A733DA39412C61E4F478677519DD0BB1893D88313CE56B468C9E50517888 --checksum-type=sha256 OpenCppCoverage\n\n      ",
				"issue_comment": "\nossf/scorecard should detect unpinned dependencies via chocolatey installer. \nossf/scorecard should detect unpinned dependencies via chocolatey installer. \nNone. \nExample: This should detect an unpinned dependency. I think this should require something like: Great, thanks. So the code to update is  \nThere are calls to  ,  , etc you can take as example (called in the same function). These functions are called from  , but you don't need to change these. If you're interested in adding support, you'll also need to add some unit tests: \n You should able to use existing test files from   by adding  your example in these files. Let me know if you're interested in helping. I can give additional help if you need. Thanks! Thanks for the pointers, I will take a stab at fixing this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1783",
				"issue_name": [
					"BUG: code review bypass via commits to local branches"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Steps:\n\nDependabot publishes a change.\nMaintainer edits that change to do whatever they want\nMaintainer approves it\nMaintainer merges it.\n\nLet's try to verify that the approver != from all commits in the PR. In the config, branch protection is not enabled thru *, I think.\nIs there a dependabot config option for dependabot to use a remote fork's branch instead of local branch on the project?\nThanks @loosebazooka for sharing!\n      ",
				"issue_comment": "Steps: \n \n \n \n \n Let's try to verify that the approver != from all commits in the PR. In the config, branch protection is not enabled thru  , I think. Is there a dependabot config option for dependabot to use a remote fork's branch instead of local branch on the project? Thanks   for sharing! another bypass I found: \n \n \n The PR is considered reviewed. Scorecard has this code   that accepts these because the committer is always \"github\". graphql has a field to know who clicked \"auto-merge\" this PR. Steps: \n \n \n \n \n Let's try to verify that the approver != from all commits in the PR. In the config, branch protection is not enabled thru  , I think. Is there a dependabot config option for dependabot to use a remote fork's branch instead of local branch on the project? Thanks   for sharing! For this, can we do something like - expand the   struct to also contain commits (today we only care about merge commit)? That should provide us the info to find these types of bypasses. For this, can we do something like - expand the   struct to also contain commits (today we only care about merge commit)? yep that's also what I had in mind. Seems doable; it may increase rate limiting usage, but we'll see. Yeah, good thing is we have tests that will tell us by how much the API usage will go up. Would be good to get this fixed soon. Do you want to take this on or keep this open for someone else? If nobody else wants it I'll take it, but if someone is interested I'm happy for them to take it. If nobody else wants it I'll take it, but if someone is interested I'm happy for them to take it. I can take this on to reduce some of your load. SG, it's a fun (and important) one!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1781",
				"issue_name": [
					"Feature: rename fields of raw results"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The current implementation used fields as some-field, let's rename to someField.\nFyi, BQ does not accept - in name.\n      ",
				"issue_comment": "The current implementation used fields as  , let's rename to  . \nFyi, BQ does not accept   in name."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1779",
				"issue_name": [
					"Feature: add e2e tests for webhook check"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          See title\n      ",
				"issue_comment": "See title"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1777",
				"issue_name": [
					"BUG: Code-Review does not understand un-squashed commits"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          The Code-Review check counts all the commits in a PR if they are merged un-squashed. So there are situations where we only look at a few commits - rather than 30.  In general, we need to fix the repo client's ListCommit() function.\nNeed some discussion how to solved this.\n      ",
				"issue_comment": "The Code-Review check counts all the commits in a PR if they are merged un-squashed. So there are situations where we only look at a few commits - rather than 30.  In general, we need to fix the repo client's   function. Need some discussion how to solved this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1776",
				"issue_name": [
					"Feature: New check for SLSA provenance generation"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          This check may replace the Signed-Release checks and verify that the releases generate SLSA provenance using the official builders from https://github.com/slsa-framework/slsa. We can:\n\nVerify the presence of .intoto.jsonl file\nVerify a workflow exists calling an official action\nVerify the signature by calling the API exposed by https://github.com/slsa-framework/slsa\n\nThis should allow us to also provide some SLSA compliance flags for scorecard, e.g. something like --slsa=3\n      ",
				"issue_comment": "This check may replace the   checks and verify that the releases generate SLSA provenance using the official builders from  . We can: \n \n \n \n This should allow us to also provide some SLSA compliance flags for scorecard, e.g. something like  Sounds like a good idea to me. Maybe expand the   check to include this? The updated check could verify various methods which ensure release integrity - GitHub signatures, cosign signatures, the SLSA workflow etc? Provenance encompasses all the other signature schemes, since it contains a signature not only of the binary, but also of how the binary was produced ,so I'm inclined to retire the   check to encourage users to use provenance instead - since it's a superset of all the others. Anyone with an opinion, please chime in This is great!  Sure, not tied to the   name at all. To confirm are you saying that if users sign their release (let's say with cosign), we should complain that it's not enough and instead drive them towards building provenance? I was thinking the new check will encompass all schemes - provenance but also signatures. Sure, not tied to the   name at all. To confirm are you saying that if users sign their release (let's say with cosign), we should complain that it's not enough and instead drive them towards building provenance? yes, so long as there is tooling to achieve it. I was thinking the new check will encompass all schemes - provenance but also signatures. provenance also contains a signature that covers the binary and additional information (builder ID, commands, etc), so it does encompasses the signature."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1775",
				"issue_name": [
					"BUG: Scorecard complains about ELF files even when they are testcases and not-executable"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nSome projects need ELF files for purposes other than running code.\nFor example, AFL contains ELF files that are example testcases that have crashed the strings program.\nI imagine binary analysis tools have the same problem.\nHowever, allstar complains about these files even if they are marked non-executable.\nReproduction steps\nSee example: google/AFL#155\nExpected behavior\nNon-executable files should not be considered a security risk.\nUnder reasonable assumptions (ie a testcase is not gonna exploit a buffer overflow in our program and use that to own the user), these files cannot do anything malicious.\nI've been told that these files are problematic because they are not reviewable.\nHowever, plenty of things are not reviewable, that are still accepted e.g. PNG and PDF files which are often included in repos.\nAdditional context\nI don't think the fact that these files can be malicious if they are marked executable is a reason to flag them.\nIt's possible to hide executables in other files that can be executed, see this PDF (viewable using evince, not chrome sadly) made by corkami which you can mark executable and execute as an elf, so binary files that aren't usually executable can attack users using this same method.\nCorkaMInuX.pdf\n      ",
				"issue_comment": "\nSome projects need ELF files for purposes other than running code. \nFor example, AFL contains ELF files that are example testcases that have crashed the   program. \nI imagine binary analysis tools have the same problem. \nHowever, allstar complains about these files even if they are marked non-executable. \nSee example:  \nNon-executable files should not be considered a security risk. \nUnder reasonable assumptions (ie a testcase is not gonna exploit a buffer overflow in our program and use that to own the user), these files cannot do anything malicious. \nI've been told that these files are problematic because they are not reviewable. \nHowever, plenty of things are not reviewable, that are still accepted e.g. PNG and PDF files which are often included in repos. \nI don't think the fact that these files can be malicious if they are marked executable is a reason to flag them. \nIt's possible to hide executables in other files that can be executed, see this PDF (viewable using evince, not chrome sadly) made by corkami which you can mark executable and execute as an elf, so binary files that aren't usually executable can attack users using this same method. \n CC "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1774",
				"issue_name": [
					"Fix the protobuf GitHub Action failures"
				],
				"issue_label": [],
				"issue_content": "\n          All my PRs are failing with:\nprotoc --go_out=../../../ cron/data/request.proto\n/bin/bash: protoc: command not found\nmake: *** [Makefile:10[8](https://github.com/ossf/scorecard/runs/5652653919?check_suite_focus=true#step:7:8): cron/data/request.pb.go] Error 127\n\nOriginally posted by @laurentsimon in #1772 (comment)\n      ",
				"issue_comment": "All my PRs are failing with: any ideas why this is happening? No idea! I wish there was an easy answer! Let me know if the failures are still going on. Hopefully,   fixed this.     still happening "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1773",
				"issue_name": [
					"Feature: Pinned-Dependencies digest hashes don't make sense for multi-platform docker images"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nDockerfiles allow you to pin the FROM using a digest hash:\nFROM golang@sha256:3c4de86eec9cbc619cdd72424abd88326ffcf5d813a8338a7743c55e5898734f AS base\n\nThis digest is however platform specific:\n\nTherefore any project which wants to release or use multi-platform docker images can't use the digest in the FROM and they're going to get dinged by scorecard.\nWith the release of M1 macs and ARM cloud servers this is becoming an increasingly common problem.\nDescribe the solution you'd like\nIs there another way of pinning the digest that could solve this problem for multi-platform images? Perhaps we can update https://github.com/ossf/scorecard/blob/main/docs/checks.md#pinned-dependencies to note that?\nDescribe alternatives you've considered\nI guess just living with the lower score.\n      ",
				"issue_comment": "\nDockerfiles allow you to pin the   using a digest hash: This digest is however platform specific: Therefore any project which wants to release or use multi-platform docker images can't use the digest in the   and they're going to get dinged by scorecard. With the release of M1 macs and ARM cloud servers this is becoming an increasingly common problem. \nIs there another way of pinning the digest that could solve this problem for multi-platform images? Perhaps we can update   to note that? \nI guess just living with the lower score. Thanks for the issue! Can you use variables to do that? We accept   and use  ? That brings up a second question: how do you store the hashes in a place that dependabot/renovatebot will understand and update? There's also a template engine, but I've not looked into it much yet. Let's add this back to the doc once we've figured out the right solution cc     any thoughts? I don't see why multiplatform images can't be referenced by the hash of the manifest list? for example to obtain the manifest list digest and we can see the contents are indeed a manifest list  this does seem to work. It builds fine on my m1 mac. Thanks! I don't know if depandabot/renovate bot actually know how to keep these up to date though, be curious to see what happens Yea that should be fine since OCI objects are all hash referenced and verified. The person usually uploading the manifest would need the same permissions as whoever is uploading the manifest so shouldn't be an issue there - unless this changes in the future. Maybe I'm misunderstanding the original question: but was the question around dockerfiles rather than the resulting manifest?  Yes in the original question I was unsure how best to pin dependencies when using multi-platform images since I thought the digest hash was platform-specific. But as it turns out theres a digest hash which is not platform-specific, and using that works just fine for  . We could probably close this issue unless it'd be worthwhile adding a note to the docs. ah, I see. So there's a digest that contains all the other images. I'd like to add this to the documentation of our check for the remediation part. Mind drafting a sentence or 2 that we could add in our doc for other users to benefit from? You can send a PR or just paste it here if you prefer. can you link to the docs? this is the file to update  \nthen run   to re-generate the "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1771",
				"issue_name": [
					"BUG: dangerous workflow alerts when code is not run"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          The dangerous workflow alerts when a secret is used in a pull request. If the untrusted code is not run (e.g., https://github.com/actions-runner-controller/actions-runner-controller/blob/master/.github/workflows/runners.yml#L51) we still create an alert but it's a false positive.\nExample: https://github.com/actions-runner-controller/actions-runner-controller/blob/master/.github/workflows/runners.yml#L51\nThe right solution is not clear. Things that may help:\n\nLook at actions used\nloo at command being run or not\nlook at if statement\nlook at defined permissions\n\n      ",
				"issue_comment": "The dangerous workflow alerts when a secret is used in a pull request. If the untrusted code is not run (e.g.,  ) we still create an alert but it's a false positive. Example:  The right solution is not clear. Things that   help: \n \n \n \n \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1738",
				"issue_name": [
					"Add support for orgs in the cron scans"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Instead of only allowing repo entries, we should allow org entries in https://github.com/ossf/scorecard/blob/main/cron/data/projects.csv\n      ",
				"issue_comment": "Instead of only allowing repo entries, we should allow org entries in  pasting   's additional ideas: Another improvement I would say is to record older commit data along with the latest one. It'll improve the granularity of the data we have"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1736",
				"issue_name": [
					"Feature - Upgrade to go 1.18"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nUpgrade to go https://go.dev/doc/go1.18\n      ",
				"issue_comment": "\nUpgrade to go "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1729",
				"issue_name": [
					"Scorecard - GitHub bot account"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nIn the process of automating scorecard-actions e2e testing a GitHub token is needed for creating issues on the scorecard-actions repo when these actions fail.\nThese actions run on a cron-job. Here is an example https://github.com/ossf-tests/scorecard-action-non-main-branch/tree/other\nI am using my own personal token to run these actions and create an issue when these actions fail ossf/scorecard-action#138\nI wouldn't want my personal token to do this.\nThe right approach would be to use scorecard-bot to do these jobs.\n@ossf/scorecard-admins Thoughts?\n      ",
				"issue_comment": "\nIn the process of automating scorecard-actions   testing a GitHub   is needed for creating issues on the scorecard-actions repo when these actions fail. These actions run on a  . Here is an example  I am using my own personal token to run these actions and create an issue when these actions fail  I wouldn't want my personal token to do this. The right approach would be to use   to do these jobs.  Thoughts? Sounds like a good idea. \nDo you have documentation links about GitHub bot accounts, how to create them, etc? It is nothing but another GitHub account. The management of that account is the issue, who has the passwords for the account. Then the management of the keys that the account generates for automation etc.  Any thoughts on this? It is nothing but another GitHub account. The management of that account is the issue, who has the passwords for the account. Then the management of the keys that the account generates for automation etc.  Any thoughts on this?    -- Can you look into   for  ? I know CNCF (cc:    ) provides this and can be requested via CNCF Service Desk (I have a few vaults for Kubernetes SIG Release). w.r.t. bots, ideally this is eventually a GitHub App (with real boteyness), but most/all of the \"bots\" I deal with today are shared GH accounts with the team holding creds in a vault somewhere. I've squatted on  , so we can use that (if everyone's in agreement). My really rough cut of bot guidelines: \n \n \n \n \n    -- Can you look into   for  ? we're on it!    -- Can you look into   for  ? we're on it! Great! Thanks just to clarify: the only permission is to be able to create issues, correct? And there's no dangerous permissions needed to do that, any PAT can do it. Is that correct? The bot can be extended for all the things like merging code, create issues, run e2e test etc. At the moment it is only for creating issues and run e2e. everything   merging code requires no dangerous permissions w.r.t the scorecard repo itself. I think what I was trying to gauge is how sensitive this bot account it. My understanding of \"merging code\" here is that it needs to bypass branch protection. It'd be nice to avoid doing this if we can everything   merging code requires no dangerous permissions w.r.t the scorecard repo itself. I think what I was trying to gauge is how sensitive this bot account it. My understanding of \"merging code\" here is that it needs to bypass branch protection. It'd be nice to avoid doing this if we can I understand your concern. Merging code isn't required now. But when we plan for something like this it is good to plan for the long term.  The projects like  ,  , and other large GitHub repositories use   to merge code. So we aren't going to be the first one. Also   shouldn't require \"bypass branch\" protection. Have you read this anywhere? If so can you please point to the doc? auto merging requires adding a review automatically. (added this comment to the auto-merge PR  ). You cannot automatically push to a protected branch, so either BP must be disabled or review must be added, which is what the auto-merge action does using the PAT provided as input. So IIUC, the bot becomes a contributor of the scorecard project, so highly sensitive. How would PAT rotation work? It also commented that we should explore \"batched\" dependabot PR, like once a week or something, which would avoid having an auto-merge feature we're on it! Thanks Jory!! I'll drop a few thoughts on things I'd like to see/have seen... Multiple roles for a bot We have multiple bots in Kubernetes orgs. \nMost/all of them are user accounts; most/all run do their tasks with as few privileges on the org as possible: ref:  In the case of  , the account has multiple PATs; some have write access to repos, but most do not. \nSetting the correctly-scoped tokens as organization secrets ( ), so we can leverage on any scorecard repo should solve the problem. Batching PRs / dependabot On the roadmap, but no timeline as of yet. ref:  ,  ,  Batching is ideal, but approval is paramount. \nAny PR that merges should be subject to same review/checks, both from maintainers and presubmits. An additional gate that we have in Kubernetes is requiring an   label on PRs from \"non-trusted\" (non-org members) before we run presubmits. Because dependabot is a GitHub App (and not an org member), we get around this limitation in SIG Release repos by assuming dependabot is trusted, and having it automatically apply the   label for PRs:  Going a layer deeper on trust, certain repos/workflows can apply a   label:  This allows us to do things like automatically updating Prow images:  \nMotivations and some detail on workflow for that here:  Back to dependabot... In instances that our tests match our expectations, then a test failure says, \"I, as a maintainer, do not trust this code to be accepted\". \nThis is currently true for any PR submitted. For dependency updates, the human element of clicking   is mechanically and arguably, could be automated away. Example workflow: \n \n \n \n \n Back to batching... One of the things I crave in non-Kubernetes orgs is   (the merge pool):  We have a few expectations that are relevant here and enforced by branch protection: \n \n \n We violate merge commits on pull requests by merging   in to the PR branch, but that violation is nullified by the fact that we prefer squash and merge as a merge strategy. A little out-of-scope here, but I have   and did a Twitter poll to check-in with other maintainers on the internet if you want to read more:  tl;dr snippet: My answer is \"it depends\". kubernetes/kubernetes does merge commits. \nI've found with small projects/fewer maintainers, squashing works really nicely, especially if it's solving/circumventing teaching people to rebase. As a more experienced maintainer, most of my commits are concise, focused on a specific area, and may include descriptions in the commit descriptions. For a larger PR, you're going to lose the value of being able to interrogate those structured commits. For a newer contributor, you may have a few scenarios: \n \n \n \n Education is important here if you want to keep them contributing. So in repos where multiple merge strategies are enabled, my preference is usually to do the following: \n \n \n I mention merge strategy because every dependabot PR leads to churn in multiple human approvals and in instances where the go.sum has been changed in a way that triggers another dependabot to start rebasing itself. This goes away with a merge queue. I haven't played around with it yet, but I would suggest a tool like Mergify:   (which is free for open source projects) I've enabled it on a few of my orgs just now, but would be good to get some opinions from maintainers here. Batching is ideal, but approval is paramount. Any PR that merges should be subject to same review/checks, both from maintainers and presubmits. For dependency updates, the human element of clicking   is mechanically and arguably, could be automated away. Human approval may also include taking a peek at the PR. \nAny thoughts on combining dependabot PRs thru  , to create a batched PR? \nI've tested this idea in  \nThe (draft) script   iterates thru PRs, checks they are authored from dependabot, combines them using  , create a new PR, then closes the original ones. \nThe downside is that it creates a bit of noise due to PRs being closed by the bot. \nI'm not 100% sure how reliable the combine part is, but I suspect it may work if the dependencies PR have no \"conflicts\". This does not require auto review/merge of PRs. Has anyone tried this before? I suppose I could try to combine PRs for the past week and see if it works or if there are conflicts. we're on it! Thanks Jory!! I'll drop a few thoughts on things I'd like to see/have seen... Multiple roles for a bot We have multiple bots in Kubernetes orgs. Most/all of them are user accounts; most/all run do their tasks with as few privileges on the org as possible: ref:  In the case of  , the account has multiple PATs; some have write access to repos, but most do not. Setting the correctly-scoped tokens as organization secrets ( ), so we can leverage on any scorecard repo should solve the problem. Batching PRs / dependabot On the roadmap, but no timeline as of yet. ref:  ,  ,  Batching is ideal, but approval is paramount. Any PR that merges should be subject to same review/checks, both from maintainers and presubmits. An additional gate that we have in Kubernetes is requiring an   label on PRs from \"non-trusted\" (non-org members) before we run presubmits. Because dependabot is a GitHub App (and not an org member), we get around this limitation in SIG Release repos by assuming dependabot is trusted, and having it automatically apply the   label for PRs:  Going a layer deeper on trust, certain repos/workflows can apply a   label:  This allows us to do things like automatically updating Prow images:   Motivations and some detail on workflow for that here:  Back to dependabot... In instances that our tests match our expectations, then a test failure says, \"I, as a maintainer, do not trust this code to be accepted\". This is currently true for any PR submitted. For dependency updates, the human element of clicking   is mechanically and arguably, could be automated away. Example workflow: \n \n \n \n \n Back to batching... One of the things I crave in non-Kubernetes orgs is   (the merge pool):  We have a few expectations that are relevant here and enforced by branch protection: \n \n \n We violate merge commits on pull requests by merging   in to the PR branch, but that violation is nullified by the fact that we prefer squash and merge as a merge strategy. A little out-of-scope here, but I have   and did a Twitter poll to check-in with other maintainers on the internet if you want to read more:  tl;dr snippet: My answer is \"it depends\". \nkubernetes/kubernetes does merge commits. \nI've found with small projects/fewer maintainers, squashing works really nicely, especially if it's solving/circumventing teaching people to rebase. \nAs a more experienced maintainer, most of my commits are concise, focused on a specific area, and may include descriptions in the commit descriptions. \nFor a larger PR, you're going to lose the value of being able to interrogate those structured commits. \nFor a newer contributor, you may have a few scenarios: \n \n \n \n Education is important here if you want to keep them contributing. So in repos where multiple merge strategies are enabled, my preference is usually to do the following: \n \n \n I mention merge strategy because every dependabot PR leads to churn in multiple human approvals and in instances where the go.sum has been changed in a way that triggers another dependabot to start rebasing itself. This goes away with a merge queue. I haven't played around with it yet, but I would suggest a tool like Mergify:   (which is free for open source projects) I've enabled it on a few of my orgs just now, but would be good to get some opinions from maintainers here. Lots of information to read go down the rabbit hole  @  any plans to support batched pull requests in renovatebot? I found   IIUC, this would allow us to batch PRs on a schedule? Are there limitations to this config? cc   "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1727",
				"issue_name": [
					"Enhancement proposal process"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nIn the bi-weekly meeting, it was discussed that Scorecard/OSSF come up with an Enhancement proposal process.\nIt would avoid surprises when a large feature lands as a PR.\n      ",
				"issue_comment": "\nIn the bi-weekly meeting, it was discussed that Scorecard/OSSF come up with an Enhancement proposal process. It would avoid surprises when a large feature lands as a PR.  I am assigning this to you as you have done similar with KEP."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1726",
				"issue_name": [
					"Feature: Dependency-Update-Tool should check whether tools are available"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Dependency-Update-Tool  checks if a project uses Dependabot or Renovate bot. Although these tools support many languages and ecosystems, there are cases they don't support, for example, C/C++ projects. For such projects, it is often not feasible to set up a tool for automated dependency updates. This is related to #74.\nFor example, Dependency-Update-Tool reports 0 / 10 for CuraEngine that is writted in C++. This score doesn't look fair because it doesn't seem to be possible to configure automatic dependency updates. Neither Dependabot nor Renovatebot can help here.\nI'd like to propose updating the check, so that it reports ? for projects that use languages/ecosystems that are not supported by Dependabot/Renovate bot. GitHub API provides a list of languates used in a project repository. The check could use this info to decide if automated dependency updates are possible.\nAlternatively, the check could return a non-zero score (but less than 10) if no automated updates are possible.\nI'd be happy to draft a pull request for that if the idea is accepted.\n      ",
				"issue_comment": " checks if a project uses Dependabot or Renovate bot. Although these tools support many languages and ecosystems, there are cases they don't support, for example, C/C++ projects. For such projects, it is often not feasible to set up a tool for automated dependency updates. This is related to  . For example,   reports   for   that is writted in C++. This score doesn't look fair because it doesn't seem to be possible to configure automatic dependency updates. Neither Dependabot nor Renovatebot can help here. I'd like to propose updating the check, so that it reports   for projects that use languages/ecosystems that are not supported by Dependabot/Renovate bot. GitHub API provides a list of languates used in a project repository. The check could use this info to decide if automated dependency updates are possible. Alternatively, the check could return a non-zero score (but less than 10) if no automated updates are possible. I'd be happy to draft a pull request for that if the idea is accepted. These docs list languages/ecosystems that Dependabot and Renovatebot support: \n \n \n Thanks for the report, and sorry for the late reply.. I've missed your issue! What you suggest seems like a reasonable thing to do. The only drawback is that the check won't be accessible for local repositories - since it now needs an API call; but I think that's fine. We could try to list files and look for extensions.. but that sounds like a a lot of work and prone to errors. I think it's fine to return -1 if it's not possible with dependabot/renovatebot. Feel free to send a PR and cc me to it for review. Thanks again for your help!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1725",
				"issue_name": [
					"BUG -  Upgrade to buildkit 0.10.0 fails because of Unit test failures"
				],
				"issue_label": [
					"bug",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          Describe the bug\nBUG -  Upgrade to buildkit 0.10.0 fails because of Unit test failures\n#1722\n2022/03/11 15:37:45 Non-pinned dockerfile:   utests.TestReturn{\n[32](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:32)\n  \tError:         nil,\n[33](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:33)\n- \tScore:         0,\n[34](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:34)\n+ \tScore:         10,\n[35](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:35)\n- \tNumberOfWarn:  1,\n[36](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:36)\n+ \tNumberOfWarn:  0,\n[37](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:37)\n- \tNumberOfInfo:  0,\n[38](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:38)\n+ \tNumberOfInfo:  1,\n[39](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:39)\n  \tNumberOfDebug: 0,\n[40](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:40)\n  }\n[41](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:41)\n\n[42](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:42)\n2022/03/11 15:37:45 Pinned dockerfile as:   utests.TestReturn{\n[43](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:43)\n  \tError:         nil,\n[44](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:44)\n- \tScore:         10,\n[45](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:45)\n+ \tScore:         0,\n[46](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:46)\n- \tNumberOfWarn:  0,\n[47](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:47)\n+ \tNumberOfWarn:  1,\n[48](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:48)\n- \tNumberOfInfo:  1,\n[49](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:49)\n+ \tNumberOfInfo:  0,\n[50](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:50)\n  \tNumberOfDebug: 0,\n[51](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:51)\n  }\n[52](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:52)\n\n[53](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:53)\n2022/03/11 15:37:45 Non-pinned dockerfile as:   utests.TestReturn{\n[54](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:54)\n  \tError:         nil,\n[55](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:55)\n- \tScore:         0,\n[56](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:56)\n+ \tScore:         10,\n[57](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:57)\n- \tNumberOfWarn:  2,\n[58](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:58)\n+ \tNumberOfWarn:  0,\n[59](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:59)\n- \tNumberOfInfo:  0,\n[60](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:60)\n+ \tNumberOfInfo:  1,\n[61](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:61)\n  \tNumberOfDebug: 0,\n[62](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:62)\n  }\n[63](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:63)\n\n[64](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:64)\n--- FAIL: TestDockerfilePinning (0.00s)\n[65](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:65)\n    --- FAIL: TestDockerfilePinning/Non-pinned_dockerfile (0.02s)\n[66](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:66)\n    --- FAIL: TestDockerfilePinning/Pinned_dockerfile_as (0.01s)\n[67](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:67)\n    --- FAIL: TestDockerfilePinning/Non-pinned_dockerfile_as (0.00s)\n[68](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:68)\n2022/03/11 15:37:45 Dockerfile with args:   utests.TestReturn{\n[69](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:69)\n  \tError:         nil,\n[70](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:70)\n- \tScore:         0,\n[71](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:71)\n+ \tScore:         10,\n[72](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:72)\n- \tNumberOfWarn:  2,\n[73](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:73)\n+ \tNumberOfWarn:  0,\n[74](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:74)\n- \tNumberOfInfo:  0,\n[75](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:75)\n+ \tNumberOfInfo:  1,\n[76](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:76)\n  \tNumberOfDebug: 0,\n[77](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:77)\n  }\n[78](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:78)\n\n[79](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:79)\n2022/03/11 15:37:45 Pinned dockerfile as no hash:   utests.TestReturn{\n[80](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:80)\n  \tError:         nil,\n[81](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:81)\n  \tScore:         0,\n[82](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:82)\n- \tNumberOfWarn:  4,\n[83](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:83)\n+ \tNumberOfWarn:  1,\n[84](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:84)\n  \tNumberOfInfo:  0,\n[85](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:85)\n  \tNumberOfDebug: 0,\n[86](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:86)\n  }\n[87](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:87)\n\n[88](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:88)\n--- FAIL: TestDockerfilePinningWihoutHash (0.00s)\n[89](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:89)\n    --- FAIL: TestDockerfilePinningWihoutHash/Dockerfile_with_args (0.00s)\n[90](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:90)\n        pinned_dependencies_test.go:799: test failed: log message not present: {Error:<nil> Score:0 NumberOfWarn:2 NumberOfInfo:0 NumberOfDebug:0}\n[91](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:91)\n    --- FAIL: TestDockerfilePinningWihoutHash/Pinned_dockerfile_as_no_hash (0.00s)\n[92](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:92)\n--- FAIL: TestDockerfileInsecureDownloadsLineNumber (0.00s)\n[93](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:93)\n    --- FAIL: TestDockerfileInsecureDownloadsLineNumber/dockerfile_downloads (0.00s)\n[94](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:94)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:curl bla | bash startLine:35 endLine:36} {snippet:pip install -r requirements.txt startLine:41 endLine:42}]\n[95](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:95)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:curl bla | bash startLine:35 endLine:36} {snippet:pip install -r requirements.txt startLine:41 endLine:42}]\n[96](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:96)\n    --- FAIL: TestDockerfileInsecureDownloadsLineNumber/dockerfile_downloads_multi-run (0.00s)\n[97](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:97)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:/tmp/file3 startLine:28 endLine:28} {snippet:/tmp/file1 startLine:30 endLine:30} {snippet:bash /tmp/file3 startLine:32 endLine:34} {snippet:bash /tmp/file1 startLine:37 endLine:38}]\n[98](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:98)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:/tmp/file3 startLine:28 endLine:28} {snippet:/tmp/file1 startLine:30 endLine:30} {snippet:bash /tmp/file3 startLine:32 endLine:34} {snippet:bash /tmp/file1 startLine:37 endLine:38}]\n[99](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:99)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:/tmp/file3 startLine:28 endLine:28} {snippet:/tmp/file1 startLine:30 endLine:30} {snippet:bash /tmp/file3 startLine:32 endLine:34} {snippet:bash /tmp/file1 startLine:37 endLine:38}]\n[100](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:100)\n        pinned_dependencies_test.go:639: test failed: log message not present: [{snippet:/tmp/file3 startLine:28 endLine:28} {snippet:/tmp/file1 startLine:30 endLine:30} {snippet:bash /tmp/file3 startLine:32 endLine:34} {snippet:bash /tmp/file1 startLine:37 endLine:38}]\n[101](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:101)\n2022/03/11 15:37:45 curl | sh:   utests.TestReturn{\n[102](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:102)\n  \tError:         nil,\n[103](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:103)\n- \tScore:         0,\n[104](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:104)\n+ \tScore:         10,\n[105](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:105)\n- \tNumberOfWarn:  4,\n[106](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:106)\n+ \tNumberOfWarn:  0,\n[107](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:107)\n- \tNumberOfInfo:  0,\n[108](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:108)\n+ \tNumberOfInfo:  1,\n[109](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:109)\n  \tNumberOfDebug: 0,\n[110](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:110)\n  }\n[111](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:111)\n\n[112](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:112)\n2022/03/11 15:37:45 aws file:   utests.TestReturn{\n[113](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:113)\n  \tError:         nil,\n[114](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:114)\n- \tScore:         0,\n[115](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:115)\n+ \tScore:         10,\n[116](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:116)\n- \tNumberOfWarn:  15,\n[117](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:117)\n+ \tNumberOfWarn:  0,\n[118](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:118)\n- \tNumberOfInfo:  0,\n[119](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:119)\n+ \tNumberOfInfo:  1,\n[120](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:120)\n  \tNumberOfDebug: 0,\n[121](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:121)\n  }\n[122](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:122)\n\n[123](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:123)\n2022/03/11 15:37:45 pkg managers:   utests.TestReturn{\n[124](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:124)\n  \tError:         nil,\n[125](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:125)\n- \tScore:         0,\n[126](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:126)\n+ \tScore:         10,\n[127](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:127)\n- \tNumberOfWarn:  37,\n[128](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:128)\n+ \tNumberOfWarn:  0,\n[129](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:129)\n- \tNumberOfInfo:  0,\n[130](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:130)\n+ \tNumberOfInfo:  1,\n[131](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:131)\n  \tNumberOfDebug: 0,\n[132](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:132)\n  }\n[133](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:133)\n\n[134](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:134)\n2022/03/11 15:37:45 gsutil file:   utests.TestReturn{\n[135](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:135)\n  \tError:         nil,\n[136](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:136)\n- \tScore:         0,\n[137](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:137)\n+ \tScore:         10,\n[138](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:138)\n- \tNumberOfWarn:  17,\n[139](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:139)\n+ \tNumberOfWarn:  0,\n[140](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:140)\n- \tNumberOfInfo:  0,\n[141](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:141)\n+ \tNumberOfInfo:  1,\n[142](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:142)\n  \tNumberOfDebug: 0,\n[143](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:143)\n  }\n[144](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:144)\n\n[145](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:145)\n2022/03/11 15:37:45 wget file:   utests.TestReturn{\n[146](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:146)\n  \tError:         nil,\n[147](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:147)\n- \tScore:         0,\n[148](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:148)\n+ \tScore:         10,\n[149](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:149)\n- \tNumberOfWarn:  10,\n[150](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:150)\n+ \tNumberOfWarn:  0,\n[151](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:151)\n- \tNumberOfInfo:  0,\n[152](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:152)\n+ \tNumberOfInfo:  1,\n[153](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:153)\n  \tNumberOfDebug: 0,\n[154](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:154)\n  }\n[155](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:155)\n\n[156](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:156)\n2022/03/11 15:37:45 proc substitution:   utests.TestReturn{\n[157](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:157)\n  \tError:         nil,\n[158](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:158)\n- \tScore:         0,\n[159](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:159)\n+ \tScore:         10,\n[160](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:160)\n- \tNumberOfWarn:  6,\n[161](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:161)\n+ \tNumberOfWarn:  0,\n[162](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:162)\n- \tNumberOfInfo:  0,\n[163](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:163)\n+ \tNumberOfInfo:  1,\n[164](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:164)\n  \tNumberOfDebug: 0,\n[165](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:165)\n  }\n[166](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:166)\n\n[167](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:167)\n2022/03/11 15:37:45 download with some python:   utests.TestReturn{\n[168](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:168)\n  \tError:         nil,\n[169](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:169)\n- \tScore:         0,\n[170](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:170)\n+ \tScore:         10,\n[171](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:171)\n- \tNumberOfWarn:  1,\n[172](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:172)\n+ \tNumberOfWarn:  0,\n[173](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:173)\n- \tNumberOfInfo:  0,\n[174](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:174)\n+ \tNumberOfInfo:  1,\n[175](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:175)\n  \tNumberOfDebug: 0,\n[176](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:176)\n  }\n[177](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:177)\n\n[178](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:178)\n2022/03/11 15:37:45 wget | /bin/sh:   utests.TestReturn{\n[179](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:179)\n  \tError:         nil,\n[180](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:180)\n- \tScore:         0,\n[181](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:181)\n+ \tScore:         10,\n[182](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:182)\n- \tNumberOfWarn:  3,\n[183](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:183)\n+ \tNumberOfWarn:  0,\n[184](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:184)\n- \tNumberOfInfo:  0,\n[185](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:185)\n+ \tNumberOfInfo:  1,\n[186](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:186)\n  \tNumberOfDebug: 0,\n[187](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:187)\n  }\n[188](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:188)\n\n[189](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:189)\n2022/03/11 15:37:45 curl file sh:   utests.TestReturn{\n[190](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:190)\n  \tError:         nil,\n[191](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:191)\n- \tScore:         0,\n[192](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:192)\n+ \tScore:         10,\n[193](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:193)\n- \tNumberOfWarn:  12,\n[194](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:194)\n+ \tNumberOfWarn:  0,\n[195](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:195)\n- \tNumberOfInfo:  0,\n[196](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:196)\n+ \tNumberOfInfo:  1,\n[197](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:197)\n  \tNumberOfDebug: 0,\n[198](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:198)\n  }\n[199](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:199)\n\n[200](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:200)\n--- FAIL: TestDockerfileScriptDownload (0.00s)\n[201](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:201)\n    --- FAIL: TestDockerfileScriptDownload/curl_|_sh (0.01s)\n[202](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:202)\n    --- FAIL: TestDockerfileScriptDownload/aws_file (0.01s)\n[203](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:203)\n    --- FAIL: TestDockerfileScriptDownload/pkg_managers (0.02s)\n[204](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:204)\n    --- FAIL: TestDockerfileScriptDownload/gsutil_file (0.01s)\n[205](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:205)\n    --- FAIL: TestDockerfileScriptDownload/wget_file (0.01s)\n[206](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:206)\n    --- FAIL: TestDockerfileScriptDownload/proc_substitution (0.01s)\n[207](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:207)\n    --- FAIL: TestDockerfileScriptDownload/download_with_some_python (0.01s)\n[208](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:208)\n    --- FAIL: TestDockerfileScriptDownload/wget_|_/bin/sh (0.00s)\n[209](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:209)\n    --- FAIL: TestDockerfileScriptDownload/curl_file_sh (0.01s)\n[210](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:210)\n--- FAIL: TestDockerfilePinningFromLineNumber (0.00s)\n[211](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:211)\n    --- FAIL: TestDockerfilePinningFromLineNumber/Non-pinned_dockerfile (0.00s)\n[212](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:212)\n        pinned_dependencies_test.go:475: test failed: log message not present: [{snippet:FROM python:3.7 startLine:17 endLine:17}]\n[213](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:213)\n    --- FAIL: TestDockerfilePinningFromLineNumber/Non-pinned_dockerfile_as (0.00s)\n[214](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:214)\n        pinned_dependencies_test.go:475: test failed: log message not present: [{snippet:FROM python:3.7 as build startLine:17 endLine:17} {snippet:FROM build startLine:23 endLine:23}]\n[215](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:215)\n        pinned_dependencies_test.go:475: test failed: log message not present: [{snippet:FROM python:3.7 as build startLine:17 endLine:17} {snippet:FROM build startLine:23 endLine:23}]\n[216](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:216)\nFAIL\n[217](https://github.com/ossf/scorecard/runs/5513098935?check_suite_focus=true#step:5:217)\ncoverage: 78.8% of statements\n\n      ",
				"issue_comment": "\nBUG -  Upgrade to buildkit 0.10.0 fails because of Unit test failures"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1709",
				"issue_name": [
					"Feature: use ",
					" "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Proposing that we use go-git for powering RepoClient APIs that require git-like features. Some advantages I see of using this:\n\nA common implementation for ListFiles, GetFileContent, ListCommits and Search that can be used across any Git-like system (GitHub, GitLab or even local git repos).\nEnables us to add support for things like tags (e.g run scorecard on --commit=v4.1.0)\nMight provide better performance since we won't be uncompressing a tarball and some added API token efficiency.\n\nConsidering to start work on this (not a top priority for me, will work on it on the side). Just wanted to run it by folks to make sure no one has concerns. @laurentsimon @naveensrinivasan @justaugustus\n      ",
				"issue_comment": "Proposing that we use   for powering   APIs that require git-like features. Some advantages I see of using this: \n \n \n \n Considering to start work on this (not a top priority for me, will work on it on the side). Just wanted to run it by folks to make sure no one has concerns.       -- What would you say is the primary motivation for   vs  ? Is it: A common implementation ... that can be used across any Git-like system (GitHub, GitLab or even local git repos). Are you thinking something like this? Suggested in  :  -- my team in Kubernetes Release Engineering maintains a GitHub package, which is pretty well-tested and would allow you to offload this. Could you look around and see if some of this fits your needs? What would you say is the primary motivation for   vs  ? Not really   vs.  , but rather using   vs. our custom logic for handling git data. Something along these lines: The implementations can   to use these if they prefer, but its not enforced at the code-level. no concerns on my side, looks like a good idea, so long it's well maintained (both seem to be the case) The implementations can   to use these if they prefer, but its not enforced at the code-level.  -- Okay, cool! We're saying the same thing, modulo impl details like file locations. Organizationally, what you have is better i.e.,  The only thing I'd nit about keeping from is:  should be an   for a few reasons: \n \n \n \n I'd call this part of the scorecard API work:  \n \n Could you expand on that? What's the usecase for making this an interface? Note that there is no caller which passes   to a callee (the usual pattern for needing an interface). Did you mean something like this?  -- Just as an example, consider functional options: ^^ in the above, any type that satisfies the   interface can now also take advantage of  Concrete example: \n \n \n I kind of see what you are driving at. Let me start coding this up, it'll probably help me untangle this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1707",
				"issue_name": [
					"Extend Vulnerabilities check with https://github.com/github/advisory-database"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScorecard does vulnerability scan with osv.dev. Recently GitHub OSS their Vulns https://github.com/github/advisory-database. It would be nice if scorecard can check with this DB for reporting any issues.\n      ",
				"issue_comment": "\nScorecard does vulnerability scan with osv.dev. Recently GitHub OSS their Vulns  . It would be nice if scorecard can check with this DB for reporting any issues. Related "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1706",
				"issue_name": [
					"Feature: Is there a way to check score based on risk levels"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\nDescribe the solution you'd like\nA clear and concise description of what you want to happen.\nDescribe alternatives you've considered\nA clear and concise description of any alternative solutions or features you've considered.\nAdditional context\nAdd any other context or screenshots about the feature request here.\n      ",
				"issue_comment": "\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...] \nA clear and concise description of what you want to happen. \nA clear and concise description of any alternative solutions or features you've considered. \nAdd any other context or screenshots about the feature request here. Is there a way to get score based on risk levels. (HIGH, MEDIUM, and LOW) ex: scorecard --repo=  --checks Branch-Protection --show-details for HIGH risk only Thanks for the issue, and sorry for the late reply! Unfortunately today there is no way to filter results by score. But this sounds like a useful feature to have! Our current implementation assigns checks a risk, and we don't distinguish between scores. However, we have this on our roadmap for this year. Probably Q4, though. There's a similar issue  We'll post updates as we get closer to getting this feature landed. Thanks!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1693",
				"issue_name": [
					"Command line unit test"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          There used to be an issue about this, but I cannot find it. Creating a new one.\nWe'd like to avoid problems like #1691 and #1690\ncc @naveensrinivasan\n      ",
				"issue_comment": "There used to be an issue about this, but I cannot find it. Creating a new one. \nWe'd like to avoid problems like   and  cc "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1683",
				"issue_name": [
					"Feature: Create a scorecard API"
				],
				"issue_label": [
					"core feature",
					"critical",
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\nDescribe the solution you'd like\nA clear and concise description of what you want to happen.\n\n #1709\n\nTracking requests based on consumers:\n\nscorecard-action: ossf/scorecard-action#107\n\n #1696\n ...\n\n\nallstar: ossf/allstar#21\n\n ...\n ...\n\n\nwitness: \n\n Public JSON Type: #1673\n ...\n\n\n\nDescribe alternatives you've considered\nA clear and concise description of any alternative solutions or features you've considered.\nAdditional context\nFrom @justaugustus in #1680 (comment):\n\n\nI'm concerned about directing users here while we don't officially yet support Scorecard as a library. Apart from the LGTM.\n\nI think we should start supporting it. There are projects using it.\n\nThis presentation is worth a look: https://dave.cheney.net/practical-go/presentations/gophercon-israel.html#apidesign\nOf note:\n\nIf it’s exported, its part of your public API.\n\nThat said, we have a responsibility to design APIs that are hard to misuse.\n\n* Scorecard-actions (this is WIP)\n\n\nossf/scorecard-action#107 (cc: @rohankh532)\n\n* Witness\n\n\n#1673 / #1245 / #1682 (cc: @colek42)\n\n* Allstar\n\n\n#1645 / ossf/allstar#114\n#1645 just merged, which makes some decent steps in the right direction and I'll open a separate tracking issue for this if one doesn't already exist.\n\nWe should document that we don't support Semver but scorecard/v4 similar to Go GitHub Libary.\n\nDisagree here and I've expressed the opinion in a few places, including the most recent (2022-02-24) biweekly meeting.\nWe've adopted SemVer-compliant versioning, which means regardless if we've enforce SemVer or not, consumers will have at least a baseline expectation that we are compliant. Whether that is true or not is a different story.\nI think we should instead declare that the next major release version (v5.0.0) is the first release in which we will be SemVer-compliant and direct our efforts into designing the module in a way that we can ensure that.\nI think we'll all agree that scorecard is a high-value project in a space with lots of eyes on it today, so we have a responsibility to those who consume it to make it as easy and clear to do so as possible.\ncc: @ossf/scorecard-maintainers\n      ",
				"issue_comment": "\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...] \nA clear and concise description of what you want to happen. Tracking requests based on consumers: \n \n \n \n \nA clear and concise description of any alternative solutions or features you've considered. From   in  : I'm concerned about directing users here while we don't officially yet support Scorecard as a library. Apart from the LGTM. I think we should start supporting it. There are projects using it. This presentation is worth a look:  Of note: If it’s exported, its part of your public API. That said, we have a responsibility to  .  (cc:  )  /   /   (cc:  )  /   just merged, which makes some decent steps in the right direction and I'll open a separate tracking issue for this if one doesn't already exist. We should document that we don't support Semver but   similar to Go GitHub Libary. Disagree here and I've expressed the opinion in a  , including the most recent (2022-02-24)  . We've adopted SemVer-compliant versioning, which means regardless if we've enforce SemVer or not, consumers will have at least a baseline expectation that we are compliant. Whether that is true or not is a different story. I think we should instead declare that the next major release version ( ) is the first release in which we will be SemVer-compliant and direct our efforts into designing the module in a way that we can ensure that. I think we'll all agree that scorecard is a high-value project in a space with lots of eyes on it today, so we have a responsibility to those who consume it to make it as easy and clear to do so as possible. cc:  I think we'll all agree that scorecard is a high-value project in a space with lots of eyes on it today, so we have a responsibility to those who consume it to make it as easy and clear to do so as possible. +1. I think we should instead declare that the next major release version ( ) is the first release in which we will be SemVer-compliant and direct our efforts into designing the module in a way that we can ensure that. I would absolutely   to have this accomplished in v5. This might require a significant amount of thought and effort though.   do you think we can accomplish this by v5 timeline (end of Q2 - early Q3)? If so, a big +1 on this and let us know how we can help contribute here. This might require a significant amount of thought and effort though.   do you think we can accomplish this by v5 timeline (end of Q2 - early Q3)? For whatever reason, I often prefer to attack a problem by brute-forcing usage, instead of immediately trying to write a design doc (especially if the code is \"newer\"/has less guarantees/restrictions and doing the work will outstrip the value). Our closest consumers will help refine the API. With that in mind, I've already done the following: \n \n \n I'm also taking a look at the scorecard-action:  \nOne of the repeated requests on that issue was \"remove the env vars\" and some of that is already possible with the new   package:  If so, a big +1 on this and let us know how we can help contribute here. Absolutely! Will keep you posted :) In witness we can simplify the UX around attesting scorecards with a public API.  We are looing forward to this feature. In witness we can simplify the UX around attesting scorecards with a public API. We are looking forward to this feature. That's awesome,  ! \nCould you say more about what specifically would be helpful to have exposed? We would want to embed the scorecard into an attestor so the user would not have to maintain it is a separate tool. We would want to embed the scorecard into an attestor so the user would not have to maintain it is a separate tool.  -- Given   is cobra, this is maybe useful -->  \nExample of what I'm working on in   to wrap/remix the scorecard  :   /  Am I correct in saying that what we want long-term is really the logic of  \n Pushed into  \n ??? (Because that's what I was considering working on next :))"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1677",
				"issue_name": [
					"Use Kubernetes ",
					" tool for releases"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Part of #1676\nref: https://github.com/kubernetes/release/blob/master/cmd/release-notes/README.md\nIs your feature request related to a problem? Please describe.\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\nDescribe the solution you'd like\nA clear and concise description of what you want to happen.\nDescribe alternatives you've considered\nA clear and concise description of any alternative solutions or features you've considered.\nAdditional context\nAdd any other context or screenshots about the feature request here.\n      ",
				"issue_comment": "Part of  ref:  \nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...] \nA clear and concise description of what you want to happen. \nA clear and concise description of any alternative solutions or features you've considered. \nAdd any other context or screenshots about the feature request here."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1676",
				"issue_name": [
					"Document scorecard release processes"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\nDescribe the solution you'd like\nA clear and concise description of what you want to happen.\nDescribe alternatives you've considered\nA clear and concise description of any alternative solutions or features you've considered.\nAdditional context\nAdd any other context or screenshots about the feature request here.\n      ",
				"issue_comment": "\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...] \nA clear and concise description of what you want to happen. \nA clear and concise description of any alternative solutions or features you've considered. \nAdd any other context or screenshots about the feature request here. I believe this is also needed for OpenSSF Best Practices badging: "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1667",
				"issue_name": [
					"Reduce the number of permissions we look for"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          In the token permission check, we look for risky permissions like contents:write or packages:write, but also for less risky ones like secevents, checks, status, see https://github.com/ossf/scorecard/blob/main/checks/permissions.go#L279\nFor the latter permissions ,they are low risk and not necessarily actionable because there are many actions that require access to them. I wonder whether we should report those or not.\nAdded this to discussion for next meeting.\ncc @azeemsgoogle @naveensrinivasan @justaugustus @chrismcgehee\n      ",
				"issue_comment": "In the token permission check, we look for risky permissions like   or  , but also for less risky ones like  ,  ,  , see  For the latter permissions ,they are low risk and not necessarily actionable because there are many actions that require access to them. I wonder whether we should report those or not. Added this to discussion for next meeting. cc       "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1652",
				"issue_name": [
					"Yarn lock support"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          see command https://yarnpkg.com/cli/install\n      ",
				"issue_comment": "see command "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1648",
				"issue_name": [
					"Move to cron to a separate repository"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The https://github.com/ossf/scorecard/tree/main/cron has multiple binaries.\n\nThis makes the build complex and increases the duration to complete\nThe module dependency is pretty heavy https://github.com/ossf/scorecard/blob/main/go.mod\nBy refactoring, should reduce the binary-size\n\nThe downsides are\n\nThe change in scorecard has to be done separately prior to any changes to cron in 2 different PR's.\nMaintaining another set of dependabot updates\n\n      ",
				"issue_comment": "The   has multiple binaries. \n \n \n \n The downsides are \n \n \n Although a good idea, there are a bunch of complications that come with this. Today, we use cron job as an e2e testing framework. To maintain that, the 2 repos   and   will need to be tightly coupled. Also, the extra maintenance work that comes with a new repository like you mentioned. My vote is to punt on this until we have dedicated resources willing to maintain   in the long run."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1626",
				"issue_name": [
					"Create multiple SARIF results for each branch protection settings"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          the branch protection results are not granular enough in SARIF. If users dismisses them once (say, they don't care about 2 reviews and are fine with 1 review), they will never show again even if a setting is changed to 0 reviews\nWe're tackling this more broadly through the raw results effort #1245, but it will take time to land all the checks.\nA temporary solution for branch protection would be useful\n      ",
				"issue_comment": "the branch protection results are not granular enough in SARIF. If users dismisses them once (say, they don't care about   and are fine with  ), they will never show again even if a setting is changed to  We're tackling this more broadly through the raw results effort  , but it will take time to land all the checks. A temporary solution for branch protection would be useful I'll give it a shot to see if it's feasible without too much effort"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1618",
				"issue_name": [
					"Improve scorecard score for scorecard repo"
				],
				"issue_label": [
					"bug",
					"priority"
				],
				"issue_content": "\n          Describe the bug\nA clear and concise description of what the bug is.\nReproduction steps\nhttps://deps.dev/go/github.com%2Fossf%2Fscorecard\nAre these false positives? If not, please fix. Also, for false positives, is there any easy way to add exceptions/ignore-lists [this is important as these workflows issues probably happen on other criticial repos as well]\nExpected behavior\n10/10 score for scorecard repo.\nAdditional context\nAdd any other context about the problem here.\n\n\n Branch protection: #1630\n OpenSSF Best Practices badge: #1629\n Dangerous workflow:\n Fuzzing: google/oss-fuzz#7268\n Token permissions:\n\n      ",
				"issue_comment": "\nA clear and concise description of what the bug is. \n \nAre these false positives? If not, please fix. Also, for false positives, is there any easy way to add exceptions/ignore-lists [this is important as these workflows issues probably happen on other criticial repos as well] \n10/10 score for scorecard repo. \nAdd any other context about the problem here. Here is the latest release  Sounds fun! \n/assign Local run against  : RESULTS Aggregate score: 8.0 / 10 Check scores: re: dangerous workflow. There's a false positive tracked in   that   is working on \nre: token permissions: we currently remove 0.5-1 points for certain low-risk permissions   (this is why scorecard has score of 8). This is something I've been thinking of removing but have not had the chance to ask folks' opinion at the meeting yet why do we need   in goreleaser  ? Why is this not  \nFyi, current packaging check does not distinguish between contents/packages  why do we need   in goreleaser  ? Why is this not  \nFyi, current packaging check does not distinguish between contents/packages  goreleaser writes to releases ( ) which needs  . In fact if a publishing workflow did not write a   file to releases, the signed release check might fail. May be the signed release check should check for signature info in the package repository (e.g. docker registry). On another note, I think that regardless of where the artifact is published, it would be good to standardize creating a tag for the release version. So if one is trying to build from source, one can use the tag to get the right commit to build the right version. Just thinking aloud... Thanks   I realized after posting the question that to push a release, we need  . Thanks for confirming! I think there are APIs to check which tag corresponds to which commit/branch. Do you think this is insufficient? Thanks   I realized after posting the question that to push a release, we need  . Thanks for confirming! I think there are APIs to check which tag corresponds to which commit/branch. Do you think this is insufficient? What I was thinking about might be out of scope for scorecards. Sometimes maintainers publish a release to artifact repository, e.g.  , but forget to create a tag. I noticed   has version 9.9.0 but no tag for that in the repo:  . I don't think scorecards can detect this though...may be some related project? do you think SLSA provenance is something that could address this problem? do you think SLSA provenance is something that could address this problem? I think so. In this case, even if the registry owner could verify if the tag exists in the associated repo, and reject the publish event if it doesn't, it will solve the problem. May be I can submit a request to the   owners... For the tokens, I submitted  . I did notice though that the logic seems to check the yml files, but does not check the default permissions granted to Github actions, so it raises issues when the top level action does not have   on   explicitly, though if nothing is specified that is what it would get. This is my first PR to the project so I'll go read the guidelines and be sure my submission is correct - I just happened to be fixing the same warnings in another repo."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1614",
				"issue_name": [
					"BUG number of required reviewers is only 0 alert even though is set to 1"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nflutter/flutter has branch protection enabled and required reviewers to 1 but scorecards still show an alert with a description that is set to 0\nReproduction steps\nSteps to reproduce the behavior:\n\nSet branch protection of a repository\nSet required reviewers to 1, add a group that is exempt from the rule(this is used for bot accounts)\nRun scorecards\n\nExpected behavior\nIf reviewers is set to >1 the alert should not be triggered\nAdditional context\nThe token we are using to run scorecards with doesn't have admin access. I wonder if this a permissions problem.\n      ",
				"issue_comment": "\nflutter/flutter has branch protection enabled and required reviewers to 1 but scorecards still show an alert with a description that is set to 0 \nSteps to reproduce the behavior: \n \n \n \n \nIf reviewers is set to >1 the alert should not be triggered \nThe token we are using to run scorecards with doesn't have admin access. I wonder if this a permissions problem. Thanks for reporting! Can you give a link about the   setting? I ran scorecard and got the following (with my personal token): Do you see something different in your results/dashboard? follow-up: you're right, your latest action run has: \n Can you run scorecard yourself with your token via command line using our docker image as explained in   and copy the result here?  Additionally, please run   and past the results Fyi, I get the following with my own token Thanks for your help! Output of   : Output of  : This other option is checked in that group: Very interesting! Which scopes do you use for your PAT? It has the following scopes:  I could not reproduce the problem with my PAT. Let's try something else first. Can you head over to  , log in as you, and run this graphQl command and paste your results? Mines look like: I think I know what is happening: The PAT was generated from an account which belongs to the group allowed to bypass pullrequest requirements. This is the output: The result of running with an account that does not belong to group allowed to bypass the codereview requirements is: I thought the APIs were returning the data in the same structure as the UI but it seems like it returns them calculated for the user making the request. I believe the fix for this issue is to create the PAT using an account that does not belong to any special group and maybe adding a note to the documentation. E.g. for flutter we want to validate the permissions from the flutter-hackers point of view. Good catch. That's a really interesting behavior.     is this expected behavior? I think we also need to retrieve the settings \"Allowed specified actors...\" so it can be reported and validated, otherwise we are missing important info about the repo itself. Very interesting find, thanks  ! I think we need help from the GitHub team here. Will follow up with them and update here."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1606",
				"issue_name": [
					"Detect more unpinned golang commands"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          There are several commands https://go.dev/ref/mod that update the go.mod/go.sum and may patch the sum file.\nWe should detect them in the Pinned-Dependencies check.\n      ",
				"issue_comment": "There are several commands   that update the go.mod/go.sum and may patch the sum file. We should detect them in the Pinned-Dependencies check. other examples "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1597",
				"issue_name": [
					"Feature - Pin dependencies - add support for gclient DEPS file"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nYes, dart and flutter use gclient from chromium to manage third party dependencies. We would like to add Pin dependencies support for this type of files.\nDescribe the solution you'd like\nUnfortunately parsing DEPS file without python may be challenging. We would like to run python in a subprocess to flatten up the third party dependencies from DEPS to identify dependencies that are not pinned.\nDescribe alternatives you've considered\nAn alternative would be to try to parse the file directly from go but it would be very difficult to cover all possible corner cases.\nAdditional context\nN/A\n      ",
				"issue_comment": "\nYes, dart and flutter use   from chromium to manage third party dependencies. We would like to add   support for this type of files. \nUnfortunately parsing   file without python may be challenging. We would like to run python in a subprocess to flatten up the third party dependencies from DEPS to identify dependencies that are not pinned. \nAn alternative would be to try to parse the file directly from   but it would be very difficult to cover all possible corner cases. \nN/A Thanks for reporting this  . I wonder if a Python parser like   can be used here? If the code complexity of these   files tends to be fairly simple we could get away with not being able to handle all corner cases. I have added the   label to this issue so that we can bring this up during the Scorecard sync next week. If possible please do drop by so that we can get your inputs on this. You can find the details to the meet and our Slack channel here:  one way we get around this problem for other package managers is that we search commands that fail unless the dependencies are pinned. For example for pip, we check for use of  .; and we warn otherwise. Is there something similar for gclient? gclient does not support checking for pinned dependencies but I can try to contact the owners and see if there is interest on implementing that functionality. Alternatively we can autogenerate a flattened DEPs file and only check for the pinned dependencies from scorecards using a plain text file with one dependency per line. gclient does not support checking for pinned dependencies but I can try to contact the owners and see if there is interest on implementing that functionality. that'd be really helpful. Let us know if there's something we can help with. Alternatively we can autogenerate a flattened DEPs file and only check for the pinned dependencies from scorecards using a plain text file with one dependency per line. what do you mean by autogenerate? gclient does not support checking for pinned dependencies but I can try to contact the owners and see if there is interest on implementing that functionality. that'd be really helpful. Let us know if there's something we can help with. Alternatively we can autogenerate a flattened DEPs file and only check for the pinned dependencies from scorecards using a plain text file with one dependency per line. what do you mean by autogenerate? gclient supports hooks that run as part of the source code checkout process, here is an  . We can add an additional hook that writes a file with the list of dependencies. This will be committed to the repository as DEPS_list,   can read files called DEPS_list and validate them using regular expressions. pros: \n \n cons: \n \n Alternatively we can autogenerate a flattened DEPs file and only check for the pinned dependencies from scorecards using a plain text file with one dependency per line. I presume this is not a norm among the clients of  ? If so, it might be a bit hacky to do this. How many repos would benefit from something like this? If its a reasonable number, we could consider this as a first step while working out a more general solution.  you are correct generating the flattened file is a work around and it won't be generic. I've been thinking on other potential solutions and I believe I found one to make the DEPS implementation generic. The overall workflow would look like: \n \n \n \n Some caveats given that the DEPS file is being parsed we won't be able to correlate failures with specific lines. Please let me know what do you think about this potential solution. Update from the WG discussion: \n \n \n \n As our next step here, consider starting with (i) as it is simple & secure and can help Chromium, Flutter + Dart teams. Meanwhile   to send a Python script for (ii) which Scorecard team will review and consider moving to (ii) long-term to make this a generic solution. Is there any update on this thread? I finished implementing the python script part.  Here is the PR: I'll send the validation of pinned dependencies to scorecard the next week. I finished implementing the python script part. Here is the PR: I'll send the validation of pinned dependencies to scorecard the next week. Is it going to be in python? We can't run Python. The parser step in the actions workflow will generate a file called \"deps_flatten.txt\" and from scorecards side I was planning to just scan for deps_flatten.txt files and then run all the pinning analysis and report extending one of the existing pin checks. Would that work or do you mean that it is not recommended to run python as part of the scorecards github actions workflow in the target repository? The parser step in the actions workflow will generate a file called \"deps_flatten.txt\" and from scorecards side I was planning to just scan for deps_flatten.txt files and then run all the pinning analysis and report extending one of the existing pin checks. Sorry, this is still not clear. Is   scorecard actions? Would that work or do you mean that it is not recommended to run python as part of the scorecards github actions workflow in the target repository? Do you want to run Python as part of scorecard actions? Probably a design document/diagram can help visualize and clarify the understanding. The parser step in the actions workflow will generate a file called \"deps_flatten.txt\" and from scorecards side I was planning to just scan for deps_flatten.txt files and then run all the pinning analysis and report extending one of the existing pin checks. Sorry, this is still not clear. Is   scorecard actions? Using   as an example   is the workflow that will be running the python script to generate the   in the local checkout before the scorecard actions run. Would that work or do you mean that it is not recommended to run python as part of the scorecards github actions workflow in the target repository? Do you want to run Python as part of scorecard actions? No, python will run as part of the workflow that runs the scorecard actions in the repository under test. Probably a design document/diagram can help visualize and clarify the understanding. Sure, please let me know if after the explanations above a design doc will still be required and I'll be happy to write it. Sounds good. Thanks. Is there any work that would be required from the scorecard project with respect to this? I believe only code reviews will be needed, I'm planning to send the PR."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1585",
				"issue_name": [
					"SAST - Recognize Clang Tidy as a SAST tool"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nDart and Flutter already use clang tidy to run several checks and perform static analysis and we would like to integrate their results with score cards.\nDescribe the solution you'd like\nDart and Flutter LUCI builders run clang tidy on every commit blocking the PR on failures. We would like to add clang-tidy to the list of supported SAST tools to pass the SAST checks.\nDescribe alternatives you've considered\nWe ran codeql manually but given the complexity of flutter/engine build system. A single iteration took +4 hours making it impossible to run on every commit.\nAdditional context\nN/A\n      ",
				"issue_comment": "\nDart and Flutter already use clang tidy to run several checks and perform static analysis and we would like to integrate their results with score cards. \nDart and Flutter LUCI builders run   on every commit blocking the PR on failures. We would like to add clang-tidy to the   to pass the SAST checks. \nWe ran codeql manually but given the complexity of flutter/engine build system. A single iteration took +4 hours making it impossible to run on every commit. \nN/A Thanks for the report. You run clang tidy as a a command or use an action that wraps it? I suppose the former, but would like to confirm. Do you use a linter as well? clang-format or another tool? We currently run it as a command inside our Android and iOS builders, but we are planning to separate them to their own builders. One thing I noticed is that the SAST tool check is validating the exact github check name which may not work for dart and flutter if we have  ,  , etc. Here are examples of our current clang-tidy executions: \n \n \n once we have   landed, I'll add support for this issue."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1580",
				"issue_name": [
					"SAST false positive: CodeQL steps in main workflow not detected"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          My current build workflow clearly uses CodeQL steps:\n      # Initializes the CodeQL tools for scanning.\n      - name: Initialize CodeQL\n        uses: github/codeql-action/init@1a927e9307bc11970b2c679922ebc4d03a5bd980 # v1\n        with:\n          languages: ${{ matrix.language }}\n\n      - name: Perform CodeQL Analysis\n        uses: github/codeql-action/analyze@1a927e9307bc11970b2c679922ebc4d03a5bd980 # v1\nbut this is not being detected:\n\nSAST tool detected but not run on all commmits:  \nWarn: 0 commits out of 6 are checked with a SAST tool\n\n\nPerhaps the check needs to be updated?\nTo be explicit: PRs require the CodeQL checks to pass before they can be merged.\nPossible cause: I removed the github/codeql-action/autobuild action as my repository doesn't require a compilation step.\n      ",
				"issue_comment": "My   clearly uses CodeQL steps: but this is  : Perhaps the check needs to be updated? To be explicit: PRs require the CodeQL checks to pass before they can be merged. Possible cause: I removed the   action as my repository doesn't require a compilation step. I ran scorecard on your repo now and got: Until you get all last 30 commits checked with CodeQL, you'll see the alert. Once the last 30 commits have CodeQL run on them, the alert will disappear. Is the message   what's causing confusion? Shall it be updated to  ? The '1 commit' is there because I added LGTM later on. The other 6 commits before that are   covered by CodeQL. The issue is that all 7 commits are covered by a SAST tool, not just the last one. Looking over the SAST step, I see it relies on Github search to detect workflows using the   action. Unfortunately, search is unreliable, e.g. the specific search  , even though I  . At fault is the   in the search text, which GitHub search seems to trip over, a  . Perhaps, instead of using code search, the plugin should load the workflow definition for each run attached to a PR (using the head_ref to get the right revision) and check if there is a   line in the yaml definition for the action? Good catch. This will be fixed in   which parses the action's yaml instead of using the search API."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1578",
				"issue_name": [
					"Add support for NuGet"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The scorecard project currently only supports npm, golang, and pip as far as I could tell. I'm a PM on the NuGet team at Microsoft and would love to help contribute adding support for NuGet in this tool or providing the right guidance to implement support for NuGet. This closely aligns with a proposal I had last year and would love to experiment with this scorecard in .NET:\ndotnet/designs#216\nPlease feel free to reach out to us over at NuGet/Home on GitHub or in this issue. Any steps on how to best contribute adding this support would be greatly appreciated!\n      ",
				"issue_comment": "The scorecard project currently only supports npm, golang, and pip as far as I could tell. I'm a PM on the NuGet team at Microsoft and would love to help contribute adding support for NuGet in this tool or providing the right guidance to implement support for NuGet. This closely aligns with a proposal I had last year and would love to experiment with this scorecard in .NET: Please feel free to reach out to us over at NuGet/Home on GitHub or in this issue. Any steps on how to best contribute adding this support would be greatly appreciated! It would be great to add support for NuGet. Here is an example of how we are looking for  \n PR's are welcome!  that's great, thanks for reaching out!  pointed you to the file where we parse commands and look for unpinned go, pip and npm commands. \nThis is the place to add it. Shameless plug: as part of the OSSF Best Practice WG, we have a work stream on package manager's best security practices. We've started with npm and pip. If you're interested in collaborating - or even lead! - a similar effort for NuGet, please let me know. An idea we've been kicking around is to work with package managers and help them display scorecard information on their website. Would you imagine providing scorecard results on the NuGet's hub//website? I'd love to discuss this direction if it's of interest. You're more than welcome to attend our bi-weekly Thu scorecard meetings! an additional place to add support for NuGet is in the Packaging check  , which tries to infer if the project published a package. We typically look for known GitHub actions and commands to detect this.  Let's discuss this further as it's very in-line with some areas I'm investigating and would love to join a scorecard meeting. Where can I get involved for those bi-weekly meetings? Please see   for details about joining the bi-weekly. Bi-weekly's happen Thursdays 1-2pm PST (next instance is on Feb 24th). Also feel free to add an agenda item to the upcoming meet in this  . You'll need to join   for being able to modify the doc. Looking forward to meeting you @ ! \nThanks   for the prompt info! Hi all, I had originally planned to join the meeting later this afternoon, but unfortunately I have a conflict I cannot clear this month. I'll be looking to join the next one on the 10th instead. Thanks for letting us know. See you in 2 weeks then!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1572",
				"issue_name": [
					"BUG - Pinned-Dependencies should not fire multiple times on multi-stage Dockerfile builds"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nExample alert: Pinned-Dependencies\n\nThe alert is telling me that this is an unpinned dependency:\nFROM base AS test-nonroot\n\n...when in reality it's a multi-stage Dockerfile build  and the base that's referred to is a previous stage defined earlier in the file:\nFROM --platform=$BUILDPLATFORM golang:${GO_VERSION}-alpine AS base\n\n(link)\nThere's no way to refer to this previous step by any pinned digest, since subsequent steps can't reproducibly know what previous steps produce, and AFAIK Dockerfile parsing doesn't support FROM base@sha256:... anyway, if base refers to a previous build stage.\nReproduction steps\nSteps to reproduce the behavior:\n\nInclude a multi-stage Dockerfile build in a repo\nScan\nSee Pinned-Dependencies alert\n\nExpected behavior\nI would not expect this FROM line to trigger a scan alert.\nAdditional context\nFROM line parsing can be somewhat ambiguous:\n\nFROM ubuntu is an unpinned image dep (ubuntu image exists)\nFROM ubuntu@sha256:... AS ubuntu and a later FROM ubuntu might indicate a new build stage based on the previous (pinned-image) stage, or it might not. Dockerfile parsing might even fail in this case, I'm not sure.\n\n      ",
				"issue_comment": "Example alert:  The alert is telling me that this is an unpinned dependency: ...when in reality it's a   and the   that's referred to is a previous stage defined earlier in the file: ( ) There's no way to refer to this previous step by any pinned digest, since subsequent steps can't reproducibly know what previous steps produce, and AFAIK Dockerfile parsing doesn't support   anyway, if   refers to a previous build stage. \nSteps to reproduce the behavior: \n \n \n \n I would not expect this   line to trigger a scan alert.  line parsing can be somewhat ambiguous: \n \n \n Thanks for the report The alert is telling me that this is an unpinned dependency: ...when in reality it's a   and the   that's referred to is a previous stage defined earlier in the file: ( ) in general, my understanding is that   means   refers to  . Since   is not pinned by hash, we consider base as not pinned. Can you explain where the reasoning above is wrong? I think I missed something. Thanks for your patience. cc   since   is not pinned, Scorecard repeatedly complains for every line in which   is reused in the Dockerfile. I tested locally that pinning   to a SHA, removes the errors on following lines. So this is actually expected behavior. Hope that makes sense. Oh interesting! Thanks for that insight. Agree this is less urgent since pinning the initial dep fixes all the alerts. I do wonder whether it's working as intended that the unpinned dep generates so many duplicate alerts, but I'm fine ignoring it and just fixing the initial unpinned dep.  Oh interesting! Thanks for that insight. Agree this is less urgent since pinning the initial dep fixes all the alerts. I do wonder whether it's working as intended that the unpinned dep generates so many duplicate alerts, but I'm fine ignoring it and just fixing the initial unpinned dep.  please create another issue for it. I see your point that we reducing the number of alerts would be beneficial. Not sure it's feasible but worth an issue. I'll just reopen since this issue already has useful context. I updated the title to reflect the current issue. Thanks for looking into this! "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1563",
				"issue_name": [
					"Add branch protection settings"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We currently do not expose the following settings: requiresConversationResolution, requiresSignatures, viewerAllowedToDismissReviews, viewerCanPush\nWe may add them\n      ",
				"issue_comment": "We currently do not expose the following settings: requiresConversationResolution, requiresSignatures, viewerAllowedToDismissReviews, viewerCanPush We may add them"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1559",
				"issue_name": [
					"support for various package managers"
				],
				"issue_label": [],
				"issue_content": "\n          Thanks. Can you add a note to add support for various package managers + create a tracking issue for each? (I realize this is missing) Today we support golang, npm and pip only (shell_download_validate.go)\nOriginally posted by @laurentsimon in #1557 (review)\n      ",
				"issue_comment": "Thanks. Can you add a note to add support for various package managers + create a tracking issue for each? (I realize this is missing) Today we support golang, npm and pip only (shell_download_validate.go) Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1554",
				"issue_name": [
					"Feature: Check that CODEOWNERS is up to date"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nCode owners can be required to approve PRs before merging into a protected branch. Maintainers may rely on code owners to validate the contents of the PRs, so code owners needs to be trusted. Over time, it's easy to continually grow the CODEOWNERS file without keeping it up to date.\nDescribe the solution you'd like\nI propose a scorecard check that the individuals in CODEOWNERS are active contributors. This is especially important for global code owners that can approve code in the entire repository. A threshold may be necessary for handling code owners that only maintain a small subset of files. For example, they may not be an active contributor, but may still be relied on when changes are needed.\nI propose that we also check that file paths listed in CODEOWNERS are actually present in the repo. For example, if we delete a directory such as /apps/, then there shouldn't be an owner for this path. If this were not removed and later /apps/ was added back, it may be undesired that the original code owner maintain ownership over that path.\n      ",
				"issue_comment": "\nCode owners can be required to approve PRs before merging into a protected branch. Maintainers may rely on code owners to validate the contents of the PRs, so code owners needs to be trusted. Over time, it's easy to continually grow the CODEOWNERS file without keeping it up to date. \nI propose a scorecard check that the individuals in CODEOWNERS are active contributors. This is especially important for global code owners that can approve code in the entire repository. A threshold may be necessary for handling code owners that only maintain a small subset of files. For example, they may not be an active contributor, but may still be relied on when changes are needed. I propose that we also check that file paths listed in CODEOWNERS are actually present in the repo. For example, if we delete a directory such as  , then there shouldn't be an owner for this path. If this were not removed and later   was added back, it may be undesired that the original code owner maintain ownership over that path."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1553",
				"issue_name": [
					"[UMBRELLA] Review/expand contribution guidance"
				],
				"issue_label": [
					"documentation",
					"enhancement"
				],
				"issue_content": "\n          I've left a few reviews/comments here and there on PRs and it's worth reviewing contribution guidelines, just to ensure everyone has the tools they need to effectively contribute to the project.\nTo be clear, we've got a lot of great documentation today.\nLet's see if we can get even better!\nEventually, this could be abstracted as general guidance for all OpenSSF projects, similar to https://github.com/kubernetes/community.\nFor current and potential contributors/maintainers, please feel free to leave comments about what kind of doc improvements you'd like to see.\n\n Contributor ladder: #1529 / #308\n Contributor review guidance: #1552\n Code signoffs: #1533\n Handling copyright dates: #1534\n Releasing:\n\n Use Kubernetes release-notes tool for releases: #1676\n Document scorecard release processes: #1677\n\n\n\ncc: @azeemshaikh38 @inferno-chromium @justaugustus @laurentsimon @naveensrinivasan @olivekl @david-a-wheeler @jeffmendoza @vmbrasseur\n      ",
				"issue_comment": "I've left a few reviews/comments here and there on PRs and it's worth reviewing contribution guidelines, just to ensure everyone has the tools they need to effectively contribute to the project. To be clear, we've got a lot of great documentation today. \nLet's see if we can get even better! Eventually, this could be abstracted as general guidance for all OpenSSF projects, similar to  . For current and potential contributors/maintainers, please feel free to leave comments about what kind of doc improvements you'd like to see. cc:                  I opened a related   under the /foundation repo; not sure if that's the right place, but maybe has more \"across-all-of-OSSF\" scope? I opened a related   under the /foundation repo; not sure if that's the right place, but maybe has more \"across-all-of-OSSF\" scope? Liked, subscribed, and hit the bell! \nThanks for cross-linking these,  !"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1552",
				"issue_name": [
					"Reviewer/maintainer guidance/expectations"
				],
				"issue_label": [
					"documentation"
				],
				"issue_content": "\n          How can we guide contributors/maintainers in doing code reviews/maintaining the project?\n\n\n\n@laurentsimon -- Another nit to briefly continue the convo from #1532 (comment):\nThe notes you left in the PR description are much clearer!\n\ndocs/checks.md: updated the doc\ndocs/checks/internal/checks.yaml: updated the source of truth for docs\n\nWhat I was suggesting in the previous PR was to make these not the PR description, but the actual commit messages.\nThat way, when the PR content gets squashed and merged, the details get included as part of the git history.\n\nGocha. The info also seems useful for the PR reviewer. So need them in both places?\n\ndo you have a doc on updating a past commit? I can try adding it\nOriginally posted by @laurentsimon in #1545 (comment)\n      ",
				"issue_comment": "How can we guide contributors/maintainers in doing code reviews/maintaining the project?  -- Another nit to briefly continue the convo from  : \nThe notes you left in the PR description are much clearer! docs/checks.md: updated the doc \ndocs/checks/internal/checks.yaml: updated the source of truth for docs What I was suggesting in the previous PR was to make these not the PR description, but the actual commit messages. \nThat way, when the PR content gets squashed and merged, the details get included as part of the git history. Gocha. The info also seems useful for the PR reviewer. So need them in both places? do you have a doc on updating a past commit? I can try adding it"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1550",
				"issue_name": [
					"Add rationales for every criterion"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThere are many criteria, but it's not always clear why those criteria are present. Why should something be pinned (for example)? In particular, sometimes criteria aren't appropriate, but it's hard to determine that without knowing its rationale.\nDescribe the solution you'd like\nProvide for each criteria the rationale.\n      ",
				"issue_comment": "\nThere are many criteria, but it's not always clear why those criteria are present. Why should something be pinned (for example)? In particular, sometimes criteria aren't appropriate, but it's hard to determine that without knowing its rationale. \nProvide for each criteria the rationale. E.g., for pinning, \n\"Pinning prevents a malicious later version (that wasn’t necessarily reviewed) revealing secrets or creating artifacts that end up in a production version, and makes it easier to determine exactly what was changed when a problem is detected.\" This rationale suggests that you don’t need to pin something if an unpinned version cannot reveal secrets AND it cannot change a generated artifact that could be used in production. For example, it might be okay to have a test unpinned - it might try to create cryptocurrency, or produce an unexpected result (e.g.,  \"always passes\"), but its damage would be limited.    assigned it to all of us. Will setup a sync for this in the coming weeks. E.g., for pinning, \"Pinning prevents a malicious later version (that wasn’t necessarily reviewed) revealing secrets or creating artifacts that end up in a production version, and makes it easier to determine exactly what was changed when a problem is detected.\" you wrote most of the doc for this check and it's pretty good already :-) This rationale suggests that you don’t need to pin something if an unpinned version cannot reveal secrets AND it cannot change a generated artifact that could be used in production. For example, it might be okay to have a test unpinned - it might try to create cryptocurrency, or produce an unexpected result (e.g., \"always passes\"), but its damage would be limited. Giving more example is useful for sure. I would recommend using the   as a reference. That has good rationale for pinning of action and token permissions. FYI - I have also created a PR to add reference to scorecard in that guide. Please do share feedback in the PR so that scorecard is easy to discover when one reads the hardening guide. \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1549",
				"issue_name": [
					"Rename \"CII Best Practices Badge\" to \"OpenSSF Best Practices Badge\""
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nRename \"CII Best Practices Badge\" to \"OpenSSF Best Practices Badge\"; the project recently changed its hame.\n      ",
				"issue_comment": "\nRename \"CII Best Practices Badge\" to \"OpenSSF Best Practices Badge\"; the project recently changed its hame.  would you able to tackle this? Can be a P1/P2 item in the current list we have. I can."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1546",
				"issue_name": [
					"AuthorAssociation should be part of User"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We've added fields like AuthorAssociation *RepoAssociation Issue and IssueComment.\nShould these live under User instead?\nIt would be more natural to have:\n// Issue represents a thread like GitHub issue comment thread.\ntype Issue struct {\n\tURI               *string\n\tCreatedAt         *time.Time\n\tCreator         User\n\tComments          []IssueComment\n}\n\n// IssueComment represents a comment on an issue.\ntype IssueComment struct {\n\tCreatedAt         *time.Time\n\tCommenter User\n}\n\ntype User struct{\n   Login string\n  AuthorAssociation *RepoAssociation\n}\n\nThis will also be useful for checks like Contributors, where we can add this field in the raw results.\n      ",
				"issue_comment": "We've added fields like     and  . \nShould these live under   instead? It would be more natural to have: This will also be useful for checks like Contributors, where we can add this field in the raw results. cc     wdut? Yeah, I think that would work, as long as we put a comment on the User struct to make clear that it's for a user in relation to a specific repository, and not a generic user object. If Azeem agrees on the approach, I can take this one. Hmm, I prefer the way it is today. It matches the GitHub graphQL objects -  \n I haven't tested to see if the same author can appear with different   for different objects. But if that is the case, it might be beneficial to keep it as is. But I see why we might want to have it inside the   object itself. So if you feel strongly about it, please go ahead. Any update on this?   any interest or further opinion? I agree with Azeem that I'd prefer to leave this code as is. I'm okay if you close this issue. ok, thanks for the input. I'll keep the issue open as I'd like to see if it's valuable/feasible to have this info in the RAW results  Any concerns about having the info the raw results? No concerns from me."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1535",
				"issue_name": [
					"Unit tests required - Vulnerabilities check"
				],
				"issue_label": [
					"no-issue-activity"
				],
				"issue_content": "\n          Needs more tests for this https://codecov.io/gh/ossf/scorecard/commit/5f9fff3b20ce7eb933978c7a4f9391cb2c9b3d89\nOriginally posted by @naveensrinivasan in #1532 (comment)\n      ",
				"issue_comment": "Needs more tests for this   FYI the same tests we had before are there. It should cover the same logic, maybe percentage-wise it's down because the checks/vulnerabilities.go introduced a new level of indirection/boilerplate. Which lines are not hit that use to be? where do we see the coverage for a PR? I did not notice it in the main thread. Is it in a check run's details? I know, I wish the tool was reported in the PR. I don't know why it was missing TBH. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1534",
				"issue_name": [
					"Handling dates in copyright headers"
				],
				"issue_label": [
					"good first issue"
				],
				"issue_content": "\n          From @/swinslow in #1532 (comment):\n\n(@justaugustus for visibility, I'm no longer employed by the LF, so I'm not speaking on their behalf here in any official way) :)\nThe LF's general guidance to projects over the past few years has been to recommend a copyright notice form that omits the year. I had previously written up a blog post that describes a \"generalized\" copyright notice format in some detail, which was related to earlier guidance for CNCF.\nThe main reason for this recommendation was to avoid the sort of question raised here: e.g. developers wondering every January whether or not to bump the notice up to the next year; with the real answer depending on an analysis of copyrightability of particular contributions that generally goes beyond what folks want to get into. And since the year is not mandatory in order for a work to be copyrighted, some projects have made the decision to omit it.\nKeep in mind the caveats that (1) there's nothing wrong with including the year, this was just one recommendation; and (2) you'd never want to modify a copyright notice that names a particular third party without their consent.\nHope this helps :)\n\nFrom @laurentsimon in #1532 (comment):\n\nThanks for the feedback. Let's discuss this in an issue and address it via a PR if we decide to update the dates or remove them entirely. @justaugustus can you create a tracking issue?\n\nFrom @david-a-wheeler in #1532 (comment):\n\nGiven the previous comments from @/swinslow , we might want to change all copyright statements to something like:\n// Copyright Security Scorecard Authors\n\nSee Copyright Notices in Open Source Software Projects. I believe the legal requirement for the copyright statement (with the date) ended in the US in 1976 :-).\n\n      ",
				"issue_comment": "From @/swinslow in  : (  for visibility, I'm no longer employed by the LF, so I'm not speaking on their behalf here in any official way) :) The LF's general guidance to projects over the past few years has been to recommend a copyright notice form that omits the year. I had previously written up   that describes a \"generalized\" copyright notice format in some detail, which was related to  . The main reason for this recommendation was to avoid the sort of question raised here: e.g. developers wondering every January whether or not to bump the notice up to the next year; with the real answer depending on an analysis of copyrightability of particular contributions that generally goes beyond what folks want to get into. And since the year is not mandatory in order for a work to be copyrighted, some projects have made the decision to omit it. Keep in mind the caveats that (1) there's nothing   with including the year, this was just one recommendation; and (2) you'd never want to modify a copyright notice that names a particular third party without their consent. Hope this helps :) From   in  : Thanks for the feedback. Let's discuss this in an issue and address it via a PR if we decide to update the dates or remove them entirely.   can you create a tracking issue? From   in  : Given the previous comments from @/swinslow , we might want to change all copyright statements to something like: See  . I believe the legal requirement for the copyright statement (with the date) ended in the US in 1976 :-). As a new in OpenSource Community, I would like to take this ticket to work on. I would like to understand more on this ticket. \nThank You A few thoughts, some of which will echo  : \n \n \n \n \n \n From Ed Warnicke in  : Out of curiosity... are you checking for copyrights in generated files? I ask, because typically I tend to think of copyright checks as linting, and linters aren't supposed to lint generated files. Typically linters don't lint generated files, detecting that they are generated by looking for the pattern at the top of generated files: Per  I would like to understand more on this ticket. So I guess to more concretely answer this, a few things need to happen: \n \n \n \n I'm fine with all the above. Thanks   I have added this item in our bi-weekly. If you drop by, we can discuss this over call. Thank you  You can infer the LF stance from CNCF's copyright header policy, we updated it not so long ago: \n There's also information here: \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1533",
				"issue_name": [
					"Determine whether code sign-offs should be required/enforced"
				],
				"issue_label": [
					"no-issue-activity"
				],
				"issue_content": "\n          \nWe don't stress for sign-off. If we want to bring such changes, can you please create an issue?\n\nOriginally posted by @naveensrinivasan in #1532 (comment)\n      ",
				"issue_comment": "We don't stress for sign-off. If we want to bring such changes, can you please create an issue? Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1529",
				"issue_name": [
					"community: Is there a contributor ladder?"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Opening this to ask if there's a contributor ladder defined for this project.\nHere's an example: https://github.com/kubernetes/community/blob/master/community-membership.md\nSubtext: I'd be happy to help do PR review here, with the hopes of working towards maintainership, if there's a path for that :)\nPrevious contributions:\n\n#1502\n#1506\n#1516\n\nMaybe this is something like expanding CODEOWNERS, similar to apiclarity/apiclarity.io#29.\nIdeas on general org/repo mgmt (some of which you're likely already doing): todogroup/governance#106 (comment)\ncc: @inferno-chromium @naveensrinivasan @azeemshaikh38 @laurentsimon (ref: \n  \n    \n      scorecard/.github/CODEOWNERS\n    \n    \n         Line 3\n      in\n      e774015\n    \n  \n  \n    \n\n        \n          \n           .github/workflows/*       @inferno-chromium @naveensrinivasan @azeemshaikh38 @laurentsimon \n        \n    \n  \n\n)\n      ",
				"issue_comment": "Opening this to ask if there's a contributor ladder defined for this project. \nHere's an example:  Subtext: I'd be happy to help do PR review here, with the hopes of working towards maintainership, if there's a path for that :) Previous contributions: \n \n \n \n Maybe this is something like expanding CODEOWNERS, similar to  . \nIdeas on general org/repo mgmt (some of which you're likely already doing):  cc:         (ref:  Opening this to ask if there's a contributor ladder defined for this project. Here's an example:  Subtext: I'd be happy to help do PR review here, with the hopes of working towards maintainership, if there's a path for that :) Hi  super excited to see you're interested in scorecard, and appreciate the greats PRs you have contributed and the ones yet to come :-)! We don't have a contributor ladder today, we only have  . So far, the scorecard's codebase has remained small enough that we have not found the need to separate owners for sub-modules within scorecard. But you make a good point about scaling the project. We welcome new maintainers as well, although we don't have clear rules about that either. Looking forward to your input/advice here. Do you want to join our next meeting on Thu? Previous contributions: \n \n \n \n Maybe this is something like expanding CODEOWNERS, similar to  . Ideas on general org/repo mgmt (some of which you're likely already doing):  cc:         (ref: ) Subtext: I'd be happy to help do PR review here, with the hopes of working towards maintainership So far, our path to maintainership has been to just ask :P I'm happy to have you on as a maintainer right away.        any objections to adding   as a Scorecard maintainer? Opening this to ask if there's a contributor ladder defined for this project. \nHere's an example:  We do need a well-defined contributor ladder in the long-run. We can discuss more about this during our upcoming sync like Laurent suggested. Can also use this issue for tracking the discussion. Subtext: I'd be happy to help do PR review here, with the hopes of working towards maintainership So far, our path to maintainership has been to just ask :P I'm happy to have you on as a maintainer right away.       any objections to adding   as a Scorecard maintainer? No objections and very happy to see Stephen's ( ) contributions here. Opening this to ask if there's a contributor ladder defined for this project. \nHere's an example:  We do need a well-defined contributor ladder in the long-run. We can discuss more about this during our upcoming sync like Laurent suggested. Can also use this issue for tracking the discussion. We need these processes for OpenSSF in general. Could be nice to have this at OpenSSF org level somewhere in a generic place, rather than in AllStar project.     - thoughts on this front. Thanks so much for the support! \nI've opened   to make the CODEOWNERS changes. We don't have a contributor ladder today, we only have  . So far, the scorecard's codebase has remained small enough that we have not found the need to separate owners for sub-modules within scorecard. But you make a good point about scaling the project. We welcome new maintainers as well, although we don't have clear rules about that either. Looking forward to your input/advice here. Do you want to join our next meeting on Thu? Is this Thursday, 4pm ET? \nIf so, I can pop by for the latter half of the meeting :) We do need a well-defined contributor ladder in the long-run. We can discuss more about this during our upcoming sync like Laurent suggested. Can also use this issue for tracking the discussion. SGTM! We need these processes for OpenSSF in general. Could be nice to have this at OpenSSF org level somewhere in a generic place, rather than in AllStar project.     - thoughts on this front. Strong agree! \nI think generic contribution guidelines, along w/ repo-specific guidelines make a lot of sense. I would suggest leveraging the .github repo for this:  We can set up an   to point people to some good places to engage and yep, I can help out with that! From  : **Is your feature request related to a problem? We need to define governance. This will help new contributors understand the policies ref: "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1522",
				"issue_name": [
					"Feature: granular remediation hints per Warning"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nSee discussion in #1500.\nDescribe the solution you'd like\nHaving more granular remediation (text) for some warnings, like \"you can find the hashes for this action here\" would be helpful.\nDescribe alternatives you've considered\nOpening issues, doc updates :)\n      ",
				"issue_comment": "\nSee discussion in  . \nHaving more granular remediation (text) for some warnings, like \"you can find the hashes for   action here\" would be helpful. \nOpening issues, doc updates :)"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1509",
				"issue_name": [
					"Feature: unit test need to verify Logger messages"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We've started adding more unit tests, thanks @naveensrinivasan\nI've noticed that many of the new unit test validate the score, but not the logging messages. The log messages are important to ensure we give the right level of info to users, and also important for SARIF: changes affect the way the results will show in the GH dashboard.\nWe have functions under utests/, such as ValidateTestReturn that can be used to validate the number of messages and their type.\nFYI, we no longer need to validate the pass/fail, since we need to remove this code #1393\nI'm creating this issue for tracking before we forget.\nAssigning to @naveensrinivasan for now as he's leading the testing efforts.\n      ",
				"issue_comment": "We've started adding more unit tests, thanks  I've noticed that many of the new unit test validate the score, but not the logging messages. The log messages are important to ensure we give the right level of info to users, and also important for SARIF: changes affect the way the results will show in the GH dashboard. We have functions under  , such as   that can be used to validate the number of messages and their type. FYI, we no longer need to validate the  , since we need to remove this code  I'm creating this issue for tracking before we forget. Assigning to   for now as he's leading the testing efforts."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1508",
				"issue_name": [
					"New check: Check for vulnerabilities in your dependency trees"
				],
				"issue_label": [
					"core feature",
					"needs discussion",
					"priority"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nRight now, vulnerabilities check gives vulns in the package itself, but does not say anything about known vulns in its deps tree.\nDescribe the solution you'd like\nEither use Open Source Insights api or something similar, to create a new check DepsVulns and provide information on known vuln in the deps.\nDescribe alternatives you've considered\nNA\nAdditional context\nNA\n      ",
				"issue_comment": "\nRight now, vulnerabilities check gives vulns in the package itself, but does not say anything about known vulns in its deps tree. \nEither use Open Source Insights api or something similar, to create a new check DepsVulns and provide information on known vuln in the deps. \nNA \nNA cc  I agree. If we are going down the rabbit hole of the dependency tree why just look into Vulns alone. What about the health of the transitive dependency tree? We already have this Scorecard data in our BigQuery. The steps involved are dependency tree and getting scorecard data from BigQuery for each of the dependencies.  I built this to identify the scorecard for all the   dependencies   - the problem is we need something to work across languages. go is easy, we need to support atleast a few more ecosystems. Note that BQ table contains the scorecard's repo data, but the missing link is package version/hash. I don't think we can use BQ table because of that, and the data must come from another source. We can potentially query each ecosystem's database ourselves, but this still requires us to parse the manifest file... If OSI API is available, that's the way to go. Otherwise we should consider creating a service that runs existing ecosystem's tools like pip-audit, cargo-audit, and expose the results via an API.  - the problem is we need something to work across languages. go is easy, we need to support atleast a few more ecosystems. I agree. But I think we should scope it as not just Vulns that was the point.  - the problem is we need something to work across languages. go is easy, we need to support atleast a few more ecosystems. I agree. But I think we should scope it as not just Vulns that was the point. ah that, that i totally agree, it could be even aggregate scorecard calc on deps too instead of just vulns I agree with Naveen. I think finding transitive dependencies (and scoring them) shouldn't be a Scorecard check. Instead, I think we should aim to partner with someone like Open Source Insights or an SBOM generation tool and plug our Scorecard result in their output dependency tree. The biggest advantage being, this will allow a tool like AllStar to apply fine-grained and complex policies on not just the repo itself but also on its dependencies. I think finding transitive dependencies (and scoring them) shouldn't be a Scorecard check. Instead, I think we should aim to partner with someone like Open Source Insights or an SBOM generation tool and plug our Scorecard result in their output dependency tree. \n I have a different opinion if we don't do it, which project is going to do it?  Unless we decide to build a project that has that information. \n Another blocker for other projects (like SBOM generation) is that they would have to have an auth for   because Scorecard doesn't expose this data via an HTTP API. Hypothetically if an SBOM tool would bring in this feature then the end-users would have to provide    for BigQuery which is a blocker for most of them. \n The biggest advantage being, this will allow a tool like AllStar to apply fine-grained and complex policies on not just the repo itself but also on its dependencies. AllStar is specific to GitHub and Scorecard would eventually not be specific to GitHub. AllStar is a not CLI tool that everyone can consume. another angle for this is that scorecard has complementary information to OSI's data: we look at workflows, shell scripts, makefiles (soon), GCB config (soon), so we could also build and expose this data \"dependencies in CICD\", which is useful during incidence response and that nobody is really looking into atm. I suppose this would even be useful in the context of criticality score. another angle for this is that scorecard has complementary information to OSI's data: we look at workflows, shell scripts, makefiles (soon), GCB config (soon), so we could also build and expose this data \"dependencies in CICD\", which is useful during incidence response and that nobody is really looking into atm. I suppose this would even be useful in the context of criticality score. So are you in favor of having the check as part of scorecard? another angle for this is that scorecard has complementary information to OSI's data: we look at workflows, shell scripts, makefiles (soon), GCB config (soon), so we could also build and expose this data \"dependencies in CICD\", which is useful during incidence response and that nobody is really looking into atm. I suppose this would even be useful in the context of criticality score. I agree!  related  \nwonder if other tools like dependabot/renovatebot provide an API to parse and output vulnerabilities from the various ecosystems cc  I had a quick look at the repo for parsing  . It uses string parsing which is brittle and likely to break. I am not excited about that project. another angle for this is that scorecard has complementary information to OSI's data: we look at workflows, shell scripts, makefiles (soon), GCB config (soon), so we could also build and expose this data \"dependencies in CICD\", which is useful during incidence response and that nobody is really looking into atm. I suppose this would even be useful in the context of criticality score. I agree!   "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1507",
				"issue_name": [
					"New check: Security Audit check"
				],
				"issue_label": [
					"core feature",
					"priority"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nNo way for anyone to check if a security audit was done for project.\nDescribe the solution you'd like\nShould check against OpenSSF Security Reviews project to give a pass or fail whether a public security audit result is available. Can start with 0/10 score first (and need some audit pass/fail result in that repo itself). Later, can think of a better scoring model via parsing different aspects of the result.\nDescribe alternatives you've considered\nNA\nAdditional context\nNA\n      ",
				"issue_comment": "\nNo way for anyone to check if a security audit was done for project. \nShould check against OpenSSF Security Reviews project to give a pass or fail whether a public security audit result is available. Can start with 0/10 score first (and need some audit pass/fail result in that repo itself). Later, can think of a better scoring model via parsing different aspects of the result. \nNA \nNA related thread  Where will Omega project store their results for audits? cc  We haven't decided yet, but I'm leaning toward a   store for assertions (perhaps \"Omega processed pkg:npm/foo@1.2.3 and did not find a critical vulnerability\"). But we're a ways off from that -- I'd also be fine making it queryable -- but we'd have to figure out exactly what kind of information we want to provide -- obviously, we're not going to drop 0-days on the world. Alpha engagements would definitely contribute to security-reviews. The security-reviews project could also dump results into the common SCIM store, too. Thanks   for the prompt reply. \nAny though on which naming convention will be used? It would be great if we could support OSV's. cc   from OSV. yes +1 to what Michael, once we have a store, we can query that. but for now, directly checking github repo with some structure on directory (like some sort of repo url or OSV identifier or something ?). I'd be willing to work on this one. I have a couple implementation questions: \n \n \n I'd be willing to work on this one. I have a couple implementation questions: \n \n good idea, yes. \n \n There was an intended API for this  , but unfortunately the ossf dashboard is going to be deprecated. I suppose for now using GitHub + links works.   what would you suggest?  is it just the ossf dashboard that is going to be deprecated, or is it the API as well? I would prefer to use the API over GitHub + links. I think the API as well, because both the API and the dashboard use the same underlying data storage. \n  and   are working on scorecard API, so maybe we should use those when they are ready. \nwdut? I think the API as well, because both the API and the dashboard use the same underlying data storage.   and   are working on scorecard API, so maybe we should use those when they are ready. wdut? I'm confused, I thought we were discussing API for security-review. How is Scorecard API related here? oops, sorry I got confused! You're right the scorecard API is irrelevant. \nI suspect the API will be deprecated too,   do you know or can you confirm? The API for metrics.openssf.org most likely will be deprecated, but there might still be an API for   (see  )? Based on   by Scott, it doesn't sound like the new SCIM data store will be available anytime soon. I'm going to unassign myself from this since we don't have a stable API to build on top of and since I'm in the process of moving. I won't have much time to work for the next couple months."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1503",
				"issue_name": [
					"Feature: support for GCB's cloud build in Dependencies-Pinning"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Add support for cloud yaml for dependency pinning, see example https://github.com/ossf/allstar/blob/main/cloudbuild.yaml#L4\n      ",
				"issue_comment": "Add support for cloud yaml for dependency pinning, see example "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1499",
				"issue_name": [
					"Feature: use .gitignore for binary-artifacts when --local is used"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          This could be fixed in the documentation saying \"do 'make clean' or its equivalent first\"\nWhen scanning a local folder, it would be nice if the .gitignore file were parsed to ignore things that wouldn't be in the git repo.\n      ",
				"issue_comment": "This could be fixed in the documentation saying \"do 'make clean' or its equivalent first\" When scanning a local folder, it would be nice if the   file were parsed to ignore things that wouldn't be in the git repo. Neat idea. Thanks for suggesting"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1491",
				"issue_name": [
					"Checks within the SecurityPolicy checks makes it hard to test"
				],
				"issue_label": [
					"bug",
					"needs discussion"
				],
				"issue_content": "\n          The security policy check instantiates repo client within the check which makes it hard to unit test\n\n  \n    \n      scorecard/checks/raw/security_policy.go\n    \n    \n        Lines 78 to 83\n      in\n      fc87431\n    \n  \n  \n    \n\n        \n          \n           dotGitHub := &checker.CheckRequest{ \n        \n\n        \n          \n           \tCtx:        c.Ctx, \n        \n\n        \n          \n           \tDlogger:    c.Dlogger, \n        \n\n        \n          \n           \tRepoClient: githubrepo.CreateGithubRepoClient(c.Ctx, logger), \n        \n\n        \n          \n           \tRepo:       c.Repo.Org(), \n        \n\n        \n          \n           } \n        \n    \n  \n\n\n      ",
				"issue_comment": "The security policy check instantiates repo client within the check which makes it hard to unit test \n  -- my team in Kubernetes Release Engineering maintains a GitHub package, which is pretty well-tested and would allow you to offload this. Could you look around and see if some of this fits your needs?   We already have this capability within the scorecard. We use the GitHub library extensively. It so happens this code doesn't use the right method which makes it hard to write unit tests."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1490",
				"issue_name": [
					"DISCUSSION: v5 milestone"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Let's start collecting and tagging issues with the v5 milestone.\n\nbadges\nraw results (beta release)\n\n      ",
				"issue_comment": "Let's start collecting and tagging issues with the v5 milestone. \n \n \n cc    One item I would add here is deprecation of v1 (pass/fail) code in this release. \n \n Adding few more: \n \n \n \n \n \n \n \n \n Updating this after discussion on the bi-weekly.   to add an issue about release process for Scorecard along with the items mentioned above. Updating this after discussion on the bi-weekly.   to add an issue about release process for Scorecard along with the items mentioned above. Added to   as  ! Unless there are objections, I'm volunteering as tribute/the Release Manager for socrecard v5 :) \nIt'll give me an opportunity to think through design and process improvements as we get closer to the release date. From  : I think we should instead declare that the next major release version ( ) is the first release in which we will be SemVer-compliant and direct our efforts into designing the module in a way that we can ensure that. I've created a tracking issue for this and added it to the v5 milestone:  Unless there are objections, I'm volunteering as tribute/the Release Manager for socrecard v5 :) It'll give me an opportunity to think through design and process improvements as we get closer to the release date. From  : I think we should instead declare that the next major release version ( ) is the first release in which we will be SemVer-compliant and direct our efforts into designing the module in a way that we can ensure that. I've created a tracking issue for this and added it to the v5 milestone:  +1"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1476",
				"issue_name": [
					"Feature: Detect if SBOMs generated"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nPast problems, including the\nlog4j vulnerability Log4Shell, have made it abundantly clear to many people that it's important\nto be able to quickly figure out what is included in some software.\nIt's be good to be able report on whether or not a project provides a software bill of material (SBOM),\nbecause that tells potential users that the developers are trying to help by providing this info.\nWe might even look at the White House Executive Order on Cybersecurity to see\nif there are other things projects could provide to help users respond to a problem.\nDescribe the solution you'd like\nDetect if SBOMs are present for all the dependencies in use.\nI'm sure there are many options, let's talk about those in the comments.\nI just want to record the idea so we can discuss it.\n      ",
				"issue_comment": "Past problems, including the \nlog4j vulnerability Log4Shell, have made it abundantly clear to many people that it's important \nto be able to quickly figure out what is included in some software. \nIt's be good to be able report on whether or not a project provides a software bill of material (SBOM), \nbecause that tells potential users that the developers are trying to help by providing this info. \nWe might even look at the White House Executive Order on Cybersecurity to see \nif there are other things projects could provide to help users respond to a problem. Detect if SBOMs are present for all the dependencies in use. \nI'm sure there are many options, let's talk about those in the comments. \nI just want to record the idea so we can discuss it."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1469",
				"issue_name": [
					"Feature: add support for all unpinned npm commmands"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We already look for npm install, update and install-test. We need support for other commands such as:\nnpm pkg set, npm pkg delete, npm exec, npx, npm run, npm set-script: those allow executing code and/or updating the lock file.\nSee https://docs.npmjs.com/cli/v8/commands/ for a list of commands.\n      ",
				"issue_comment": "We already look for npm install, update and install-test. We need support for other commands such as: \n ,  ,  ,  ,  ,  : those allow executing code and/or updating the lock file. See   for a list of commands."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1451",
				"issue_name": [
					"BUG: Allow \"neutral\" results for workflow listing"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          In the SAST check, we allow both \"success\" and \"neutral\" for a check result. I think we need to do the same for workflow run in https://github.com/ossf/scorecard/blob/main/checks/sast.go#L30\nTo be confirmed.\ncc @azeemsgoogle\n      ",
				"issue_comment": "In the SAST check, we allow both \"success\" and \"neutral\" for a check result. I think we need to do the same for workflow run in  To be confirmed. cc  Can a workflow have a \"neutral\" status? I suppose so, because workflow runs appear within check runs under the GitHub app's name \"github-actions\". So my current understanding is that a workflow run is just a type of check run. cc   may know better Looking at   I think the idea was to wait for examples of GHActions that can set their status to neutral. Even though I have never seen actions like that I think if they exist in the wild it would probably make sense to add \"neutral\" there too. Judging by   (where CodeQL shouldn't have been triggered in the first place) it seems that neutral statuses pop up due to a GitHub bug probably so I don't think they should be included there."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1428",
				"issue_name": [
					"Feature: do not filter ",
					" in results once we have raw result support is landed"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          See #1256 (comment)\nThe testdata folders should be filtered out thru the policy\n      ",
				"issue_comment": "See  The testdata folders should be filtered out thru the policy"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1427",
				"issue_name": [
					"Feature: encourage developers to share SAST results"
				],
				"issue_label": [
					"enhancement",
					"help wanted",
					"needs discussion"
				],
				"issue_content": "\n          Platforms like lgtm.com and sonarcloud.io enable SAST and make the results public. I think we should encourage this among projects. Given this issue #1268, we could then score the SAST check as follows:\n\nIf a SAST tool is enabled, give 4 points\nIf the SAST tool is enabled on PR, give additional 3 points\nIf the results are public, give an additional 3 points\n\n      ",
				"issue_comment": "Platforms like lgtm.com and sonarcloud.io enable SAST and make the results public. I think we should encourage this among projects. Given this issue  , we could then score the SAST check as follows: \n \n \n \n cc  If a SAST tool is enabled, give 4 points I'm not sure about sonarcloud.io but LGTM is usually enabled outside GitHub and if it isn't used to analyze PRs it's hard to tell whether it's actually used by projects. I think the presence of files like   and   could be a hint but I don't think projects have to keep those files if they can be built by default of it they don't additionally use custom queries. Coverity is another beast that can be integrated in mysterious ways that's hard to detect. In systemd and util-linux scripts sending data to it are kind of hidden in GitHub actions run once a day:  Other than that I like the idea in general. Thanks! I'd add that with Coverity it isn't even clear what \"public\" means :-) Even when reports are \"public\" there has to be a Coverity account. Without it, Coverity asks people to sign in as far as I know. Thanks. By public, I meant that the results are available for anyone to see. Do you know if there are public APIs to retrieve the data? For example, systemd's results seem to be available at  ... but it's too brittle to parse html this way, so I'd rather use a REST API. I found   but I've not looked at whether this is for repo owners only. sonarcloud has a similar public interface, e.g.,   and some API  If you know anyone at one of these companies, feel free to cc them on this issue. Last time I checked the license of LGTM explicitly forbade people from using its API if it wasn't related to sending code of open-source projects hosted on GitHub. I'm not sure if anything has changed since then though. It was acquired by GitHub at some point. Coverity Scan isn't responsive usually so I wouldn't even try. According to  You may not use any automated technology to scrape, mine or gather any analysis, results or other information from the LGTM service or otherwise access the pages of the LGTM service for any unauthorized purpose. I think we'd be using the API only for open-source project hosted on GitHub, so we should not be in violation. cc  If you can check whether sast is enabled via API you're fine I believe. Thanks  . Another idea is to mine the data to infer time to fix, etc., and use this info in the Vulnerabilities check  On second thought considering that projects can be added to LGTM by virtually anybody I think it should be safe to say that if it isn't used to analyze PRs it isn't used in general. Now that restrictions are slowly being lifted It would be great if LGTM could also be used to analyze project hosted elsewhere:   :-) Seriously though, considering that scorecard can be used anywhere in various ways those terms of service would stop me anyway if by analogy with cron jobs run by analogy with   or   I wanted to set up a large-scale scorecard analysis of thousands of projects on a regular basis. Looking at  , it seems sonarcloud forbids automated applications as well An \nAccount may not be opened or shared by multiple people or an automated software application \n(e.g., an Internet bot). \nYou expressly understand and agree that your continued use of any Services after that \ndate shall be your automatic acceptance of this Agreement. According to  You will use the Software only in accordance with this Agreement and the applicable Registered Project Acknowledgement; and You will not use the Software to store or transmit any malicious code, interfere with or disrupt the integrity of the Software, or attempt to gain unauthorized access to the Software or its related systems or networks. whatever that means :-) I know the folks at Coverity working on their inventory if its helpful I can have them come to an openSSF meeting. LMK. I'm not sure what the inventory is but it would be great if they could make reports really public by default when \"Project summary and defects are viewable in read-only mode\" is set to true. The systemd CI tends to send emails to contributors who it thinks introduce new defects and if they aren't particularly familiar with Coverity they usually end up clicking on the \"add me to the project\" button and waiting for the maintainers to approve their requests. At some point the documentation was updated: Access to Coverity and oss-fuzz reports is limited. Please reach out to the maintainers if you need access. but it's probably hard to find and I assume most \"drive-by\" contributors don't go out of their way to just be able to look at those reports. I think it's more important in general than access to their API (though I agree it would be nice if their licence could be reworded to make it safe to use scorecard). I forgot to say that the previous comment is more or less applicable to CodeQL/Scorecard Action (or any other action utilizing the security tab) as well. It would be great if that dashboard could be made partly public. Though it should probably go to  . I'm on the same page,   . I meant to ask   if there is a chance the CodeQl results or the scanning dashboard could be made public in the future? This could be a simple opt-in for users to opt in. I see that lgtm.com already runs CodeQl for free, that the OSSF Omega project is also going to run CodeQl for open-source projects, and that anyone can run CodeQl on existing repos by forking them and running the workflow periodically. So it seems like the data is already public in a sense. As GitHub already runs CodeQl, would it make sense to make the results public (opt-in) instead of duplicating this effort? the OSSF Omega project is also going to run CodeQl for open-source projects  out of curiosity was the Alpha-Omega project approved in the end? It's just that I'm not sure I agree with the Alpha part at least and it would be interesting to figure out where it's going. Just to clarify, what I know about it is based on the public proposal I came across in December I think. At the time it was a draft and it could be that it might have changed and I don't know about it. it's going ahead. I think the official announcement will be made in March. Got it. Thanks! Hopefully those one-off \"long-term\" engagements will be more helpful than they usually are."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1420",
				"issue_name": [
					"BUG: SAST and CI-Tests wrongly succeed when action is enabled"
				],
				"issue_label": [
					"bug",
					"help wanted"
				],
				"issue_content": "\n          SAST and CI-Test use the PR.App.Slug field to determine if a SAST/CI tool is used. When running scorecard's GitHub action, we detect our own action as a SAST/CI tool.\nWe should exclude it. The URL can be used to determine the name of the tool run, e.g., https://api.github.com/repos/laurentsimon/scorecard-action-test-3/check-runs/4143313445\n      ",
				"issue_comment": "SAST and CI-Test use the   field to determine if a SAST/CI tool is used. When running scorecard's GitHub action, we detect our own action as a SAST/CI tool. We should exclude it. The URL can be used to determine the name of the tool run, e.g.,   is this something you'd like to add to the repo interface, i.e. a way to retrieve the tool name from the run/Slug?  is thi \nsomething you'd like to add to the repo interface, i.e. a way to retrieve the tool name from the run/Slug? U  is thi \nsomething you'd like to add to the repo interface, i.e. a way to retrieve the tool name from the run/Slug? U It seems non-intuitive to me that users can't improve their SAST/CI scores by enabling Scorecard actions. We shouldn't ignore Scorecard action in these checks IMO. If these checks seem to \"wrongly succeed\", then I would argue that the way we dole out points in these checks is where the problem lies. Maybe we should re-think the scoring system in these checks instead? It seems non-intuitive to me that users can't improve their SAST/CI scores by enabling Scorecard actions. We shouldn't ignore Scorecard action in these checks IMO. good idea. I initially thought scorecard should be running silently in the background and not affect the results. But you make a good point. If these checks seem to \"wrongly succeed\", then I would argue that the way we dole out points in these checks is where the problem lies. Maybe we should re-think the scoring system in these checks instead? I agree the scoring system is not great for those checks. I think it requires more information about SAST tools. Originally I wanted to classify what features or vulnerability classes each tool can find, but this seems too ambitious at this point. A more accessible scoring could be: \n \n \n \n Until we get there, we could maybe start by implementing point 2, ie give 3 points for scorecard as supply-chain tool. wdut? For CI-Tests check, I'm not sure what we should do. The check is currently coarse . Maybe we can keep it as-is for the time being? Discussed offline: can update the SAST check to use   API. Basically, find all SAST related workflows and check for successful runs of this workflow. I did some digging and I don't think listing workflow runs works. The SAST check uses the Check API because it allows listing not just workflow runs, but also GitHub apps. See doc in    updates: GitHub runs apps. Some apps include  ,  ,  ,   and can be identified by their slug or their app ID. \nFor GitHub-owned apps like GitHub actions, the workflow and job name is set by users: in this case we can parse the action and look for the action used + successful workflows, as suggested in  . I'll start a PR that follows this structure:  \nI'll wait till   is resolved to implement the  there is an additional complication I've encountered:   returns as per commit runs. If a PR contains N commit, the runs will be returned for these N commits, then for the commits in the next PR, etc. The API allows up to 100 results per page, but it's not straightforward Hmm, tricky. The API allows us to filter by   and   too. Wonder if that can help. Exploring the REST and GraphQL APIs bit more, will update here if I find anything. an alternative (first iteration) solution we may consider is to identify workflows by their names, and map these names to the check run's name. So we would: \n \n \n There are corner cases, e.g. if a dev gives the exact same name to 2 different workflow, or if someone has update the workflow name... But this may be a good first step to improve the check until we're able to get workflow runs for a particular PR. wduy? follow-up on my last comment. The name in the check run is   if   is present,   otherwise: I've checked the partnered workflows for code scanning   and found that \n \n \n On a related note: this list of scanning tools may be a good list to support I think I found a way to do this. Check runs have a  , and it's possible to list workflow runs by   as well: this gives us the workflow ID which we can map to a file - and therefore an action. I'll work on a PR to see if it actually works or not ... and see how many additional API calls it requires. I suspect it needs an additional call per action run, so an additional "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1417",
				"issue_name": [
					"Feature: add support for keyless signed release"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We should add support for keyless cosign signing in the Signed-Release check.\ncc @asraa\n      ",
				"issue_comment": "We should add support for keyless cosign signing in the Signed-Release check. cc "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1406",
				"issue_name": [
					"add ability to pass ignore list"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          What would you like to be added:\nWe are skipping the binary-check for the testdata folders as implemented in isTestdataFile() function.\nNot sure but passing a new ignoreDirs []string field to struct CheckRequest will solve the issue. (It should apply *foo* logic for every element that we traversed?) Which we call in BinaryArtifacts function afterward.\nWhy is this needed:\nTo prevent Binary-Artifacts false positives.\nFrom the discussion #593 started by @developer-guy; @westonsteimel said we probably need some enhancements to consider other names for test directories, as @laurentsimon already proposed an idea to ability to skip given folders in the config.\nAdditional context:\nOut of context:  In the pull https://github.com/ossf/scorecard/pull/1263/files I'm not sure whether it's that secure to check a single line of if command to trigger skip logic by force asserting *test* matching. Can someone please enlighten me about the following logic; wouldn't it be dangerous from the security perspective? If I put some vulnerable executables in random *test* folders?\nif strings.Contains(strings.ToLower(path), \"test\")\n      ",
				"issue_comment": ": We are skipping the binary-check for the   folders as implemented in   function. Not sure but passing a new   field to struct   will solve the issue. (It should apply   logic for every element that we  ?) Which we call in   function afterward. : To prevent  . From the discussion   started by  ;     we probably need some enhancements to consider other names for test directories, as   already   to ability to skip   folders in the config. : Out of context:  In the pull   I'm not sure whether it's that   to check a single line of   command to trigger skip logic by force asserting   matching. Can someone please enlighten me about the following logic; wouldn't it be dangerous from the security perspective? If I put some vulnerable executables in random   folders? Thanks for the issue. : To prevent  . Can you provide a repo example where you're observing too many false positives? From the discussion   started by  ;     we probably need some enhancements to consider other names for test directories, as   already   to ability to skip   folders in the config. : Out of context: In the pull   I'm not sure whether it's that   to check a single line of   command to trigger skip logic by force asserting   matching. Can someone please enlighten me about the following logic; wouldn't it be dangerous from the security perspective? If I put some vulnerable executables in random   folders? You're correct there is a risk of false negatives, e.g., if the file is loaded and executed by someone (unit tests). If you're a repo owner, we'd expect you to know what you're doing with the executables (scorecard only helps you flag potential risky practices). One legitimate use case may be fuzzing inputs. If you're a repo consumer, then it's up to you to decide whether you consider the results acceptable or not. That said, it may be useful for repo owners to declare explicitly (with a description) why it may be acceptable for some directories to not pass certain scorecard checks. A human writing a policy on top of scorecard results may use them as hints to decide what is or is not acceptable (cc     relevant discussion around security insight work.). The GitHub action we'll release in Jan'22 may be a good place for users to declare which directories are   for their repo and why. We have not worked on this yet, though. We think it may be easier when we close this issue   (WIP) Note this related to  \nA list of acceptable directories may be useful for other checks such as Pinned-Dependencies: currently we're not able to distinguish between external dependencies and internal dependencies (e.g., a docker image that belongs to the same repo and is used without pinning intentionally). The requirement to pin internal dependencies may be acceptable in certain cases."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1393",
				"issue_name": [
					"Feature: deprecate Pass/Fail model"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Scorecard will deprecate Pass/Fail model along with the confidence score by EO'Q1 2022. This issue tracks the progress of it.\n      ",
				"issue_comment": "Scorecard will deprecate Pass/Fail model along with the confidence score by EO'Q1 2022. This issue tracks the progress of it."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1389",
				"issue_name": [
					"Feature: add Scorecard to OSS-Fuzz and CIFuzz"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Improve Scorecard's Fuzzing score by adding it to OSS-Fuzz and enabling CIFuzz GH Action.\n@jonathanmetzman fyi.\n      ",
				"issue_comment": "Improve Scorecard's Fuzzing score by adding it to OSS-Fuzz and enabling CIFuzz GH Action.  fyi. Which API do you want to fuzz within scorecard? If it is just to get the checkmark for fuzzing then it doesn't add value. Probably good for discussion. FWIW Looking at  , I think it would make sense to cover h2non/filetype at least (assuming scorecard will keep using it for parsing untrusted data). As far as I know scorecard also parses random bash scripts with   so ideally it should probably be covered as well (though I agree technically it doesn't improve the fuzzing score of scorecard itself) As far as I know scorecard also parses random bash scripts with   so ideally it should probably be covered as well (though I agree technically it doesn't improve the fuzzing score of scorecard itself) AFAIK the   is fuzzed probably randomly not using oss-fuzz. FWIW Looking at  , I think it would make sense to cover h2non/filetype at least (assuming scorecard will keep using it for parsing untrusted data). I agree,I think it would be better to include the fuzzing that repository instead of Scorecard. Thanks for bringing it up. It makes sense. scorecard has its own file parser located in   though (which I think is a good candidate for fuzzing). AFAIK the   is fuzzed probably randomly not using oss-fuzz Good to know. Thanks! But given that finding and fixing bugs found by fuzz targets is like playing whack-a-mole if it isn't fuzzed continuous it's probably safe to say it isn't fuzzed. scorecard has its own file parser located in   though (which I think is a good candidate for fuzzing). AFAIK the   is fuzzed probably randomly not using oss-fuzz Good to know. Thanks! But given that finding and fixing bugs found by fuzz targets is like playing whack-a-mole if it isn't fuzzed continuous it's probably safe to say it isn't fuzzed. I agree. It's a valid use case. I wrote an issue on that repo  IMO, we should start with a basic framework which allows us to start fuzzing. Over time we can keep identifying and adding more APIs as we see fit. The recent nil-ptr crashes during file parsing are the reason I think this will be useful. Even though the parsing libraries we use might be fuzzed, the crashes were due to our logic. Looks like the oss-fuzz will support native fuzzing which is a lot easier with the 1.18 release. We should wait until then.  FWIW considering that scorecard isn't on OSS-Fuzz yet and its testing infrastructure doesn't rely on public OSS-Fuzz corpora I think it would be better to integrate it with   in the sense that as long as steps requiring external storage like corpus pruning are behind something like   it's compatible with forks, which in turn makes it easier for external contributors to test their patches before sending them. Another plus is that CFLite can be kind of pinned and doesn't drag the \"Pinned-Dependencies\" score down with it. (Though judging by something that looks like release notes it seems it should be possible to pin it eventually as well). We could look at this when we migrate to go 1.18 especially because it is going to be builtin and is going to be supported by oss-fuzz   assigning this to you track progress. Part of  . This is creating crashes  FWIW since the fuzz target is public I don't think scorecard bug reports should be private because they can be easily reproduced. For example, I don't have access to bug reports but I can trigger all the crashes like:  looking at   it seems a seed corpus wasn't added there. I think it would probably make sense to address that in  . Seed corpora are briefly mentioned at  There are multiple crashes, I have gotten a few emails about this from oss-fuzz. There need to be multiple things that have to be done before Scorecard is integrated into oss-fuzz The reason behind the above steps was based on me integrating sigstore into oss-fuzz  I think this is the order of things that have been done to avoid getting multiple crashes. This is some amount of work. We have to decide to either fix this or another option is to remove   this till we get to do it from our end. The default disclosure policy is   days for oss-fuzz. Unless we prioritize to work on this.          Thoughts? The default disclosure policy is 90 days for oss-fuzz FWIW I don't think that's what really happens in practice. Once fuzz targets are public it's safe to assume that bugs they can discover are (kind of) publicly known (because they are actually run outside of OSS-Fuzz). I completely agree with   on how scorecard should be integrated. The default disclosure policy is 90 days for oss-fuzz FWIW I don't think that's what really happens in practice. Once fuzz targets are public it's safe to assume that bugs they can discover are (kind of) publicly known (because they are actually run outside of OSS-Fuzz). I completely agree with   on how scorecard should be integrated."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1380",
				"issue_name": [
					"Feature: add linter check"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Linters are a great way to enforce certain coding practices and avoid mistakes, as well as to improve the code quality to help reviewers.\nI think it'd be useful to support this in scorecard. Maybe this could live under the existing SAST check. I think it's be enough to start with check for the presence of this action https://github.com/github/super-linter, which wraps all linters.\n      ",
				"issue_comment": "Linters are a great way to enforce certain coding practices and avoid mistakes, as well as to improve the code quality to help reviewers. I think it'd be useful to support this in scorecard. Maybe this could live under the existing SAST check. I think it's be enough to start with check for the presence of this action  , which wraps all linters. I think this needs some discussion."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1377",
				"issue_name": [
					"Feature: speed up Validate check in pre-submits"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Looks like check-linter and  check-osv should be able to run without installing and compiling scorecard. So we should be able to separate them.\nThis should additionally speed up the validation checks.\n      ",
				"issue_comment": "Looks like   and    should be able to run without installing and compiling scorecard. So we should be able to separate them. \nThis should additionally speed up the validation checks."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1371",
				"issue_name": [
					"Feature: add check for vulnerability alerts"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Most package managers have a *-audit tool: pip-audit, cargo-audit, npm-audit, etc. that pull security advisories from public databases (OSV, CVEs, package-specific databases, etc)\nDependabot and renovabot also have options to alerts uses when vulnerabilities in their dependencies are disclosed.\nIt would be useful to capture this in scorecard. This could live under Dependency-Update-Tool (which we could rename to Dependency-Management-Tool).\nFor commands, we may need to parse commands in run field of GH workflows, as suggested in #1031 (comment), unless there is a GitHub action for it.\nNote that we already parse commands for the Pinned-Dependency check but we have not yet separated out command parsing #1220\n      ",
				"issue_comment": "Most package managers have a *-audit tool: pip-audit, cargo-audit, npm-audit, etc. that pull security advisories from public databases (OSV, CVEs, package-specific databases, etc) \nDependabot and renovabot also have options to alerts uses when vulnerabilities in their dependencies are disclosed. It would be useful to capture this in scorecard. This could live under Dependency-Update-Tool (which we could rename to Dependency-Management-Tool). For commands, we may need to parse commands in   field of GH workflows, as suggested in  , unless there is a GitHub action for it. Note that we already parse commands for the Pinned-Dependency check but we have not yet separated out command parsing  cc  filed   to ask whether they are interested in creating a GitHub action for it. cc        I found an API to check for it for dependabot  I found an API to check for it for dependabot  That is cool! should this be its own check or inside Dependency-Update-Tool? IMO no. Probably good for discussion.  what do we need to do to detect that dep security alerts are enabled in renovatebot? Dependency security alerts don't need explicit enabling in Renovate Bot repository config. If the bot's token has been granted access to the vulnerabilities feed then they'll be automatically used. This is currently the case for   but has been accidentally missing for   until now but has now been requested to all org admins. Thanks. So we'll assume renovatebot has this enabled by default."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1369",
				"issue_name": [
					"Add gradation to license analysis"
				],
				"issue_label": [
					"enhancement",
					"help wanted"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nCurrently scorecard looks for a license, but it simply gives a 10 if there's a license found at all and a 0 if there's no license found at all. That doesn't give any gradation.\nDescribe the solution you'd like\nAdd more gradations for licenses. I think scorecard was intended for OSS projects, so I propose giving \"10\" for projects with a license known to be OSS per OSI or Free Software as defined by the FSF. Otherwise, give it a 5. For projects on GitHub you can just reuse the GitHub analysis, as GitHub has an API that provides license info. You can see an example of this use in the CII Best Practices' github_basic_detective and floss_license_detective.\nFurther discussion here: #1038\nDescribe alternatives you've considered\nDifferent people have different views of copyleft, I don't think scorecard should force a particular view of whether copyleft or not-copyleft is better (less risk). You could argue either way, which to me suggests you shouldn't make the argument.  Even those who prefer BSD-style licenses will often use the Linux kernel (GPL), because how you use software is very important. As a result, I think \"is it approved by OSI or FSF\" should be enough, don't force further gradations.\nAdditional context\nAgain, further discussion here: #1038\n      ",
				"issue_comment": "Currently scorecard looks for a license, but it simply gives a 10 if there's a license found at all and a 0 if there's no license found at all. That doesn't give any gradation. Add more gradations for licenses. I think scorecard was intended for OSS projects, so I propose giving \"10\" for projects with a license known to be OSS per OSI or Free Software as defined by the FSF. Otherwise, give it a 5. For projects on GitHub you can just reuse the GitHub analysis, as GitHub has an API that provides license info. You can see an example of this use in the CII Best Practices'   and  . Further discussion here:  Different people have different views of copyleft, I don't think scorecard should force a particular view of whether copyleft or not-copyleft is better (less risk). You could argue either way, which to me suggests you shouldn't make the argument.  Even those who prefer BSD-style licenses will often use the Linux kernel (GPL), because   you use software is very important. As a result, I think \"is it approved by OSI or FSF\" should be enough, don't force further gradations. Again, further discussion here: "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1366",
				"issue_name": [
					"Feature: generate SBOMs for scorecard container images "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nNo, this is not related to a problem.\nDescribe the solution you'd like\nWe've worked on building container images by making use of the google/ko project recently1. Then I realized that google/ko has the support of generating SBOMs based on SPDX format, and it'll be released soon2. Maybe we might want to use this feature to generate SBOMs for the scorecard container images.\nDescribe alternatives you've considered\nAlternatively, there are various tooling around this3456 concept to generate it There are several methods designed specifically for delivering SBOMs, including SPDX (Software Package Data Exchange), Software Identification (SWID) Tags, and Cyclone DX.\nAdditional context\nAdd any other context or screenshots about the feature request here.\ncc: @naveensrinivasan @Dentrax\nFootnotes\n\n\n#1127 ↩\n\n\ngoogle/ko#511 ↩\n\n\nhttps://github.com/anchore/syft ↩\n\n\nhttps://github.com/spdx/spdx-sbom-generator ↩\n\n\nhttps://github.com/tern-tools/tern ↩\n\n\nhttps://github.com/kubernetes/release/tree/master/cmd/bom ↩\n\n\n\n      ",
				"issue_comment": "\nNo, this is not related to a problem. We've worked on building container images by making use of the   project recently . Then I realized that   has the support of generating SBOMs based on SPDX format, and it'll be released soon . Maybe we might want to use this feature to generate SBOMs for the scorecard container images. Alternatively, there are various tooling around this  concept to generate it There are several methods designed specifically for delivering SBOMs, including  ,  , and  . \nAdd any other context or screenshots about the feature request here. cc:    \n   \n   \n   \n   \n   \n   \n what info does   support for the output? What I mean is, it seems to support listing the goland project's dependencies using  . What else? Does it capture dependencies installed via  , etc? I have not played around with  , so forgive me if those questions make no sense :-) it also supports SPDX format which you see   by using a flag named \"--sbom=spdx\", IMHO, it should be the default value btw. Here is the result from my binary by running the following commands: cc:    what info does   support for the output? What I mean is, it seems to support listing the goland project's dependencies using  . What else? Does it capture dependencies installed via  , etc? I have not played around with  , so forgive me if those questions make no sense :-) We don't ko integrated it into our container builds yet. We are using cloud build with docker. The first step would be to replace docker in our cloudbuild. what info does   support for the output? What I mean is, it seems to support listing the goland project's dependencies using  . What else? Does it capture dependencies installed via  , etc? It assumes you're not installing any deps with apt-get etc, which it looks like you're not anyway:  I'd be happy to help get scorecard onto ko if there's interest. Yes, we aren't doing any apt-get. There are multiple Dockerfiles  That would be great if you can help with   builds with cloudbuild. These are the cloudbuild YAML's in these folders \n \n \n \n We already have started working towards migrating to   "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1358",
				"issue_name": [
					"Feature: auto rebase of PRs"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We spend too much time rebasing PRs to kick off pre-submit runs. Especially when we have many PRs open, it's a waste of time.\nIt would be great to be able to auto-rebase PRs automatically, so that once we've clicked \"auto-merge\" button, the PR gets merged automatically when they can (of course, LGTM should still be enforced).\nI found this https://github.com/marketplace/actions/automatic-rebase which seems encouraging. I wonder if this requires dangerous write permissions... since it needs to push to remote cloned branch...?\nAnswering my own questions: cirrus-actions/rebase#84 they need pull-request:write` and this is a danger in itself since it allows changing the code before we merge :/\nProbably we need to review the code and pin to this hash for the code we've looked at.\nDo we loose other important things by using an auto PR rebase feature?\n      ",
				"issue_comment": "We spend too much time rebasing PRs to kick off pre-submit runs. Especially when we have many PRs open, it's a waste of time. \nIt would be great to be able to auto-rebase PRs automatically, so that once we've clicked \"auto-merge\" button, the PR gets merged automatically when they can (of course, LGTM should still be enforced). I found this   which seems encouraging. I wonder if this requires dangerous write permissions... since it needs to push to remote cloned branch...? \nAnswering my own questions:   they need pull-request:write` and this is a danger in itself since it allows changing the code before we merge :/ \nProbably we need to review the code and pin to this hash for the code we've looked at. Do we loose other important things by using an auto PR rebase feature?       their code is mostly a shell script and a docker files that pulls a few utilities  file   looks ok. There are a few places where it does not put env variable between \"\", which is also caught when I used   on their code.  I wonder why PRs would have to be rebased in the first place? Considering that PRs are usually reviewed almost as soon as they are opened I think as long as they don't conflict with the main branch it should be fine to merge them even if GitHub says that they are out of date. If it isn't clear whether PRs can break something it's always possible to ask contributors to force-push their branches to retrigger the tests and let GHActions rebase/merge them on top of the main branch to make sure the CI is green. Looking at   it seems for it to work it has to change contributors' repositories and personally I wouldn't be happy if bots rebased branches in my repositories :-) Another issue is that contributors can flip the \"Allow edits and access to secrets by maintainers\" flag to prevent changes like that."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1351",
				"issue_name": [
					"Feature: remove support for table output"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The table output is buggy and does not work well as soon as there are many results. There are also bugs w.r.t display #699\nSince the JSON output is readable by human, shall we drop support for the table altogether?\nOne less thing to maintain, especially as I've already heard 2 users complain about it.\n      ",
				"issue_comment": "The table output is buggy and does not work well as soon as there are many results. There are also bugs w.r.t display  Since the JSON output is readable by human, shall we drop support for the table altogether? \nOne less thing to maintain, especially as I've already heard 2 users complain about it. cc      SGTM cc  Sounds good to me.  Playing around with the table project    noticed if I add the following code I'm able to generate this kind of table output (note: I'm using the repo github.com/googleapis/gaxios), is this the table output we are after ? If this is the output we are after then the fix is to add after  can you help answer   's question? \nAFAIK, the terminal's width is a limiting factor and it makes it really hard to output tables that fit the screen. \nUnless   has another idea, it may be better to remove support for table entirely. I agree, lets remove the table support. There are 2 ways to look at this: \n Maintenance - In terms of less maintenance point of view looking at the amount of code to be maintained is less compared to checks and other modules of the application. \n User's perspective - Don't have full context about the issue that customer are complaining about but from the stats mentioned in this issue there are 2 users complaining compared to more than 2 users not complaining. In my mind this tells me that the number of people finding this feature useful outweigh the number of people complaining. \n Would be useful to get better insights to gauge whether the table output provided by scorecard is indeed useful for people or it's something that they don't care about. This is just my point-of-view, will leave to the team to decide to remove or not the feature :) There are 2 ways to look at this: \n \n correct. But it still requires maintenance once in a while, e.g. the bugs that were filed a few months ago and still not addressed. \n \n you can add me to the list of ppl complaining :-) More seriously, many people who don't find it readable won't complain and will switch to using the JSON output instead, without ever telling us. Would be useful to get better insights to gauge whether the table output provided by scorecard is indeed useful for people or it's something that they don't care about. It is not readable today when you enable  . The text does not wrap, AFAIK. Does your option fix it? This is just my point-of-view, will leave to the team to decide to remove or not the feature :) appreciated, thanks for sharing. I was coming here to suggest tablewriter! \nI think we should switch to that instead of dropping table output. I think we use   already?  do you see value in maintaining the table output? We sort of concluded that it was not worth the effort given the time it requires to fix alignment issues. Wdut? I think we use   already? Looks like we do!  do you see value in maintaining the table output? We sort of concluded that it was not worth the effort given the time it requires to fix alignment issues. Wdut?  should be relatively easy to fixup. \nI think being able to write to a markdown table would be super useful: \n \n \n ah, markdown could be useful.. many users try to print to terminal and it does not fit the screen size, etc... which what the intention of removing the table format. I'm ok to continue keeping it. Our main reasoning to consider removing this was too many folks complained that the table output was terrible. If someone can fix the formatting and make it nice and clean to read I'm very much onboard with it."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1343",
				"issue_name": [
					"Feature: remove 1 point per unpinned action"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          See #1319 (comment)\nWe currently give a score of 0 if a single unpinned action if found. This is too harsh. Instead, we will remove one point for each unpinned action.\n      ",
				"issue_comment": "See  We currently give a score of 0 if a single unpinned action if found. This is too harsh. Instead, we will remove one point for each unpinned action."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1337",
				"issue_name": [
					"BUG: goreleaser proxying fails"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          goreleaser fails with error=failed to proxy module - https://github.com/ossf/scorecard/runs/4301302671?check_suite_focus=true\nMost likely due to #1282. @naveensrinivasan assigning this to you. Feel free to re-assign if this is unrelated to changes in #1282.\n      ",
				"issue_comment": "goreleaser fails with   -  Most likely due to  .   assigning this to you. Feel free to re-assign if this is unrelated to changes in  . The issue with this is timing. The build was run because there was new tag that was pushed  But the go proxy didn't have that version yet. I re-ran the action and it ran successfully.  How can we solve this issue? Any suggestions would be helpful. in theory it should have proxied it on the first try, not idea why it didn't though... maybe some flake on sum.golang.org? in theory it should have proxied it on the first try, not idea why it didn't though... maybe some flake on sum.golang.org? OK, we will try again and let you know. Thanks"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1334",
				"issue_name": [
					"BUG: workflow commands are not pinned"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          We're not pinning our dependencies in the Makefile commands run in workflow.\nSome example include all the additional tooling: it's currently stored under tools and is installed by running:\ncd tools; cat tools.go | grep _ | awk -F'\"' '{print $$2}' | xargs -tI % go install %\nThis does not use the tools/go.sum because it runs all install as go install github.com/XXX. We should be pinning these dependencies properly.\nI've started adding pinning in #1332 for license check.\n      ",
				"issue_comment": "We're not pinning our dependencies in the Makefile commands run in workflow. \nSome example include all the additional tooling: it's currently stored under   and is installed by running: \n This does   use the   because it runs all install as  . We should be pinning these dependencies properly. I've started adding pinning in   for license check.  do you agree with my claim?  do you agree with my claim? Maybe not!   proxy caches the version of the deps and shouldn't change AFAIK. Maybe   can clarify my assumption. We're using a shell script to manually extract the dependencies to install. There must be a better way to do that, I hope"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1327",
				"issue_name": [
					"Feature: more robust way of logging errors in sub-checks"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          Today, we return an inconclusive results (accompanied by an error) if a sub-check catches an error. For example, the Pinned-Dependencies check is currently fairly (too!) large, so a single error makes the entire check return -1.\nIn #1324 (comment), we implemented a heuristic to avoid running the sub-check, using the filename. I feel this approach is fragile. Dockerfile templates are common, and are not always called template, tpl, etc. See #710 and https://raw.githubusercontent.com/kubernetes/kubernetes/master/build/server-image/Dockerfile.\nIt'd be useful to have a more robust solution. I'n not sure what a better solution is.\nMaybe we should create an Error() function to log errors in sub-checks without affecting the entire check result.\nAny other ideas?\n      ",
				"issue_comment": "Today, we return an inconclusive results (accompanied by an error) if a sub-check catches an error. For example, the Pinned-Dependencies check is currently fairly (too!) large, so a single error makes the entire check return  . In  , we implemented a heuristic to avoid running the sub-check, using the filename. I feel this approach is fragile. Dockerfile templates are common, and are not always called  ,  , etc. See   and  . It'd be useful to have a more robust solution. I'n not sure what a better solution is. \nMaybe we should create an   function to log errors in sub-checks without affecting the entire check result. Any other ideas?     relevant for shell parsing  I do think we should simply log when we can't parse a file instead of failing the entire check, essentially do what   does but for any parsing error. It would be nice if we could examine the files that can't be parsed in case we can improve our code to parse better, but inevitably we're going to encounter files that are formatted incorrectly. Maybe we can keep a list of files that we know are not parseable, and just ignore them. In regards to skipping template Dockerfiles, I think using the filename is a decent start. The file   is not, strictly speaking, a template; it is a fully valid Dockerfile, just with build arguments. To handle a file like this would should enhance are logic to look for hashes as you suggest in  , but we shouldn't skip it because it's a template. I do think we should simply log when we can't parse a file instead of failing the entire check, essentially do what   does but for any parsing error. It would be nice if we could examine the files that can't be parsed in case we can improve our code to parse better, but inevitably we're going to encounter files that are formatted incorrectly. Maybe we can keep a list of files that we know are not parseable, and just ignore them. that'd be great, yes! In regards to skipping template Dockerfiles, I think using the filename is a decent start. The file   is not, strictly speaking, a template; it is a fully valid Dockerfile, just with build arguments. To handle a file like this would should enhance are logic to look for hashes as you suggest in  , but we shouldn't skip it because it's a template. Do you know if developers commit their generated dockerfiles from a template? \nCan you list a few template dockerfiles that caused problem for scorecard? In the next iteration of the check, would is still be useful to try to parse the templates and just log a message of parsing fails (like you suggest above)? Do you know if developers commit their generated dockerfiles from a template? I think typically developers use a template dockerfile to build with different architectures or different distros as part of a pipeline. They might sometimes commit the dockerfiles generated from a template, but probably not usually. Can you list a few template dockerfiles that caused problem for scorecard? This is the only one I'm aware of that caused errors for scorecard: \n \nHere's another that would cause have caused errors if we were scanning them: \n In the next iteration of the check, would is still be useful to try to parse the templates and just log a message of parsing fails (like you suggest above)? I think that would be a better approach. It looks like some of the dockerfiles that we're now skipping (  for example) are able to be parsed just fine. Do you know if developers commit their generated dockerfiles from a template? I think typically developers use a template dockerfile to build with different architectures or different distros as part of a pipeline. They might sometimes commit the dockerfiles generated from a template, but probably not usually. Can you list a few template dockerfiles that caused problem for scorecard? This is the only one I'm aware of that caused errors for scorecard:   Here's another that would cause have caused errors if we were scanning them:  do you know if the templating engine is part of thee official docker tooling or is it provided by a third-party? In the next iteration of the check, would is still be useful to try to parse the templates and just log a message of parsing fails (like you suggest above)? I think that would be a better approach. It looks like some of the dockerfiles that we're now skipping (  for example) are able to be parsed just fine. do you know if the templating engine is part of thee official docker tooling or is it provided by a third-party? It's third-party tooling. how about creating an   function to logs in addition to   and  ? It would allow logging the error and skip minor parsing issues without returning an error for the entire check. Wdut? Yeah, I think that's a good approach. Added to discussion for next sync."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1321",
				"issue_name": [
					"Calculate risk based on score of the check"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nIn the future the Token-Permissions check will give different scores depending on how well the best practices has been followed. But even if one gets a score of 9, they will still get a High risk issue in the SARIF file. This score then gets shown in the Code scanning alerts dashboard when the Scorecards GitHub action is used in a repository.\nDescribe the solution you'd like\nThe risk of the issue that is emitted in the SARIF file should be based on the score of the check. If the score is high, the risk should be lower.\nDescribe alternatives you've considered\nAn alternative could be for each repository owner to have a policy file to set threshold of risk.\nAdditional context\nThis is related to discussion at #1128\n      ",
				"issue_comment": "\nIn the future the Token-Permissions check will give different scores depending on how well the best practices has been followed. But even if one gets a score of 9, they will still get a High risk issue in the SARIF file. This score then gets shown in the Code scanning alerts dashboard when the Scorecards GitHub action is used in a repository. \nThe risk of the issue that is emitted in the SARIF file should be based on the score of the check. If the score is high, the risk should be lower. \nAn alternative could be for each repository owner to have a policy file to set threshold of risk. \nThis is related to discussion at  This is also relevant for the GitHub workflow dependency pinning: we give 10 points if all actions are pinned by hash, and 8 if the GitHub-owned actions are not."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1320",
				"issue_name": [
					"BUG: Failures in Branch-Protection during release testing"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Release tests found some issues with Branch-Protection check. Looks like related to recent scoring PR. @laurentsimon assigning to you for further debugging.\n./scorecard --repo=github.com/kubermatic/dashboard-v2 --checks=Branhc-Protection\n./scorecard --repo=github.com/xwp/stream --checks=Branch-Protection\n./scorecard --repo=github.com/americanexpress/one-app-cli --checks=Branch-Protection\n\n      ",
				"issue_comment": "Release tests found some issues with Branch-Protection check. Looks like related to recent scoring PR.   assigning to you for further debugging. Release tests found some issues with Branch-Protection check. Looks like related to recent scoring PR.   assigning to you for further debugging. \"reason\": \"could not find branch name release/v2.18: branch not found\", \n  can you take care of this one? ./scorecard --repo=github.com/xwp/stream --checks=Branch-Protection \n./scorecard --repo=github.com/americanexpress/one-app-cli --checks=Branch-Protection \"reason\": \"internal error: invalid score 2 != 0\", \nI will look into these two. Assigning to   the part on version not being found. not urgent It seems like this is because there's a large number of branches (>30) and the GitHub API making the call to   only returns 30 at a time. I'll see if we can pagify/get all. Ah, this was fixed by increasing the  . Is this a hard-limit? I could also increase this so that it makes another API call to fetch branches if it wasn't found in the first 30. I see.   can you chime in? Will this trigger issues with API rate limiting in the weekly cron?  how do we calculate the \"right\" number of  ? Heuristics? Nice catch! I'm a bit hesitant about increasing   since that'll increase the API cost and we don't know how many branches we'll have to iterate through to get the right ones. My suggestion would be to update   to take a   struct that contains an optional list of branch names to retrieve. I haven't found a way to batch retrieve these branches, so we'll then have to make multiple calls to get the BranchProtectionRules for these names. . This still means more API calls but at least since the number of branches we are interested in is probably a handful, it's better than iterating through a bucket load of them. One FYI, GitHub allows you to list all  , which is great but there is no similar option for   which is what we rely on mostly given a read-only token. So another long-term solution here is to work with GitHub to get this exposed. My suggestion would be to update   to take a ListBranchesOptions{} struct that contains an optional list of branch names to retrieve. I think I can work with that! Given that we only want the release and default branches for branch protection, I can plumb that in through setup to create the query for those branch names."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1311",
				"issue_name": [
					"Resolve false positives with Dangerous-Workflow check"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nSee #1283 (comment)\nDangerous-Workflow doesn't handle false positives where a pull_request_target is used with untrusted code checkout BUT token permissions are set and environment protection rules or gating labels are set.\nhttps://dev.to/petrsvihlik/using-environment-protection-rules-to-secure-secrets-when-building-external-forks-with-pullrequesttarget-hci\nReproduction steps\nSteps to reproduce the behavior:\n1.\n2.\n3.\n4.\nExpected behavior\nA clear and concise description of what you expected to happen.\nAdditional context\nAdd any other context about the problem here.\n      ",
				"issue_comment": "\nSee  Dangerous-Workflow doesn't handle false positives where a   is used with untrusted code checkout BUT token permissions are set and environment protection rules or gating labels are set. \nSteps to reproduce the behavior: \n1. \n2. \n3. \n4. \nA clear and concise description of what you expected to happen. \nAdd any other context about the problem here. how about the scenario where   is used and permissions are set to   but   is not   or  ? how about the scenario where ref is used and permissions are set to XXX:write but XXX is not contents or packages? maybe i'm missing something but isn't even read perms dangerous? PR checkout would still allow reading secrets, e.g. with some malicious script in the untrusted checkout you're right, absolutely. I think I was assuming there's no secrets besides the default (permission-restricted) GitHub secret used in the PR. I would say another instance that will likely be a false positive is when the job is gated by an environment (like in scorecard's  ). If the environment requires reviews, then it's definitely a false positive. I expect, though, that we can only see environment protection rules with an admin API token. Maybe when we're not using an admin token and the dangerous job has an environment, we give it a high but not perfect score. When using an admin token, we can check the protection rules for the environment, give it a 0 if it doesn't require reviews, and give it a perfect score if it does. spot on, we got an alert in the scanning dashboard now   would looking for   field be enough to reduce false positives in this case? Or would it introduce false negatives?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1302",
				"issue_name": [
					"Feature: re-enable CI-Tests in cron job"
				],
				"issue_label": [
					"bug",
					"enhancement"
				],
				"issue_content": "\n          the title says it all.\n      ",
				"issue_comment": "the title says it all. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1271",
				"issue_name": [
					"Feature: enable branch protection on pull request for GitHub action"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          I'd be useful to enable BP on pull request triggers for the GH action.\n      ",
				"issue_comment": "I'd be useful to enable BP on pull request triggers for the GH action. Possible ways to do it: \n \n \n \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1270",
				"issue_name": [
					"Feature: provide an ignore list for Binary-Artifact check in GitHub action"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We want to give the ability of ignoring certain files to developers when applying the score policy. We can add this in the scorecard action.\n      ",
				"issue_comment": "We want to give the ability of ignoring certain files to developers when applying the score policy. We can add this in the scorecard action."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1268",
				"issue_name": [
					"Feature: SAST tool run on PR should count more than those run after merge"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          See #1031 (comment)\n(Additional long-term improvements are in #966 (comment))\nWe would like to give more points to repos that run SAST before merging code, i.e. on pull_request event.\nFYI, cron-scheduled runs are automatically disabled after 60 days of inactivity https://docs.github.com/en/actions/managing-workflow-runs/disabling-and-enabling-a-workflow\n      ",
				"issue_comment": "See  \n(Additional long-term improvements are in  ) We would like to give more points to repos that run SAST before merging code, i.e. on pull_request event. FYI, cron-scheduled runs are automatically disabled after 60 days of inactivity   FYI I'm not sure whether it's the right place to mention this issue with the checks analyzing PRs but since I've run into it mostly using the SAST check I think it would probably make sense to switch from the \"n out of 30 PRs have something\" approach to something that would account for glitches preventing status checks from being registered properly or weird results that GH API returns once in a while. Otherwise it seems even if projects where SASTs are run on PRs were rated higher they would most likely loose some points anyway due to what looks like transient GH issues. For example, LGTM was run in   but GH API says that it wasn't. One option would be to figure out whether LGTM is listed among the apps that can be triggered on pull requests with   and if it's there and it is run in the majority of pull requests it should be safe to say that projects are analyzed with LTGM. I'm not sure about other apps though I'm not sure whether it's the right place to mention this issue with the checks analyzing PRs but since I've run into it mostly using the SAST check I think it would probably make sense to switch from the \"n out of 30 PRs have something\" approach to something that would account for glitches preventing status checks from being registered properly or weird results that GH API returns once in a while. Otherwise it seems even if projects where SASTs are run on PRs were rated higher they would most likely loose some points anyway due to what looks like transient GH issues.  is this a known issue with GitHub APIs? One option would be to figure out whether LGTM is listed among the apps that can be triggered on pull requests with   and if it's there and it is run in the majority of pull requests it should be safe to say that projects are analyzed with LTGM. I'm not sure about other apps though with this API, looks like we could get the last N pushes that have used it; and we'd need to map them to the last PRs. Is my understanding correct? That sound doable. Are there limitations in terms of   can use this API:   Is it accessible by non-apps? summary: \n \n \n \n tool configured on each push => increase the score (we don't do this) It should probably be \"on every PR\".   for example doesn't run SASTs on push events. It runs LGTM on PRs and CodeQL and Coverity once a day due their various limitations. ah, right. Agreed."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1245",
				"issue_name": [
					"Feature: Separate check for policy/score evaluation"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We need to separate check and its policy evaluation. This will allow us to return 'raw' results for users to apply arbitrary policies.\nThe first step is to do the separation within the checks package. Then all checks are migrated, we will create a pkg.RunRawScorecards() and create our default policy (currently the scores) thru an additional call.\nWe will later expose this in the CLI\n      ",
				"issue_comment": "We need to separate check and its policy evaluation. This will allow us to return 'raw' results for users to apply arbitrary policies. \nThe first step is to do the separation within the   package. Then all checks are migrated, we will create a   and create our default policy (currently the scores) thru an additional call. \nWe will later expose this in the CLI We need to separate check and its policy evaluation. This will allow us to return 'raw' results for users to apply arbitrary policies. \nThe first step is to do the separation within the   package. Then all checks are migrated, we will create a   and create our default policy (currently the scores) thru an additional call. \nWe will later expose this in the CLI Cool! Is this part of v4 release? We need to separate check and its policy evaluation. This will allow us to return 'raw' results for users to apply arbitrary policies. \nThe first step is to do the separation within the   package. Then all checks are migrated, we will create a   and create our default policy (currently the scores) thru an additional call. \nWe will later expose this in the CLI Cool! Is this part of v4 release? no. Just too many checks to migrate :-) v5 or v6.  assigning to you for now. cc   this is the  large blocker that will help us provide more granularity in the results and give is the ability to provide remediation steps for each warning. (something that came up in  ) Thanks for CC'ing me,  !  Have you thought of assigning each check a unique ID, and then allowing command-line flags like   as in  We thought about something along these lines early on. I think we'll achieve what you describe with policies. Once we have the separation of checks vs policy, we'll be able to apply  , and even more expressive policies such as   or  , or  . The file approach works well on simple examples, but is hard to generalize to other uses case like, say  . We feel the policies will allow flexibility for advanced users, without bloating the CLI arguments. We will have built-in policies, of course. Wdut? Your ideas sound great.  (I'm not surprised.) glad that it makes sense to you too. Everyone is asking for different exceptions and special cases, policies are the only solution that seemed viable in the end. Let's hope this works"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1242",
				"issue_name": [
					"BUG: ",
					" data format"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Currently, the RepoClient data format uses primitive Golang types. This means that when certain values are not returned, the result struct will have default values set. This can lead to incorrect results.\nWe should update the RepoClient data to use *bool, *string etc. And the convention will be that if the value is non-nil, only then analyze the resut.\n      ",
				"issue_comment": "Currently, the   data format uses primitive Golang types. This means that when certain values are not returned, the result struct will have default values set. This can lead to incorrect results. We should update the   data to use  ,   etc. And the convention will be that if the value is  , only then analyze the resut. Could it be that this causes  ? Most likely yes. I have a PR out to fix this for  :  . With this PR, this is the result I get: Does that align with your expected Scorecard output? It does but with   applied I got  and   look weird I'd say :-) Let me try to rebuild   anew with that PR Looks like in your version of   both   and   are fixed (but it somehow lost the \"Stale review dismissal\" subcheck). In my version of   the two issues are somehow still present but manifest themselves a bit differently. I'm not sure if that helps but just in case Ok, another great catch  ! \n \n \n \n Hope that helps. Closing this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1220",
				"issue_name": [
					"Feature: separate common shell utility function from "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We'd like to be able to re-use some of the functions for other checks, such as Dependency-Update-Tool, see #1175\nSo we should try to separate basic functionalities into their own shell_utils.go file.\n      ",
				"issue_comment": "We'd like to be able to re-use some of the functions for other checks, such as  , see  \nSo we should try to separate basic functionalities into their own   file. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1201",
				"issue_name": [
					"Feature - Scorecard should sign releases with cosign"
				],
				"issue_label": [
					"enhancement",
					"slsa"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScorecard should sign when a new GitTag is pushed to using OIDC and Keyless Option https://github.com/sigstore/cosign\n\nBinaries\nTarball\nThe commit SHA COSIGN_EXPERIMENTAL=1 ./cosign sign-blob --key cosign.key <(git rev-parse HEAD) https://github.com/sigstore/cosign/blob/main/FUN.md\n\nFor now, it should sign with the gpg key and cosign (keyless option)\n      ",
				"issue_comment": "\nScorecard should sign when a new GitTag is pushed to using OIDC and Keyless Option  \n \n \n \n For now, it should sign with the gpg key and cosign (keyless option) This can be achieved by using goreleaser build hooks Here is the output of  The subject is  And here is the output of the container that was pushed by GH Action  The subject is   /   This proves the   is not the   who pushed the  .  We discussed this in our meeting to confirm this. Let me know if you have any other questions. This looks great! But since this is for a docker image, and we are concerned with the release artifacts in this issue, want to clarify that my understanding translates well: \n \n \n \n  for any additional feedback. This proves the Subject is not the author who pushed the tag. We discussed this in our meeting to confirm this. Yeah, to get around this you would need the workflow to sign with an annotation over the GIT_SHA, and then have the author sign the GIT_SHA Is it possible to add a GIT_SHA annotation when signing? Yes, with  Can we upload the .sig file along with the released artifact? For example, along with scorecard_x.x.x_linux_amd64.tar.gz upload a scorecard_x.x.x_linux_amd64.sig file. Yes, you can detach the signature with cosign and upload it The expected Subject in this case would be github.com/ossf/scorecard/blob/main/.github/workflows/goreleaser.yaml, correct? Yes, but also with  Awesome, thanks  ! So along with  , we need the   annotation and a step to upload the   files.  let me know if you are interested in taking this. It could be removed with   resolved, but yes! Awesome, thanks  ! So along with  , we need the   annotation and a step to upload the   files.  let me know if you are interested in taking this. I will take it. Thanks Is the plan to sign in a GH workflow or from GCP? Is the plan to sign in a GH workflow or from GCP? GH Workflow. This proves the Subject is not the author who pushed the tag. We discussed this in our meeting to confirm this. Yeah, to get around this you would need the workflow to sign with an annotation over the GIT_SHA, and then have the author sign the GIT_SHA Is it possible to add a GIT_SHA annotation when signing? Yes, with    The   does not have   The cosign sign-blob does not have -a, --annotations Ah, this is true. Blobs don't have layers so annotations can't be stored there. In that case it will need to come from the cert from issue  You may still sign-blob with keyless, but the GIT_SHA will be missing until that issue is resolved.  The cosign sign-blob does not have -a, --annotations Ah, this is true. Blobs don't have layers so annotations can't be stored there. In that case it will need to come from the cert from issue  You may still sign-blob with keyless, but the GIT_SHA will be missing until that issue is resolved. Thanks, Do you have an idea as to when is it likely to be worked up on  ? Should be in pretty soon! You can still use sign-blob as is, just won't see git_sha in the cert yet. Should be in pretty soon! You can still use sign-blob as is, just won't see git_sha in the cert yet. +1 Should be in pretty soon! You can still use sign-blob as is, just won't see git_sha in the cert yet. Thanks! Ran into this issue    Ran into this issue  Marking   as a duplicate of this and closing the other issue. Keyless mode is not the only option for providing provenance. There are other ways to do the same thing like the following (I mentioned them in the issue that was closed a while ago): \n \n \n \n \n This one is for keyless mode  \n \n \n \n , this issue also duplicates with    looks like   will make everything simpler, thanks to  Keyless still has an issue in rekor when uploading binaries  . Till there is a solution for this we cannot sign-blob using keyless. Keyless mode is not the only option for providing provenance. There are other ways to do the same thing like the following Thanks  ! Looks like the   would be the simplest/easiest to implement. Any advantages or reasons to prefer the keyless way over this?     for feedback. this issue also duplicates with  It's related, but it's for the Docker container images rather than the release artifacts. So keeping it open since this issue does not solve  . Hello  ,  I have great news for you. Recently, we've opened an issue in GoReleaser to support keyless mode.  There are lots of helpful information and conversation in there. I highly recommend you to take a look at those.  Also, we already did some work on the ko project to adapt to these new changes , and we are now waiting for the new releases for both cosign and GoReleaser projects. We can do the same for the scorecard project too 🙋🏻‍♂️ \n   \n   \n Keyless mode is not the only option for providing provenance. There are other ways to do the same thing like the following Thanks  ! Looks like the   would be the simplest/easiest to implement. Any advantages or reasons to prefer the keyless way over this?     for feedback. this issue also duplicates with  It's related, but it's for the Docker container images rather than the release artifacts. So keeping it open since this issue does not solve  . Advantages of Keyless       wrote about this. Like I mentioned before Keyless signing of blobs is not possible until the   issue is fixed  We already have   singing our binaries, so we aren't missing signing. The only advantage of signing our binaries with   public key/private key to sign our binaries is that we get the transparency log entry (rekor) compared to  . So we should wait until we get the KeyLess option and also Keyless is still experimental and not yet GA.    this blog post  also explains  . \n   \n    GoReleaser v1.0.0 has just been released and is ready for the Keyless signing with cosign. .    GoReleaser v1.0.0 has just been released and is ready for the Keyless signing with cosign. . Thanks  does this unblock us now from implementing this issue or do we have pending blockers still? Till this fixed   no one can use rekor with large files for signing blob. So it is still a blocker. Makes sense, thanks! I heard about cosign but have never used it. I wonder if it's production-ready so to speak in the sense that it could be easily used by people packaging software to verify tarballs and maybe could even be integrated into package managers? So far everybody seems to have been using GPG partly because it seems to be everywhere. (I'm sorry if it's completely off-topic here. I'm just trying to figure out what cosign is and whether it could potentially be used to sign releases consumed downstream somewhere) While I was experimenting with signing and verifying blobs without keys I came across  , which I think answers my question regarding whether   is production-ready or not. I should have probably read the documentation first :-) Looking for packages I noticed that it seems it's packaged on Arch Linux only:   so my takeaway is that cosign looks promising but it doesn't seem to be possible to switch to it at this point unfortunately if releases are supposed to be verified by people packaging software for various distributions."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1200",
				"issue_name": [
					"Feature - Record scorecard card scans into Rekor"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThe scorecard scans should attest that the scan was done to a repository state (commit SHA) or binary release. https://rekor.sigstore.dev https://github.com/sigstore/rekor\n      ",
				"issue_comment": "\nThe scorecard scans should attest that the scan was done to a repository state (commit SHA) or binary release.    This would be pretty easy to do! do you mean the scorecard scans from the cron jobs? This would be pretty easy to do! do you mean the scorecard scans from the cron jobs? As of now in the cronjobs later in the GitHub Actions because we can utilize the OIDC Provider in GitHub Actions and probably in cronjob (if not KMS) Or should we wait for in-toto attestations   before we do this? in-toto attestations for scans  Question on storing the scorecard scans in rekor . \nscorecard uses the Git SHA as the version when performing scans compared to semver  for other binary releases. \nDoes it make sense to utilize in-toto format? If we use the in-toto format for storing scans, how can consumers search for Scorecard scans based on repository and a commit SHA? We can use the existing in-toto attestations.  are you still working on this?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1196",
				"issue_name": [
					"Feature: add unit test for ",
					" function in "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Function getEnabledChecks() determines which checks are enabled, using a combination of 1) supported checks for the repo interface, 2) wether a policy is given or not, 3) whether --checks is passed as argument or not.\nChanging this behavior would be a breaking change.\n      ",
				"issue_comment": "Function   determines which checks are enabled, using a combination of 1) supported checks for the repo interface, 2) wether a policy is given or not, 3) whether   is passed as argument or not. Changing this behavior would be a breaking change. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1194",
				"issue_name": [
					"Feature - Managed make parser"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThe makefile parser is homegrown and harder to maintain\nDescribe the solution you'd like\nExplore in using https://github.com/mrtazz/checkmake\n      ",
				"issue_comment": "\nThe makefile parser is homegrown and harder to maintain \n \nExplore in using  Stale issue message We do parsing of makefiles in scorecard? Or are you saying that   is hard to maintain? The feature ask is to have a Makefile parser similar to the GitHub workflow parser we currently have. It can potentially be used to analyze build scripts in repos. Ok, it'd be used for things like  ? Yes,   would be a great usecase for this.   fyi. Yes I want this parser for Makefiles! Let's make it for v5 release if nobody objects! Thanks for the find  \n  if you want to do more work on pinning dependencies, feel free to assign to yourself :-)"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1188",
				"issue_name": [
					"Feature - Vendor dependencies for hermetic builds"
				],
				"issue_label": [
					"enhancement",
					"needs discussion",
					"slsa"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nVendor dependencies for hermetic builds.\n      ",
				"issue_comment": "\nVendor dependencies for hermetic builds. Can you elaborate a little more? \n \n \n Can you elaborate a little more? \n \n Let's do it for scorecard builds \n \n    - Download the dependencies so that we can build without network access. Dependabot can update vendored dependencies. where does go stored the vendored dependencies?    and  here are a couple of examples \n \n \n do you know how much latency this would add to the download of the tarball? \nLet's add this topic to the agenda for next meeting? do you know how much latency this would add to the download of the tarball? \nCould you please explain which tarball? The   or our dependencies. Let's add this topic to the agenda for next meeting? \nYes I will. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1174",
				"issue_name": [
					"Feature: parse lock files and verify hashes are present"
				],
				"issue_label": [
					"enhancement",
					"help wanted",
					"priority"
				],
				"issue_content": "\n          I've learned today that certain lock files don't actually contain a \"hash lock\". Npm, for example, allows unlocked dependencies in their lock files. So we should verify the lock is present.\nFor Npm:\n      \"resolved\": \"some-link\",\n      \"integrity\": \"sha1-blabla\",\n\nIf the lock supports sha-2, that's even better.\n      ",
				"issue_comment": "I've learned today that certain lock files don't actually contain a \"hash lock\". Npm, for example, allows unlocked dependencies in their lock files. So we should verify the lock is present. For Npm: If the lock supports sha-2, that's even better.  FYI as per this comment  , we have a broader plan to inventory all the quirks in package managers, possibly starting with npm. \nFYI    FYI    I've learned today that certain lock files don't actually contain a \"hash lock\". Npm, for example, allows unlocked dependencies in their lock files. So we should verify the lock is present. For Npm: If the lock supports sha-2, that's even better.  Sorry not clear about this, would it be possible to link an example ?. Thanks. Let's look at the following hypothetical package-lock.json: This is what we would expect be present for every dependency in the lock file. But (surprise), the lock file may contain non-pinned dependencies (I suppose for flexibility), so the following content: is valid. BUT it does no pin the dependencies. So we need to explicitly check for the presence of these lines. Let me know if this is still unclear. an update: if package.json lists   and the lock file does not list it,   will automatically fetch the latest version of  . So I thin we also need to verify that all dependencies present in package.json are present in package-lock.json. Let me know if you disagree.  any further question I can help with? Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1144",
				"issue_name": [
					"Feature: crowdsourcing scorecard run via GitHub action"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We (scorecard team) run scorecard weekly on 200k repos.\nThis documentation proposes an alternative: let repo owners run scorecard in a GitHub workflow. How do we trust the results then?\nThis is what the proposal is about, by using OIDC flow.\n      ",
				"issue_comment": "We (scorecard team) run scorecard weekly on 200k repos. This documentation proposes an alternative: let repo owners run scorecard in a GitHub workflow. How do we trust the results then? This is what the   is about, by using OIDC flow. Great idea! This should help us a lot with scaling! Assigning to   since she's helping with this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1107",
				"issue_name": [
					"Project mvdan.cc/sh/"
				],
				"issue_label": [],
				"issue_content": "\n          Scorecard uses the mvdan.cc/sh/ for parsing shell-script. This is maintained by a single individual and is looking for sponsorship.\n\nConsider becoming a sponsor if you benefit from the work that went into this release! https://github.com/sponsors/mvdan\n\n\n#1098\n      ",
				"issue_comment": "Scorecard uses the mvdan.cc/sh/ for parsing shell-script. This is maintained by a single individual and is looking for sponsorship. Consider becoming a sponsor if you benefit from the work that went into this release!  \n Maybe good opportunity for  , assuming scorecard is considered a critical piece of software    FYI    Any updates on this? We use this library extensively Maybe good opportunity for  , assuming scorecard is considered a critical piece of software Stale issue message I don't know why this was closed. Probably mistake."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1100",
				"issue_name": [
					"Make Packaging check 'job'-aware"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          currently the Token-Permission workflow works at the workflow level to decide whether certain dangerous permissions are acceptable (requiresPackagesPermissions and isSARIFUploadWorkflow). We should make the check 'job'-aware, since jobs run isolated from each other.\n      ",
				"issue_comment": "currently the Token-Permission workflow works at the workflow level to decide whether certain dangerous permissions are acceptable (  and  ). We should make the check 'job'-aware, since jobs run isolated from each other. FYI  Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1097",
				"issue_name": [
					"BUG: githubv4.Query: Resource not accessible by integration in Branch-Protection"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          An interesting error came up in the run for the GitHub action Branch-Protection\nerror during branchesHandler.setup: internal error: githubv4.Query: Resource not accessible by integration\nNote: this was for a push event, not a PR. Let's see if this continues in next push. It did not happen before in previous pushes.\nI've never seen it before. @azeemsgoogle ideas?\n      ",
				"issue_comment": "An interesting error came up in the run for the GitHub action  Note: this was for a push event, not a PR. Let's see if this continues in next push. It did not happen before in previous pushes. I've never seen it before.   ideas? Possibly similar to  . Need to investigate further. This seems to still be occurring:  another occurrence in  another one  Apparently, this happened all the time in  \n  does the problem occur on pull requests or on push events to main branch? Apparently, this happened all the time in  \n  does the problem occur on pull requests or on push events to main branch? I see this error both for pull requests and push. BTW this error is probably due to a token permissions issue. The workflow only has  . Are you calling a GitHub API to get this result? I agree it looks like a permission problem. We use   and  ; and this error only happens for the Branch Protection APIs using graphQl. Mhhhh.. this page   states   I think that's the cause. We're using the GitHub token provisioned to the workflow, but it's not an OAuth or PAT. If that's the reason, looks like we would either need to use the RESTful APIs for the GitHub action, or disable branch protection in GitHub action. We initially moved away from REST APIs to graphQl because of rate limiting. In the case of a GitHub action, developers only need to access their own repo so we don't really need to worry about rate limiting: that seems to be confirmed since other checks use REST APIs and appear to be working as expected.  wdut?  the graphQl APIs seem to require a PAT/OAuth token and don't automatically work with the GitHub token provisioned to workflows. Do you know the reasoning behind this? Is there a particular permission needed for the workflow GitHub token to make this work? I agree it looks like a permission problem. We use   and  ; and this error only happens for the Branch Protection APIs using graphQl. Mhhhh.. this page   states   I think that's the cause. We're using the GitHub token provisioned to the workflow, but it's not an OAuth or PAT. If that's the reason, looks like we would either need to use the RESTful APIs for the GitHub action, or disable branch protection in GitHub action. We initially moved away from REST APIs to graphQl because of rate limiting. In the case of a GitHub action, developers only need to access their own repo so we don't really need to worry about rate limiting: that seems to be confirmed since other checks use REST APIs and appear to be working as expected.  wdut? JFYI - I realized that looking up branch protection requires  , and I believe   does not have   permission. So even if Graph API had the same permission model, I doubt this would work with the  ... Interesting, thanks for sharing! I don't understand why the GitHu tokens have different permission models though :/ Thanks for the info! Do you know the reasoning behind this? Is there a security concern? Or is this something that may get fixed in the future? The issue is that the generic actions does not get the right permissions. , if this is the case, then moving to REST API won't solve the problem. Basically, we'll need PAT if we want to run Branch-Protection. Have you tried running the Action without the Branch-Protection check if that works? yes I meant to mention that. It may be the case that REST APIs will never work. I have a TODO item to test it out in the action using  . Will post my result. But I'm afraid you''re right,  SG. Will assign this to you for now. Can close this issue after confirming.  reviving this thread. As we try to make scorecard as friction-less as possible for installation, we realize the PAT requirement does create friction. I wanted to understand what you meant by  What can workflow tokens not expose a read permission to read branch protection settings? What is the security concern? \nI know there's a new   permission that was introduced for OIDC, for example. Thanks Jose. Let us know what you find. So long as the permission is controlled by a workflow field, it does not strike me as being more dangerous than   or   or  . Feedback from our team is that to get branch protection rules you'd have to have admin:read which is too broad to enable. cc  Thanks for the info. Fyi, branch protection is accessible with a PAT owned by a non-repo maintainer, except 3 settings that require   as as repo maintainer. I'd expect the same should work for a workflow token. I don't follow why    for the repo itself should be necessary to read other settings, since it's available publicly anyway. Also, in general, using ephemeral workflow tokens is beneficial because PATs cannot be rotated easily. Let me know if I mis-understood something."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1095",
				"issue_name": [
					"BUG - Pinned-Dependencies should not look into vendor directories"
				],
				"issue_label": [
					"bug",
					"Pinned-Dependencies"
				],
				"issue_content": "\n          Describe the bug\ngo run . --repo=github.com/hyperledger/fabric --checks=Pinned-Dependencies --show-details\nvendor/github.com/nxadm/tail/Dockerfile:\n\nvendor/github.com/nxadm/tail/Dockerfile:                                                                                        \n'go get -v github.com/nxadm/tail'\n\n      ",
				"issue_comment": "\ngo run . --repo=github.com/hyperledger/fabric --checks=Pinned-Dependencies --show-details  FYI... is there value in alerting repo owners that some code in their repo downloads software unpinned, even if it's in   directory? Wdut? Instead of disabling the check for vendor directories, an alternative would be to provide an option to configure which directories to be discarded. The main issue with this is that the default run we make (and made public via BQ table) would not have have the option turned on based on each repo's preference. An alternative solution could be to add this in the  . This way, a result would contain a universal/canonical/ground truth result, and anyone could apply a policy/view to the result (e.g., discard certain directories based on the policy they apply). Maybe worth further discussion in the meeting? Yes, I like the policy idea. It is up to the repository owner to decide what is critical for them. Let's discuss this in our meeting. Completely agree with the benefits of splitting the analysis between the current repository and its (vendored) dependencies, as at the moment this results in \"false positives\" - considering the target repository as scope of the pinned dependencies scoring. To some extent, anything related to a project's dependencies could be better handled on a category of its own around supply chain scoring.    did this discussion happen? Was there any agreement on the way forwards? Completely agree with the benefits of splitting the analysis between the current repository and its (vendored) dependencies, as at the moment this results in \"false positives\" - considering the target repository as scope of the pinned dependencies scoring. To some extent, anything related to a project's dependencies could be better handled on a category of its own around supply chain scoring.    did this discussion happen? Was there any agreement on the way forwards? We didn't get a chance to discuss about this. Do you have any suggestions?  thank you for the quick reply. My suggestion would be: \n \n \n This way the information is still available, and in both cases actionable. The latter requiring upstream contribution or the replacement of a given dependency. Thanks for the suggestion. If we can easily map vendored dependencies to their original GitHub project - which I think is doable for golang projects - I think that would work. For which other languages would it work or not work? \n \n \n I am not aware of any major language that supports vendoring quite the same way as golang. \n \n \n This may work for NodeJS, but I am not sure there is a specific structure (or folder name) that is used across the community. thanks. So essentially we need a list of folder names to exclude ( ,  ,  ,  ,  ,  ,  ,  ,  , etc). Is this the best heuristics we can use? Or are there other ways to do this? cc  Yes, that's a good heuristics.  Set up an environmental variable with the list of folder names or paths to exclude.  The folder names vary too much to come up with an absolute list.  It's best to leave it up to the users to edit/modify. On the other hand, make sure these pinned vendor lists are not seen or checked into the public repository.  It can potentially lead to security or data breaches. On the other hand, make sure these pinned vendor lists are not seen or checked into the public repository. It can potentially lead to security or data breaches. what do you mean?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1056",
				"issue_name": [
					"Feature - Web API for Scorecard bigquery data"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          **Is your feature request related to a problem?\nThe scorecard bigquery is accessible only via bigquery client/API. This is an issue in consuming the scorecard data.\nTools like osv are great because it has a REST endpoint https://osv.dev/docs/#section/Getting-Started and it can be consumed by other tools without having to jump through hoops.\nFor example, I was able to build https://github.com/naveensrinivasan/stunning-tribble to validate any new go dependencies against osv with the help of the rest API and not needing a google cloud account.  This brings an advantage of now new dependencies are checked against for OSV's (let's set aside the existing dependencies can have new OSV/CVE which we have to validate as a cron job and also this is an option https://docs.github.com/en/code-security/supply-chain-security/managing-vulnerabilities-in-your-projects-dependencies/about-alerts-for-vulnerable-dependencies#detection-of-vulnerable-dependencies)\nHTTP is the standard for querying data.  Now with 150k repositories, we will have collected significant data and it would be best only if it is accessible as an interface that the community wants in the format.\nDescribe the solution you'd like\nProvide a REST endpoint over Scorecard data that can be retrieved from the BigQuery and return as JSON\n      ",
				"issue_comment": "**Is your feature request related to a problem? \nThe scorecard bigquery is accessible only via bigquery client/API. This is an issue in consuming the scorecard data. Tools like   are great because it has a REST endpoint   and it can be consumed by other tools without having to jump through hoops. For example, I was able to build   to validate any new   dependencies against   with the help of the rest API and not needing a google cloud account.  This brings an advantage of now new dependencies are checked against for OSV's (let's set aside the existing dependencies can have new OSV/CVE which we have to validate as a cron job and also this is an option  ) HTTP is the standard for querying data.  Now with   repositories, we will have collected significant data and it would be best only if it is accessible as an interface that the community wants in the format. Provide a REST endpoint over Scorecard data that can be retrieved from the BigQuery and return as     were building a demo that used scorecard data and an API would have been helpful. One of the options that we discussed was to export the BigQuery   run into cloud buckets so that it provides an HTTP endpoint for consumers to query. This avoids us from maintaining a server and handling uptime. Stale issue message We are hoping to start work on this soon. Re-opening."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1032",
				"issue_name": [
					"Scorecard should try to earn a CII Best Practices badge"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScorecard should try to earn a CII Best Practices badge.\nAdditional context\nBoth scorecard & the CII Best Practices badge are OpenSSF projects focused on best practices to improve OSS security. They have different approaches and somewhat different criteria, but I think each would be helped by working on the other. I'm the lead of the CII Best Practices badge project, and I'm trying to meet the scorecard criteria (or complain when I think there's a problem). I think it'd be great if scorecard reciprocated.\n      ",
				"issue_comment": "\nScorecard should try to earn a  . \nBoth scorecard & the CII Best Practices badge are OpenSSF projects focused on best practices to improve OSS security. They have different approaches and somewhat different criteria, but I think each would be helped by working on the other. I'm the lead of the CII Best Practices badge project, and I'm trying to meet the scorecard criteria (or complain when I think there's a problem). I think it'd be great if scorecard reciprocated. Stale issue message Not stale. Happy to help if you'd like! Yes,Thank you! What is the process? I've started the process by clicking \"get a badge\". Waiting for OSSF to accept the OAuth request.  - presuming you're logged into GitHub, the \"get a badge\" click should have gotten you to a list & getting started pretty quickly. If you're hung, something is wrong. To start you should be able to just go to the  , click on \"Get Your Badge Now!\", log in, and tell it the URL of scorecard  . What happened? If you're stuck I want to make sure you (& anyone else) gets unstuck :-). I indeed got a list of repos, but   does not show up. I think it's because the OAuth does not have the scope to read it, I see \"Access request pending\" under Installation > Application  assigning this to you since you earned Scorecard a CII Best Practices   (yay, thanks a lot!). We can close this unless you plan on working on this further and want to use this to track your progress.  assigning this to you since you earned Scorecard a CII Best Practices   (yay, thanks a lot!). We can close this unless you plan on working on this further and want to use this to track your progress. Will keep this open as there's still a bunch to do! :)"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1031",
				"issue_name": [
					"SAST not detected in CII Best Practices badge workflows"
				],
				"issue_label": [
					"bug",
					"help wanted"
				],
				"issue_content": "\n          Describe the bug\nSAST tools are not detected in the CII Best Practices badge code\nReproduction steps\nSteps to reproduce the behavior:\n\nRun scorecard on https://github.com/coreinfrastructure/best-practices-badge\n\nIt reports:\n    {\n      \"details\": [\n        \"Warn: 0 commits out of 30 are checked with a SAST tool\",\n        \"Warn: CodeQL tool not detected\"\n      ],\n      \"score\": 0,\n      \"reason\": \"SAST tool is not run on all commits -- score normalized to 0\",\n      \"name\": \"SAST\"\n    },\n\nExpected behavior\nFirst, the blanket claim SAST tool is not run on all commits is way too strong. It's hard to detect such tools in all cases, so the text here is extremely misleading. It should say something like:\nDid not detect a SAST tool that is run on all commits\nSecond, there's an easy case Scorecard should detect:\nIn .github/workflows/main.yaml it uses Brakeman via this line:\n        uses: devmasx/brakeman-linter-action@0dc80fcccf87915ccb1761669014015214f36287 # pin@v1.0.0\n\nBrakeman within a GitHub workflow is easy to detect & is a security-focused SAST tool. That should certainly be detected.\nFYI, most of the analysis of the badge is via CircleCI, not via GitHub workflows. In the long term you may want to add that analysis. Here's an example, for your amusement. Most of its static analysis tools are invoked via a system common Ruby program. In .circleci/config.yml there is this:\n    - run:\n        name:  Run pronto GitHub\n        command: >\n          bundle exec pronto run -f github text\n          -c=$(git log --pretty=format:%H | tail -1) --exit-code\n\nThis runs all the \"pronto\" static checks. You can find out what they are by looking at gems with the name \"pronto-...\" in Gemfile, in this case:\n  gem 'pronto-eslint', '0.11.0'\n  gem 'pronto-rails_best_practices', '0.11.0'\n  gem 'pronto-rubocop', '0.11.1'\n\nESlint and Rubocop are static analysis style checkers mainly, but they do have some security-related rules. rails_best_practices definitely has a number of security-related rules.\nI mention all of this because it's a good example of why detecting static analysis tools across all projects is hard... and thus why the warning message should more clearly acknowledge that scorecard didn't find a tool instead of claiming that there is no tool.\n      ",
				"issue_comment": "\nSAST tools are not detected in the  \nSteps to reproduce the behavior: \n \n It reports: First, the blanket claim   is way too strong. It's hard to detect such tools in all cases, so the text here is extremely misleading. It should say something like: Second, there's an easy case Scorecard should detect: In   it uses Brakeman via this line: Brakeman within a GitHub workflow is easy to detect & is a security-focused SAST tool. That should   be detected. FYI, most of the analysis of the badge is via CircleCI, not via GitHub workflows. In the long term you may want to add that analysis. Here's an example, for your amusement. Most of its static analysis tools are invoked via a system common Ruby program. In   there is this: This runs all the \"pronto\" static checks. You can find out what they are by looking at gems with the name \"pronto-...\" in  , in this case: ESlint and Rubocop are static analysis style checkers mainly, but they do have some security-related rules.   definitely has a number of security-related rules. I mention all of this because it's a good example of why detecting static analysis tools across all projects is hard... and thus why the warning message should more clearly acknowledge that scorecard didn't   a tool instead of claiming that there   no tool. Examples of CI/CD to support (taken from another issue): \n.github/workflows (we support) \n.circleci \n.travis.yml \n.gitlab-ci.yml .github/workflows (we support) I wouldn't say this is fully supported. For example,   has   sending data to   daily. To get   to handle workflows like that it should be changed so that it could analyze the status of GitHub workflows and scanners used to analyze projects. Other than that, as I mentioned in  , the \"last 30 commits\" approach doesn't seem to cover the usual workflow where every PR is analyzed before getting merged. I think the check is misleading for the most part and probably it shouldn't even be used by default at least. : To be fair,   didn't say \"fully supported\" just \"support\". It's challenging to handle all cases. Scorecard needs to keep getting refined to increasingly handle more cases over time. Originally I had proposed this issue to focus on a specific case. This issue could be broadened, or a separate issue could be created to discuss all the cases & then point to this issue as an example of a specific case. Either way works for me. Does somehow have a preference? : To be fair,   didn't say \"fully supported\" just \"support\".  fair enough. Scorecard needs to keep getting refined to increasingly handle more cases over time. Agreed. Though I think until the check can cover most use cases it shouldn't be rolled out by default. I'm not sure maintainers would be happy to receive reports based on the findings of   run by people (or maybe even bots) who didn't even try to figure out what it does. There are already enough \"bug reports\" produced by bespoke static analysis tools (most of which are false positives of course). : To be fair,   didn't say \"fully supported\" just \"support\".  fair enough. Scorecard needs to keep getting refined to increasingly handle more cases over time. Agreed. Though I think until the check can cover most use cases it shouldn't be rolled out by default. I'm not sure maintainers would be happy to receive reports based on the findings of   run by people (or maybe even bots) who didn't even try to figure out what it does. There are already enough \"bug reports\" produced by bespoke static analysis tools (most of which are false positives of course). Thanks for the comments. This is a double-edge sword. On one hand, you're right that users may be upset. On the other, not rolling it out prevents us from gathering improvement suggestions from early adopters.. before the tool reaches wider adoption. We can do 2 things: \n Instead of returning a score of  , we can return an inconclusive result: we would just say \"we could not figure it out\". We already have support for this so it's easy to add. \n Are you interested in creating a tracking issue and add thoughts on how we can improve the situation (I don't mind using this issue, it's up to you)? Something we've thought about before is to add support for parsing the commands used in   field of the workflow, in order to identify tools that are run without a GitHub action. I think this would complement the current approach. I suspect parsing the shell script (if   invokes one) may probably be needed as well. There is a list of tools to support over time, possibly from these folks   - but it takes time. Once we have the basic covered, it'd be easier to reach out to tool maintainers and ask them to add the name of their utilities. \n Let us know what you think. Community feedback is key to improving scorecard. Instead of returning a score of 0, we can return an inconclusive result: we would just say \"we could not figure it out\". We already have support for this so it's easy to add. Sounds good to me. FWIW I think the Binary-Artifacts check should be demoted as well. Are you interested in creating a tracking issue and add thoughts on how we can improve the situation Unfortunately I can't say I know how to improve the situation. Parsing the run field and bash scripts could certainly help but given that some projects hosted on GitHub send data for analysis bypassing GitHub Actions altogether it doesn't seem to be enough. At the end of the day I think what matters is whether projects are analyzed successfully at least weekly and to find out whether SASTs work their project pages like   or   should be somehow taken into account. Thank you for the suggestion, that's really helpful! Another option would be to detect the   of many CI/CD pipeline configuration files, and give either \"we don't know\" or give a little credit if Scorecards isn't parsing them. Parsing all such files is challenging, and determining what they do is even harder, but their complete   could be considered considered concerning. thanks all. So to summarize: \n \n \n \n \n I'd add that I think projects analyzed more frequently should have higher score. Currently the SAST check would rate  projects analyzed by CodeQL, say, once a week higher than projects where every PR is analyzed before getting merged because it's much more likely that frequent workflows fail due to different transient issues related to the availability of SASTs for example. I don't think that's fair. Parsing all such files is challenging, and determining what they do is even harder, but their complete lack of presence could be considered considered concerning Looks like it would just turn the SAST check into the CI check (where as far as I understand the presence of various CI services is checked for the most part) I kind of complained about weird results   produces in  Anyway, on an unrelated note, somehow systemd and a project maintained by exactly one person pushing commits directly to the master branch with no PRs, protected branches, reviews, tests and fuzz targets whatsoever keep receiving approximately the same \"global\" score from scorecard (probably due to inconclusive checks). Apart from bugs I've reported and fixed I think at least partly those results can be explained by the fact that if we're talking about SASTs for example   can't tell the difference between projects taking this stuff seriously like systemd where at this point 3 different SASTs are used on a daily basis with graphs showing that those reports are really fixed and projects where one action is run hopefully weekly. I think that apart from frequency the number of SASTs should affect the score as well. I kind of complained about weird results   produces in  Anyway, on an unrelated note, somehow systemd and a project maintained by exactly one person pushing commits directly to the master branch with no PRs, protected branches, reviews, tests and fuzz targets whatsoever keep receiving approximately the same \"global\" score from scorecard (probably due to inconclusive checks). Apart from bugs I've reported and fixed I think at least partly those results can be explained by the fact that if we're talking about SASTs for example   can't tell the difference between projects taking this stuff seriously like systemd where at this point 3 different SASTs are used on a daily basis with graphs showing that those reports are really fixed and projects where one action is run hopefully weekly. I think that apart from frequency the number of SASTs should affect the score as well. you're right. It's a current limitation. We should check the number of non-fixed problems reported by the SAST. We d'like to also do Point 2 of  , but this is such a huge project... I'd add that I think projects analyzed more frequently should have higher score. Currently the SAST check would rate projects analyzed by CodeQL, say, once a week higher than projects where every PR is analyzed before getting merged because it's much more likely that frequent workflows fail due to different transient issues related to the availability of SASTs for example. I don't think that's fair. I think scorecard should only look at the run on the last commit of the PR, in which case all your PRs would be \"pass\" and it should not penalize you. If that's not what we do, we should change it. I agree on giving more weight to SAST-on-PR vs SAST-as-cron or push request. We have not had the time to implement this yet. I'll create a PR for both. Parsing all such files is challenging, and determining what they do is even harder, but their complete lack of presence could be considered considered concerning Looks like it would just turn the SAST check into the CI check (where as far as I understand the presence of various CI services is checked for the most part) here is a linter action we should support  \nLinters are not exactly SASTs, though. Maybe give a few points for having it"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1024",
				"issue_name": [
					"BUG: contributor checks does not validate number of companies per contributor"
				],
				"issue_label": [
					"bug",
					"good first issue",
					"hacktoberfest",
					"help wanted"
				],
				"issue_content": "\n          a contributor can forge their company association on GH. (tracked in another issue).\nIn addition, the number of companies is not verified by our code. That makes it easier for a single user to commit 5 PRs and add 3 companies to their profile, hence getting a top score.\nWe should only take a single company per user.\n      ",
				"issue_comment": "a contributor can forge their company association on GH. (tracked in another issue). \nIn addition, the number of companies is not verified by our code. That makes it easier for a single user to commit 5 PRs and add 3 companies to their profile, hence getting a top score. We should only take a single company per user. Stale issue message  offered to take this up (ref:  ) Awesome, thanks  Thank you     \nRegards, Hi   I would like to know more about this issue. \nKindly let me know if we can huddle for sometime to understand the background of the issue. \nThank You There are 2 sides: \n At the time we created the issue, we fetched the company's name from a field that users can update arbitrarily. So for example, users could set the field to \"Google\" even though they were not part of the Google org. I'm not 100% sure whether the current codebase has the same problem or not. You nee to check and see if you can forge these result... which is fun! \n We don't enforce a number of companies per user. So as a user, I can add 10 companies and it would pass the check with high score. I think we should enforce 1-2 companies per user max. Or maybe even just 1 company. \n Please let me know if this help or not. Hi  , I see some settings in  , is that somewhere I need to make the changes? const ( \nminContributionsPerUser    = 5 \nnumberCompaniesForTopScore = 3 \n// CheckContributors is the registered name for Contributors. \nCheckContributors = \"Contributors\" \n) Kindly guide me. Thank You do I need to add companies in github profile and test it ? Hi  , I see some settings in  , is that somewhere I need to make the changes? const ( minContributionsPerUser = 5 numberCompaniesForTopScore = 3 // CheckContributors is the registered name for Contributors. CheckContributors = \"Contributors\" ) Kindly guide me. Thank You I think this requires no changes do I need to add companies in github profile and test it ? yes it would be great to see if this is forgeable or not. Thank You "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/1014",
				"issue_name": [
					"Dependency-Update-Tool: What about libraries (not applications)?"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          The Dependency-Update-Tool text seems to assume that only applications are considered.\nIf a library uses pinned dependencies then this text also makes sense. However, libraries generally aren't pinned, so \"updating dependencies\" doesn't make as much sense. A variant of this text might make sense, e.g., libraries shouldn't force their users to use known-vulnerable libraries, especially after some grace period. However, that's more complicated to word & it's not clear to me that current tools can do this well (other than noticing when a library forbids the use of later versions).\nThis probably needs more careful wording to deal correctly with libraries.\n      ",
				"issue_comment": "The Dependency-Update-Tool text seems to assume that only applications are considered. If a library uses pinned dependencies then this text also makes sense. However, libraries generally aren't pinned, so \"updating dependencies\" doesn't make as much sense. A variant of this text might make sense, e.g., libraries shouldn't force their users to use known-vulnerable libraries, especially after some grace period. However, that's more complicated to word & it's not clear to me that current tools can do this well (other than noticing when a library forbids the use of later versions). This probably needs more careful wording to deal correctly with libraries.  assigning to you if you don't mind. Libraries using dependency \"ranges\" is indeed not only a valid use case, but in most cases desirable in order to: \n \n \n When something is   a library (i.e. it's an application), I do believe they should pin versions - but not necessarily for security reasons. And I should also point out that a lot of people really resist this, so we try not to be too opinionated. If a project commits a lock file then I think that covers most of the security risk. Certainly if I had to choose between a project pinning their direct dependencies in   versus locking their entire dependency tree in   then I'd choose the latter. Considerations: \n \n \n I think the answer is probably: \n \n \n In other words, if you're a library with one dependency  , there should be no recommendation that you pin that to an exact version. However, if the latest version of   is   then there should be a recommendation for your library to support either   or   only. I'm not certainly about whether to urge a library to support lock files or not. There is certainly very split opinion from open source maintainers who don't want the noise/hassle of keeping lock files updated when they manage dozens or hundreds or packages - even if automation is possible using e.g. Renovate. Applications should have a lock file IMO, although that still doesn't protect   of any application with dependencies at install time because lock files are not published to e.g. npmjs. The only exception was the rarely used and mostly disliked  . In other words the threat vector is: \n \n \n \n The above would mean that anyone running   would end up with   almost immediately after it's published. Thanks for reaching out. We need some external feedback and validation! Libraries using dependency \"ranges\" is indeed not only a valid use case, but in most cases desirable in order to: \n \n \n When something is   a library (i.e. it's an application), I do believe they should pin versions - but not necessarily for security reasons. And I should also point out that a lot of people really resist this, so we try not to be too opinionated. Our doc   says the same for applications. However, our code has not caught up with the recommendation yet - it looks for lock files   which typically means pinning by hash. We currently advise pinning container images by hash, but pinning applications by versions. I'm not sure why we treat containers and package dependencies differently.  do you remember the reasoning? If a project commits a lock file then I think that covers most of the security risk. Certainly if I had to choose between a project pinning their direct dependencies in   versus locking their entire dependency tree in   then I'd choose the latter. Considerations: \n \n It seems in certain cases a package can contain both an application and a module, e.g.  's   and  . How does npm handle the presence of a lock file in this case? It applies it to both   and  ? \n \n I think the answer is probably: \n \n I think this is in line with our doc. \n \n we don't have this right now in our doc. Thanks for the suggestion. In other words, if you're a library with one dependency  , there should be no recommendation that you pin that to an exact version. However, if the latest version of   is   then there should be a recommendation for your library to support either   or   only. I'm not certainly about whether to urge a library to support lock files or not. There is certainly very split opinion from open source maintainers who don't want the noise/hassle of keeping lock files updated when they manage dozens or hundreds or packages - even if automation is possible using e.g. Renovate. Applications should have a lock file IMO, although that still doesn't protect   of any application with dependencies at install time because lock files are not published to e.g. npmjs. Is it fair to say this is a gap in the toolchain? Something we should recommend to package managers' maintainers? The only exception was the rarely used and mostly disliked  . We currently don't differentiate between shrinkwrap and standard lock files in the implementation or in the documentation. Is it worth recommending this file since it covers a stronger threat model (users of the package will get the same dependencies's versions)? In other words the threat vector is: \n \n \n \n The above would mean that anyone running   would end up with   almost immediately after it's published. Would it be benefitcial to get more stakeholders involved in this discussion and write a recommendation doc with all nuances clarified (for each package manager/language, for libs vs applications) present it at the   and then implement/document it in scorecard?  wdut?  do you have a contact from dependabot we may loop in for this discussion? Stale issue message text has been updated in our docs. We no longer check for package managers' lock file. We're thinking of a more generic solution that relies on package manager's features to enforce pinning: pip's  , npm's  , etc"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/994",
				"issue_name": [
					"Feature: Pin dependencies for other CI/CD"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We should add support to other CI/CD, such as circleCI, etc.\nExample file for circleci: https://github.com/coreinfrastructure/best-practices-badge/pull/1647/files#diff-78a8a19706dbd2a4425dd72bdab0502ed7a2cef16365ab7030a5a0588927bf47\n      ",
				"issue_comment": "We should add support to other CI/CD, such as circleCI, etc. \nExample file for circleci:  Examples of CI/CDs: \n.github/workflows (we do support) \n.circleci \n.travis.yml \n.gitlab-ci.yml Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/986",
				"issue_name": [
					"BUG: unit tests"
				],
				"issue_label": [
					"bug",
					"critical"
				],
				"issue_content": "\n          See comment in #980\n      ",
				"issue_comment": "See comment in  Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/982",
				"issue_name": [
					"Feature: automated semver releases"
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          We use a webhook to create stable docker images when a release test run successfully completes. Similarly we should have a webhook, such that when prod cron job completes successfully, we use semver specification to automatically create a new tag and release. On a high-level this is what the webhook needs to do:\n\nWe'll create and maintain a Scorecard GitHub app whose token/keys will be used for automated signing of tags/commits.\nUse the latestTag of Scorecard repo and the commitSHA of the successful run to get all commits in-between.\nDepending on the commit message headings (:sparkles:, :bug:, :book: etc.) decide whether to increase the PATCH version or the MINOR version. MAJOR version changes will be done manually and need not be handled through the webhook.\nCreate a new tag in compliance with semver semantic and push this new tag to Scorecard repo. This should trigger a new release and our work is done.\n\nhttps://github.com/go-git/go-git might come in handy for doing the Git operation in Golang.\n      ",
				"issue_comment": "We use a   to create   docker images when a release test run successfully completes. Similarly we should have a webhook, such that when prod cron job completes successfully, we use semver specification to automatically create a new tag and release. On a high-level this is what the webhook needs to do: \n \n \n \n \n  might come in handy for doing the Git operation in Golang. how do we handle automated release notes? There was a discussion about automated release note generation, can't seem to find the issue/thread for it. Gist was that Kubernetes does something like this already, so we could do it too.   may know more about this. But, to your point, yes we should also consider adding support for automated release note generation also. Let's create a separate issue for automated release note generation? Scope of the issue can be: \n \n \n \n My main concern/question is how we automate release notes that are human-consumable. A list of merged PRs is not very eligible for human. Unless we think it's overkill for minor releases. wdut? Sorry it was not Kubernetes, but   -  , released by  . Like you said, the release notes should be human readable not just changelog. And it is possible to automate it that way. Stale issue message There was a discussion about automated release note generation, can't seem to find the issue/thread for it. Gist was that Kubernetes does something like this already, so we could do it too.   may know more about this. My team manages this tool:  \nHere are some examples of output from the tool: \n \n \n While I haven't personally used it, the tool also supports supplying your own go template to enable further formatting. One thing to note is that it expects a code-fenced block in the PR description (with   as the code type): The idea here being that while we can't always expect the commit history to be perfect (maybe lacking convention or from a new contributor with multiple commits that need squashing), as maintainers we have access to edit the PR descriptions to ensure the   is does a reasonable job describing the change. Some examples:  ,  ,  Similar to how PR titles are validated here, in several Kubernetes repos we automatically block PRs without either: \n \n \n As for releasing: \n \n \n \n  would the work that you are doing on   make it feasible to generate automated release notes?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/967",
				"issue_name": [
					"Feature - Provide docker image sha for failed pinned dependencies "
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nRight now scorecard provides a warning message for  Warn: unpinned dependency detected in Dockerfile: ' alpine:3.14 when the docker file does not have a pinned SHA.\nDescribe the solution you'd like\nInstead of just providing the warning it would be helpful to provide the actual SHA that would have to be used to address this issue.\nThis can be achieved by using https://github.com/google/go-containerregistry/tree/main/cmd/crane and running something like crane digest alpine:3.14 which provides the SHA\nsha256:e1c082e3d3c45cccac829840a25941e679c25d438cc8412c2fa221cf1a824e6a\n      ",
				"issue_comment": "\nRight now scorecard provides a warning message for    when the docker file does not have a pinned SHA. \nInstead of just providing the warning it would be helpful to provide the actual SHA that would have to be used to address this issue. \nThis can be achieved by using   and running something like   which provides the SHA \n Seems like a good idea to me. Good idea. Here are additional thoughts: \n \n \n I think the fact that we can use  , which is written in golang is down to luck. Over-time, we will likely want to suggest other tips for other checks. For example,   found out how painful it is to fix the   check manually  . A tip or a diff would be useful there too. Unfortunately, many tools won't be written in golang (bringing other complications, e.g. sandboxing, etc). Sometimes, remediation tips may require more work and be stateful, which will be hard to handle in scorecard. We probably need a  , where we could implement more complicated logic server-side and provide a REST API for users (scorecard, allstar) to generate specific tips/diff. \n Note that for the specific case of dependency updates/pinning, I opened an issue on dependabot about this  . Basically, if we have an option to turn on   in dependabot config, it would automatically the first hash in a new PR. \n Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/966",
				"issue_name": [
					"Feature - Security Scan "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nShould Scorecard run a security scan on repositories? With something like https://github.com/coinbase/salus\n      ",
				"issue_comment": "\nShould Scorecard run a security scan on repositories? With something like  Here's a question I would ask: Do the raw results of an automated security scan provide a meaningful insight as to the security of a project? I would argue it does not due to the high number of false positives in SAST tools. IMO gosec provides good security defaults. The whole idea is to make OSS aware of security implications that most projects aren’t providing. The goal is to encourage them to address these. Most of these linters have an option nolint, which meant the owner of the repository took their time to address those false positives even if it is. I've thought about this further, and I think this is worth doing. Summarizing yesterday's discussion here: \n \n \n \n \n \n \n \n there are various shades of grey we may consider here (sorry was OOO yesterday): \n \n \n \n One big blocker is false positive, like already mentioned. At this point we need to think about how we handle annotation: language-specific vs language-agnostic? Then, to remove FPs, we could proceed in stages: \n \n \n With a service, we could, long-long-term: \n \n \n \n Note in false positives: all programming languages are not born equal. for C, FP are close to unmanageable in practice. For C++, depends on coding style (use a lot of unique pointers, etc). For most other languages (Python, Golang, Rust, etc) FPs are manageable. I like what you've put down here. I just wanted to note the things you put under long-long term sounds like the things Project Omega from Michael Scovetta aims to do. do you have a link to the project? Yes,  also related (shameless plug)  Stale issue message cc   this thread looks awfully similar to alpha/omega project, as   pointed out. Let's try to ingest their data in scorecard when it's ready; or even work with them."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/935",
				"issue_name": [
					"Granular ",
					" check"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We should make the Vulnerabilities check more granular by taking into account the severity of the vulnerabilities. Example:\nCritical vulnerabilities -> score -= 10\nHigh -> score -= 7\nMedium -> score -= 2\nLow -> score -= 1\nSuggestions welcome. We could also take the time the vulnerabilities have been present for. Bit I think this complicates too much and I'm not sure it helps.\n      ",
				"issue_comment": "We should make the   check more granular by taking into account the severity of the vulnerabilities. Example: \nCritical vulnerabilities -> score -= 10 \nHigh -> score -= 7 \nMedium -> score -= 2 \nLow -> score -= 1 Suggestions welcome. We could also take the time the vulnerabilities have been present for. Bit I think this complicates too much and I'm not sure it helps.  is this something you could do for q4? Severity isn't actually a standard part of the schema, as it's hard to define a standardized value that makes sense for all ecosystems. There's also debate on if a severity value is generally that useful at all -- a real severity to a user depends on they they use it. agreed on the context being important from a dependent's point of view. For the repo owner, severity helps with prioritization and  could have been useful. Any thoughts on how we could make the check more granular? Stale issue message another idea around improving the vulnerability check is the following. Since scorecard always works at HEAD for a repo, it's likely all repos have their OSV/CVE vulnerabilities fixed at HEAD on main. I think we need a way to assess existing vulnerabilities, and the time it take to be fixed. Some examples: \n \n \n \n cc  There is also   that we may use to retrieve existing vulnerabilities from scanners"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/920",
				"issue_name": [
					"Feature: Add support for CVS"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We currently support GitHub. We would like to support other CVS:\n\nGitlab\nBitbucket\nApache Jira\n...\n\nThese are the ones deps.dev, in order of most to least number of projects tracked. Not that those 3 combined amount to less than 2% at the moment.\n      ",
				"issue_comment": "We currently support GitHub. We would like to support other CVS: \n \n \n \n \n These are the ones deps.dev, in order of most to least number of projects tracked. Not that those 3 combined amount to less than 2% at the moment. Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/898",
				"issue_name": [
					"BUG: check information in README.md and other docs should be programmatically generated from checks.md"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          An internal team complained that the description in https://github.com/ossf/scorecard#scorecard-checks is different from those in checks.md (Packaging check)\nWe should pull all the doc for checks from checks.yaml and generate the section of the README programatically. We now have a short field in the checks.md that we may be able to re-use (does not support clickable links, though).\nMore generally, anywhere we refer to checks, we should generate that programatically.\n      ",
				"issue_comment": "An internal team complained that the description in   is different from those in checks.md (Packaging check) We should pull all the doc for checks from checks.yaml and generate the section of the README programatically. We now have a   field in the checks.md that we may be able to re-use (does not support clickable links, though). More generally, anywhere we refer to checks, we should generate that programatically. note that in   ( ) we could also retrieve the check names from the yaml file the run examples should also be generated programatically. the run examples should also be generated programatically. Can you please explain further on run examples? oops, sorry.. that was vague. Ideally   would be generated from a scorecard run to generate the output example. I don't know how feasible this is in practice. another thing we need to fix: We tend to update checks' source code but forget to update the checks' documentation in the same PR. We need to catch this. One idea would be to: \n \n \n In addition to the above, we could also add   in the PR template to inform the reviewer. WDUT? we can do the same for keep the cron section of the readme up-to-date, by monitoring changes under  Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/897",
				"issue_name": [
					"Feature: add other fuzzing platforms to the Fuzzing check"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Our current check only looks for OSS-Fuzz integration. We want to expand our list of supported platforms.\nWhich fuzzing platforms to support remains an open question. Suggestions welcome!\n      ",
				"issue_comment": "Our current check only looks for OSS-Fuzz integration. We want to expand our list of supported platforms. Which fuzzing platforms to support remains an open question. Suggestions welcome! Another one to add would be clusterfuzzlite which is launching soon:  . Thanks Oliver. To detect clusterfuzzlite, we need to check for a workflow which uses   action, is this correct? \nDo you expect users to enable it on PRs or push to main or something else? If this is simple enough, should add this in check soon. What about results of this check from this Fuzz? On a different note should we add this check to scorecard? Thanks Oliver. To detect clusterfuzzlite, we need to check for a workflow which uses   action, is this correct? Do you expect users to enable it on PRs or push to main or something else? Yep! Just check for the   action. I think we can be flexible about where it's enabled (PR or push or otherwise) so we don't need to check that. What about results of this check from this Fuzz? \nOn a different note should we add this check to scorecard?   Could you clarify what you mean here? Are you suggesting a more detailed check beyond just checking if Fuzzing is enabled? Thanks Oliver. To detect clusterfuzzlite, we need to check for a workflow which uses   action, is this correct? Do you expect users to enable it on PRs or push to main or something else? Yep! Just check for the   action. I think we can be flexible about where it's enabled (PR or push or otherwise) so we don't need to check that. What about results of this check from this Fuzz? \nOn a different note should we add this check to scorecard?  Could you clarify what you mean here? Are you suggesting a more detailed check beyond just checking if Fuzzing is enabled? Yes , how do we report the results of these fuzz runs? Thanks Oliver. To detect clusterfuzzlite, we need to check for a workflow which uses   action, is this correct? Do you expect users to enable it on PRs or push to main or something else? Yep! Just check for the   action. I think we can be flexible about where it's enabled (PR or push or otherwise) so we don't need to check that. great! Do you have cycles to add it before you officially release clusterfuzzlite? Or shall we add this to v4 milestone (~EOY)? Thanks Oliver. To detect clusterfuzzlite, we need to check for a workflow which uses   action, is this correct? Do you expect users to enable it on PRs or push to main or something else? Yep! Just check for the   action. I think we can be flexible about where it's enabled (PR or push or otherwise) so we don't need to check that. great! Do you have cycles to add it before you officially release clusterfuzzlite? Or shall we add this to v4 milestone (~EOY)? I can probably knock this one out in the next week or so. Awesome. I've created   and added to v4 milestone. Stale issue message  is there a special command/keyword we can use to say \"this issue should not be closed automatically by the bot\"? I wanted to also not that   has fuzzing out of the box. didn't know if it should be included in this issue or a different one I think it fits in this issue. If you know how to check for its use (there's a command for it?) please let us know. Feel free to send. aPR if you have time for it. Thanks! I'll see what I can do :) I like the idea. In the future scorecard plans to provide fuzzing coverage. Having something like oss-fuzz provides an API to get the coverage metrics. I don't know how we can get it from go 1.18 and verify it is correct. I think oss-fuzz integrates with the native fizzing, but not sure how exactly. \nWorth checking unrelated to my investigation We need to look for the   command in the workflow  . The assumption is that the command is used directly in the workflow, and not in a script that's invoked via the workflow. We can start simple and then iterate, though    could we start with checking for   files in the repository which contain functions matching the regex   ? Would that be a good start for adding this support? I think   takes any string and it's convention to use the prefix Fuzz \nNot that I am opposed to the ideas, but letting you know it's based on convention rather than validation  mentions this as a requirement. But maybe this is not a hard requirement? Did some local testing -   ignores functions not starting with  . yeah, same (verified aswell) \nexpected the   to at least. warn me I think it's fair to start with the regex   provided. The warning is something Go team should fix, rather than us, no? Note that a while back, we were contemplating having a   checks (or part of an existing check) so the work you're doing could be useful in this context too. Yup, it's a go team thing. Noting it here as I didn't start the conversation there If we are extending the Fuzzing check to include looking for fuzzer definitions for Go (which I think is a good idea!), we should also extend this to other languages, which all have a consistent way to do this that we can detect. e.g. for C/C++, we can look for instances of LLVMFuzzerTestOneInput in c/cc/cxx/cpp files.  - can someone from fuzzing team pick this, it will be nice to have this scorecard check comprehensive.  -- this ( ) might be a interesting side project to tackle!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/884",
				"issue_name": [
					"Support Makefiles in ",
					" check"
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          We currently check for the presence of curl | bash and other unpinned dependency patterns for shell scripts in the repo, in GitHub workflows' run, dockerfiles' RUN.\nWe need to do the same for Makefiles, see #427 (comment)\n      ",
				"issue_comment": "We currently check for the presence of   and other unpinned dependency patterns for shell scripts in the repo, in GitHub workflows'  , dockerfiles'  . We need to do the same for Makefiles, see "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/874",
				"issue_name": [
					"Feature: Improve dependabot detection thru PRs or parsing config file"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nWhen running Scorecard against this repo https://github.com/tngan/samlify I get an score 0 on the Dependency Update Tool check, however the repo does have dependabot.\nReproduction steps\nSteps to reproduce the behavior:\n\nRun docker run -e GITHUB_AUTH_TOKEN=<token> gcr.io/openssf/scorecard:stable --show-details --repo=https://github.com/tngan/samlify\nCheck the score of the Dependency Update Tool.\n\nExpected behavior\nDependency Update Check has a non-zero score because the repo has dependabot.\n      ",
				"issue_comment": "\nWhen running Scorecard against this repo   I get an score 0 on the Dependency Update Tool check, however the repo does have dependabot. \nSteps to reproduce the behavior: \n \n \n \nDependency Update Check has a non-zero score because the repo has dependabot. do you have dependabot enabled as a GitHub setting rather than a config file under  ? I'm not the owner of the samlify repository so I can't fully confirm. But after some testing on a private repo I own I noticed that dependabot is not detected if enabled through Github settings, I would suspect same thing is happening with samlify. yep that makes sense. We encourage owners to make their config public. That said, we have plans to improve the check to look for dependabot PRs (accepted/rejected) to make the check more robust. Thanks for reminding us, I realized there was no tracking issue for it. If you're interested in helping, PRs are welcome! Additional note: we should also check for reverted dependabot PRs by maintainers That said, we have plans to improve the check to look for dependabot PRs (accepted/rejected) to make the check more robust. Any ideas on how to implement this ? I've been looking at the Github API and haven't found a way to know if the repository has dependabot (or similar) enabled. \nAnother way would be to check for commits authored by the dependabot in the last N days, what do you think about it ? That said, we have plans to improve the check to look for dependabot PRs (accepted/rejected) to make the check more robust. Any ideas on how to implement this ? I've been looking at the Github API and haven't found a way to know if the repository has dependabot (or similar) enabled. \nAnother way would be to check for commits authored by the dependabot in the last N days, what do you think about it ? I think that is a good start and aligned with what we had in mind. \nDo you know if this API   can list PRs that have been rejected/dismissed as well?  please let us know if you have better ideas/insights for now. FYI         how many days shall we look back for merged PRs? see also a discussion  , between dependabot security and dependabot dep. We should check whether the config file can differentiate between the two types of dependabot. FYI  (renaming the title of this issue to  ) Looking at PRs to get a hint about dependabot usage sounds like a good idea to me. I would say a good starting point would be to use the existing   API. We can then consider expanding the support to non-merged or reverted PRs too."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/859",
				"issue_name": [
					"How can contributors result be trusted, people can create fake organizations"
				],
				"issue_label": [
					"core feature",
					"needs discussion"
				],
				"issue_content": "\n          See title\n      ",
				"issue_comment": "See title related to  somewhat relevant  Stale issue message cc   please feel free to suggest ideas. They may be other fields we can look for besides the general. user description fields, maybe. I understand the issue correctly,   suggested you could look into implementing OIDC on the scorecards service. Scorecard has a check called \"Contributors\" that attempts to list which organization the contributors work for. We don't control external repos' contributors, we're only running scorecard on them. Currently, we're using some information from the user's profile, but the information has no validation and can be set to anything. For example, anyone can add \"Google\" to it. We're wondering if there's way to validate this information. There is this API  , but when I run it on my username, it returns an empty list, even though I'm a member of the Googler org, and external contributor of Google org, for example. one second thought, I think we updated the code and we no longer use the profile's company names, so the data may be correct and not spoofable. It does not seem to work for me though. Not sure if there's something special to make it public or not.  what shall we check for in renovabot config file to see if this is enabled?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/839",
				"issue_name": [
					"BUG: Parsing errors"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Noticing many parsing errors during cron job. Not blocking cron job, but good to fix since otherwise we just return ErrScorecardInternal. Some examples to reproduce shown below:\nscorecard --repo=github.com/ufal/udpipe --checks=Pinned-Dependencies\n\nscorecard --repo=github.com/ubisoft/Sharpmake --checks=Pinned-Dependencies\n\nscorecard --repo=github.com/uber/okbuck --checks=Pinned-Dependencies\n\nscorecard --repo=github.com/uber/NullAway --checks=Pinned-Dependencies\n\nscorecard --repo=github.com/aliyun/aliyun-odps-python-sdk --checks=Token-Permissions,Pinned-Dependencies\n\nscorecard --repo=github.com/alibaba/GraphScope --checks=Token-Permissions\n\nscorecard --repo=github.com/u-boot/u-boot --checks=Pinned-Dependencies\n\nThere might be more cases. Will add as I find them. Would be good to fix these and add these repos to cron/data/projects.release.csv as and when we fix them.\n      ",
				"issue_comment": "Noticing many parsing errors during cron job. Not blocking cron job, but good to fix since otherwise we just return  . Some examples to reproduce shown below: There might be more cases. Will add as I find them. Would be good to fix these and add these repos to   as and when we fix them.  are you interested in helping for this issue? It's in the Pinned-Dependency check that you're looking at to add lines. Sure, I'll take this one. There are several different categories these parsing errors fall into. I'll list them and state how I plan to address them: \n \n \n \n \n Let me know if you have any suggestions. Thanks Chris, this is great! Comments inline: \n \n How would we check for this? If we have parsers that can understand this, that can work. But if we are planning to use string matching to do this, I'm not too sure about this one. \n \n \n \n All of this sound good to me. How would we check for this? If we have parsers that can understand this, that can work. But if we are planning to use string matching to do this, I'm not too sure about this one. This is a pretty simple check, so I think string matching would be appropriate here. I was unable to find any open-source parsers for github workflows. Thanks Chris, this is great! Comments inline: \n \n workflows also have a   param you can additionally look for. How would we check for this? If we have parsers that can understand this, that can work. But if we are planning to use string matching to do this, I'm not too sure about this one. \n \n \n you should not need to parse the file if it's a shell, because it will be done by  . All you need to do is return early. \nWe're already calling   (see  ) to check if a file is a shell script, and I forgot to add it for github workflows :-) So I think just calling isShellScriptFile` should be enough. \n \n this is already done in  , see supported shell names in  \nI think   is missing a call to  All of this sound good to me. Laurent, thanks for the comments. workflows also have a   param you can additionally look for. Is there an example yml file you can point to that shows the   param? a few places: \n \n \n Ok, maybe if all the values in   or   start with \"windows-\", we can assume the script is powershell. Otherwise, we can look for   (using a slightly more sophisticated regex). sounds like a reasonable approach. Also verify that if   is present, at least one windows os is specified in  Found one more. Might be similar to previous ones I found. One more from  The error from   is because there is a   in  . I'll fix this by only trying to unmarshal files in   if it has a   or   extension. From  : Workflow files use YAML syntax, and must have either a .yml or .yaml file extension. The error from   is actually failing on the   file. The same fix should handle this one also. @nathan-415, fyi.   Can we close this?   Can we close this? Not yet. There's still one more issue I need to address: shell code that isn't able to be parsed. I found another one   that fails  Ok, there are 4 repos that give us  . Here's the deal with each of them: \n \n \n \n \n That leads me to a broader question of how we handle shell code that we are unable to parse. Currently, the entire check fails with a score of '?'. What I would like to do instead is log a warning whenever we fail to parse a file, but continue on with the check. It would be good if we put a system in place to monitor for these warnings so we can evaluate if there is a bug in our parsing code. How does this sound? It would be good if we put a system in place to monitor for these warnings +1. See  . You can use that to record/monitor these parse errors. We can start with this to get a sense of how bad the problem is and then decide on whether we should log a warning and continue.  thanks for fixing so many of the parsing issues. The number of parsing errors we see in the cron job has significantly reduced! Some new parsing issues I see as of now: Monitoring data   put in place for shell errors. We seem to consistently have some shell related failures when running the cron job. Although the number is not that high which is a good thing. But we clearly seem to fail parsing shell code on a significant number of repos. Another parsing issue reported by   in  : The Token-Permission check seems to error out on an invalid GitHub workflow from  The mistake is in parsing   in I believe this should be a list of jobs. What happens is that no top-level perms are defined, so this checks run level perms. Then it parses   as a map, when in fact it is a list, and the result is an internal error. , can you provide an example yaml file (or a repo) where this would fail? I looked through   and didn't see any yaml file that looked like it had   as a list instead of a map. , can you provide an example yaml file (or a repo) where this would fail? I looked through   and didn't see any yaml file that looked like it had jobs as a list instead of a map. Ooops! I failed to update this. I found two things \n \n \n The YAML linked in that blog will return github internal error for parsing workflow for the Token-Permissions check   FYI, code is  -ing: On a similar note why not use this library for parsing      ? Only briefly looked at  . Sounds like a good idea to me   .  wdyt?  Thanks for fixing the panicky code. \n  That library looks great! Glad you found it. I would love doing less maintenance for parsing actions. \n  I expect that when we move to the library for parsing actions, the yaml included in the blog will get parsed correctly. If it doesn't, we can look into it further at that time.  Thanks for fixing the panicky code. \n  That library looks great! Glad you found it. I would love doing less maintenance for parsing actions. \n  I expect that when we move to the library for parsing actions, the yaml included in the blog will get parsed correctly. If it doesn't, we can look into it further at that time.  Do you want to do this? Or I can take it.  Do you want to do this? Or I can take it. I can do this one.  Do you want to do this? Or I can take it. I can do this one. Thanks Parsing errors that showed up with the new actionlint parser:"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/792",
				"issue_name": [
					"Feedback on Scorecard result data"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          I had a conversation with Jose Duart from Google and he had some interesting observations on Scorecard data from BQ that he analyzed.\n\nSome repos may be pretty well established (e.g https://github.com/yaml/pyyaml), so not a lot of commits will be happening. We mark these repos as Not Active, which is unexpected from a users POV. We probably need better signals to understand if repos are Active or improve how we score these repos.\nThe Vulnerabilities check simply tells if there is an open vulnerability or not. This is not a strong signal when considering a package as a dependency. A stronger signal might be something like - how long do the repo owners take on average to fix vulnerabilities once they become known.\n\nThese might be interesting points to discuss in our next meeting, so creating an issue.\n      ",
				"issue_comment": "I had a conversation with Jose Duart from Google and he had some interesting observations on Scorecard data from BQ that he analyzed. \n \n \n These might be interesting points to discuss in our next meeting, so creating an issue. I had a conversation with Jose Duart from Google and he had some interesting observations on Scorecard data from BQ that he analyzed. \n \n in addition to whether issues are closed or commented on, etc I think when the repo has dependabot installed we would get some PRs merged once in a while. the statistics API may be useful to assess the activity of a repo  increasing the time period may also be useful - related to  this API may also be useful  another idea is to use the list of transitive deps, see if some have been updated and if the project has accepted dependabot PRs. That's pretty involved, though Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/779",
				"issue_name": [
					"New check: signed commits"
				],
				"issue_label": [
					"duplicate",
					"enhancement"
				],
				"issue_content": "\n          there seems to be a setting under branch protection called Require signed commits which may be useful to add.\nWe could also have this as part of separate check when git/git#1041 is merged and operational.\n@FStelzer @djmdjm do you have some rough estimate when ssh signing git/git#1041 will go live and if it will be supported as part of the branch protection setting?\n      ",
				"issue_comment": "there seems to be a setting under branch protection called   which may be useful to add. \nWe could also have this as part of separate check when   is merged and operational.    do you have some rough estimate when ssh signing   will go live and if it will be supported as part of the branch protection setting? some related discussion at  I can't really give any indication of when this will be generally available. There is still a bit of review / discussion of specifics going on. And even if it is merged/released soon i have no idea on when github might provide the feature. \nI guess when the feature made it into a git release you can open a ticket with github to ask for it. But they'll have to implement some things to manage the allowedSigners as well. I can't really give any indication of when this will be generally available. There is still a bit of review / discussion of specifics going on. And even if it is merged/released soon i have no idea on when github might provide the feature. \nI guess when the feature made it into a git release you can open a ticket with github to ask for it. But they'll have to implement some things to manage the allowedSigners as well. Thanks! duplicate  landed  ! \nTime to open an issue then? I'm actually wondering how a third-party would verify the commits: does GitHub make the public keys available somewhere? landed  ! \nTime to open an issue then? I'm actually wondering how a third-party would verify the commits: does GitHub make the public keys available somewhere? It is a going to take a while before it lands in GitHub The change was merged to master. So it should be in git 2.34 which will probably release in mid november. \nRegarding verification its up to github to either let the repo owner authorize keys or generate a list from users having push access to the repo. You can easily create a list yourself since github has public urls for the ssh keys: \n edit: sorry misread your question. Github could publish the signers file, or you could build an individual one (a ‚trust on first use‘ feature in git will follow). Or a project could require all signed commits and then check a list into the repo itself. Who to trust can still be a personal or a projects decision. The change was merged to master. So it should be in git 2.34 which will probably release in mid november. Regarding verification its up to github to either let the repo owner authorize keys or generate a list from users having push access to the repo. You can easily create a list yourself since github has public urls for the ssh keys:  good first step, but it is not part of the commit history. Makes it hard to retroactively validate commit history via other tooling. To decouple trust from github, may also be useful to have a transparency log for this. There's been a lot of proposal about this in the past, in particular w.r.t identities for chat apps like Signal. Seems like a natural fit for this problem... in theory :-D edit: sorry misread your question. Github could publish the signers file, or you could build an individual one (a ‚trust on first use‘ feature in git will follow). Or a project could require all signed commits and then check a list into the repo itself. Who to trust can still be a personal or a projects decision. agreed it's up to users. the allowedSignersFile has a valid-before/after option for every key and is meant to be edited/appended continually. \nso forges like github/gitlab/bitbucket can edit this file every time a user their access granted or revoked not by adding/removing their key but by setting the validity timestamps and adding the key again in case it gets readded. \nif a user changes their key it would also just set valid-before=now() and add the new one with valid-after=now(). \nonly if you want to invalidate all prior signatures of a key you should move it to the revokedkeys file. this allows for verifying the history up to the point the file was created. if you deem your repo secure enough to also store this allowed signers list you can then add it to the repo and everyone can verify the whole commit history using just plain git (and the necessary ssh-keygen tool). ideally github could default to the users already present ssh key but also allow for uploading one only used for signing (i use different keys for signing & auth). Stale issue message (Noting the official thread for SSH signing:  )"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/744",
				"issue_name": [
					"Feature: Use ",
					" instead of "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Simplify our workflows and instead of Dockerfiles use google/ko.\n      ",
				"issue_comment": "Simplify our workflows and instead of Dockerfiles use  .  Does ko support tags? The cork job uses couple of tags to “latest” and “stable” to differentiate which between test runs?  Does ko support tags? The cork job uses couple of tags to “latest” and “stable” to differentiate which between test runs? hello  , yes   work with tags, let me give you an example: Thank you  ! btw  , we can do this if you want us to do, we are looking for an opportunity to contribute     Thank you! I am assigning it to you. Let me know if you have any questions. Just an FYI there are multiple Dockerfiles in different directories.   It would be great if the new ones can support Multi-Platform Images as an option.  would be useful for reproducibility   Thanks for your interest in Scorecard   and thank you for taking up this issue. Feel free to add this issue to   where we are discussing our next milestone. hello    , I did a bunch of things in the PR but I'm not sure about what I did, so, I just want to discuss a bit about the changes that I did in here: \n \n \n \n \n \n Here's a suggestion - how about we start with a simple PR first which does the following: \n \n \n \n At this point, we'll have a basic setup to generate and test the   image. If there are important diffs in this image, we can iterate until these diffs are fixed. Once, that is accomplished we can attack the problem of replacing   with  . Repeat for all other Dockerfiles. Wdyt?    Here's a suggestion - how about we start with a simple PR first which does the following: \n \n \n \n At this point, we'll have a basic setup to generate and test the   image. If there are important diffs in this image, we can iterate until these diffs are fixed. Once, that is accomplished we can attack the problem of replacing   with  . Repeat for all other Dockerfiles. Wdyt?    I agree with the plan  ! Thanks Hello    , thank you so much for helping me. [x] - Introduces a .ko.yaml file with the right ldflags and other build settings. \n[x] - Adds a new step to dockerbuild in Makefile, which generates a local scorecard-ko image using ko. \n[ ] - Adds a step in the Makefile which diffs the images scorecard-ko and scorecard (image from Dockerfile). Consider using a tool like container-diff. IMHO, this   resolves the ones that I put [x] in front of it. I'll make the third one ASAP because there are some problems with the image name that ko was built via the   flag. Here is why  \n   A great find by  . Adding here since its somewhat relevant to this issue -  Thanks  Adds a step in the Makefile which diffs the images scorecard-ko and scorecard (image from Dockerfile). Consider using a tool like container-diff. Thanks,  ! Can we skip this for this PR?   Thoughts? Sure we can skip it for this PR. Re-opening this since I assume there is more to be done here. Is that correct   ? Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/717",
				"issue_name": [
					"Cleanup verbosity level"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We currently use the zap package for logging. We also have a detail level (warn, info, debug) as part of #712. Can we simplify the code to only use our own internal detail level as logging level, and get rid of zap dependency?\nSee #712 (comment) and #712 (comment).\n@naveensrinivasan @azeemsgoogle\n      ",
				"issue_comment": "We currently use the zap package for logging. We also have a detail level (warn, info, debug) as part of  . Can we simplify the code to only use our own internal detail level as logging level, and get rid of zap dependency? See   and  .   Stale issue message"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/689",
				"issue_name": [
					"Frozen check: differentiate libs and programs"
				],
				"issue_label": [
					"bug",
					"enhancement"
				],
				"issue_content": "\n          A library's dependency should not be pinned. We have a TODO in the code to account for this already, but we have not implemented it yet.\nIt may be possible to detect if the package/code is a library or a program thru the manifest file.\nOther ideas around pinning https://jbeckwith.com/2019/12/18/package-lock/\n      ",
				"issue_comment": "A library's dependency should not be pinned. We have a   in the code to account for this already, but we have not implemented it yet. It may be possible to detect if the package/code is a library or a program thru the manifest file. Other ideas around pinning  cursory read of package.json: \n  entry declares a CLI/program,   declares an entry point for a library, aka module. Example   and  . \nIt is possible to declare a package that contains both a module and a CLI: in this case I think we should flag the use a lock files for the module. A library's dependency should not be pinned. We have a   in the code to account for this already, but we have not implemented it yet.  went through the   branch there isn't any   file ? It's been renamed to  . \nDo you want me to assign this bug to you?  yes please. Thank you :) Thanks   ! Hi  After reading through the comments and looking at   following is my understanding for the frozen check: \n This check is for   project where the code will check for existence of   in root directory of the project \n If the   has   information  (eg:  ) it is considered as  \n If the   has   information (eg:  ) it is considered as  \n Now to the question: \n What score do we want to have for the above ? \n Are there any other check besides   and   ? \n Let me know if I'm on the right track. You're definitely on the right track. That's how I understand   as well. \nIn terms of score: \n \n \n \n Thanks again for looking into this!  I have a question re :   is the understanding of pinned dependencies is as defined here   (under   section) ? So anything without   or   is considered ? Further questions to your answer: \n \n \n \n  I have a question re :   is the understanding of pinned dependencies is as defined here   (under   section) ? correct. So anything without   or   is considered ? In scorecard, we define pinning as  , see   for example. Pinning by version is also possible as in the link above. You make a good point though: we could give some points for pinning by version, because it's better than no pinning at all. Needs more thought about this. Further questions to your answer: \n \n correct. (default is 10) \n \n correct. (default is 10) \n \n Is this situation possible? Let's start by logging and returning an inconclusive result. see   for how hash pinning  I'm working on your PR. Please be patient, I'm trying to understand a little better the difference between   and  . not related to the discussion on splitting the check  . Just trying to make sense of various lock files in npm to advise on the PR. Did bit of reading about   and here is my understanding. The best explanation that I found so far about   is from here  Following are some of the things that I think is useful to know: From the explanation not all project will have   but all Node based project will always have  . Project that have   want to have the correct version installed to avoid installation of version when using   or  . So my understanding the   guarantee the same dependencies version installed when the application is used anywhere. If   requirement is to ensure that dependencies used should not be   to a specific version than looking at   should be sufficient. If   requirement is to ensure that dependencies used should not be   to a specific version than looking at   should be sufficient. Thanks for digging into this. I also read      So we can implement as follows: \n \n \n Point 1 is what you suggested. Point 2 is something we can additionally implement. Wdut?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/688",
				"issue_name": [
					"Feature: improve packaging"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Improvements:\n\n\nthe Packaging checks only looks for GH packaging workflows. This is not the only way to publish code. We should check for the presence of the package on language repos.\nExample:\nfor npm: the package.json has a \"repository\" field, and metadata may be available from the npm API.  Alternatively, we could look at the name in package.json of the repository, then check npm to see if that package exists.\n\n\nThe check currently uses regex, we should switch to parsing properly.\n\n\nwe're missing some of the registries in https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-rubygems-registry\n\n\nwe're missing go packages, see https://github.com/ossf/scorecard/blob/main/.github/workflows/goreleaser.yaml\n\n\nwe're missing github marketplace actions\n\n\nUpdate the Token-Permission workflow as well, as it also checks for the need of packages permission.\n\n\n      ",
				"issue_comment": "Improvements: \n the   checks only looks for GH packaging workflows. This is not the only way to publish code. We should check for the presence of the package on language repos. \nExample: \nfor npm: the package.json has a \"repository\" field, and metadata may be available from the npm API.  Alternatively, we could look at the   in package.json of the repository, then check   to see if that package exists. \n The check currently uses regex, we should switch to parsing properly. \n \n \n we're missing github marketplace actions \n \n Improvements: \n \n For golang, we would like to query the   and see if a corresponding package exists pkg.go.dev developer here. pkg.go.dev learns everything it knows from the Go module proxy,  . Visit that page for a description of the protocol. So the proxy is the source of truth, and it can also handle much higher QPS than us. However, it's less discriminating: it doesn't examine what it's given to make sure it's really a Go module. For instance, it doesn't check for the presence of   files. (No   file, no module.) We do. So if that is important to you, then checking pkg.go.dev is reasonable, provided it's at relatively low QPS. If you know the version of the module you're looking for, supplying it will reduce load on us. A sufficient check for existence is to check the status of a HEAD request, and treat anything other than 200 as false. pkg.go.dev developer here. pkg.go.dev learns everything it knows from the Go module proxy,  . Visit that page for a description of the protocol. So the proxy is the source of truth, and it can also handle much higher QPS than us. However, it's less discriminating: it doesn't examine what it's given to make sure it's really a Go module. For instance, it doesn't check for the presence of   files. (No   file, no module.) We do. So if that is important to you, then checking pkg.go.dev is reasonable, provided it's at relatively low QPS. If you know the version of the module you're looking for, supplying it will reduce load on us. A sufficient check for existence is to check the status of a HEAD request, and treat anything other than 200 as false. Thanks, is there an API for the pkg.go.dev? It would help a lot instead of doing HTML parsing. There is no API, but if you're just checking for existence you don't need to parse HTML. Is there some other information you need? Thanks   So essentially we just need to check for the HTTP status. Should be good enough for our current use case I think.   anything else you think we need? I can't think of anything as of now. Thanks  what can be done on the pypi side, similar to  ? IIUC you're looking for a way to determine if a given project name is published on PyPI? That would require checking if the name exists (via HTTP status) at either: \n \n \n More details on available APIs here:  That's exactly what we need. Thank you!  is there an API that takes as input a GitHub repository instead of a package name? Or would we need to query the \"project links\" to infer the package name to input to the API? If so, what's the best way to do it?  You mean that you have a GitHub repo and you want to determine what PyPI project it corresponds to? There's a couple ways: \n \n \n \n Thank you   Let's wait until OIDC provides the magic."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/664",
				"issue_name": [
					"Include dependency verification in scorecard"
				],
				"issue_label": [
					"enhancement",
					"needs discussion",
					"Pinned-Dependencies"
				],
				"issue_content": "\n          Dependency verification, as implemented by Gradle for example, allows verifying both checksums and signatures of dependencies actually used in a build. It is, IMHO, significantly more important than using an automated dependency upgrade tool, in comparison.\nIt would be great if this was actually considered in the score, since we strongly encourage users to enable dependency verification as a tool to reduce the risks of supply chain attacks.\n      ",
				"issue_comment": ", as implemented by Gradle for example, allows verifying both checksums and signatures of dependencies  . It is, IMHO, significantly more important than using an automated dependency upgrade tool, in comparison. It would be great if this was actually considered in the score, since we strongly encourage users to enable dependency verification as a tool to reduce the risks of supply chain attacks. Thanks for the suggestion. This is definitely something we should consider supporting long-term. We've been hesitant to add any check around signature verification so far, mostly because there does not seem to be a well rounded story around key management/discoverability/revokation. We need to take a second look. Related link for npm:  There are other ongoing efforts like   that are relevant. Any feedback, suggestion or ideas are welcome! I think we can work on Dependency verification check \n , probably  the signature verification later.  Thoughts? I think we can work on Dependency verification check  , probably the signature verification later.  Thoughts? Agreed it's a great idea. We can start with the integrity part and implement it in the Pinned Dependency check. Is it what you had in mind too,   ? I think we can work on Dependency verification check  , probably the signature verification later.  Thoughts? Agreed it's a great idea. We can start with the integrity part and implement it in the Pinned Dependency check. Is it what you had in mind too,   ? Yes, that's what I had in mind. For the first pass we check if the file exists and if it has   flag enabled. I think we can work on Dependency verification check  , probably the signature verification later.  Thoughts? Agreed it's a great idea. We can start with the integrity part and implement it in the Pinned Dependency check. Is it what you had in mind too,   ? Yes, that's what I had in mind. For the first pass we check if the file exists and if it has   flag enabled.   What about the score calculation? How should this affect the score? Yes, that's what I had in mind. For the first pass we check if the file exists and if it has   flag enabled.  What about the score calculation? How should this affect the score? IIUC, the   file will contain the text above. We currently don't support gradle in   check: how about we add it there and consider metadata checksum verification as hash pinning/lock file? In this case, a score of   would be fine. wdut? Are there any other subtleties we need to be aware of? For example, for Npm,    was a surprise to me."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/660",
				"issue_name": [
					"Support private git servers (like Github Enterprise instances)"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nI would like to run scorecard for a repository that is hosted on my Github Enterprise instance. I currently can't.\n$ scorecard --repo github.corp.net/nv35/myrepo\n2021/07/05 09:24:35 unsupported host: github.corp.net\n\nDescribe the solution you'd like\nAllow to use the others github instances just like the .com one. The API path would likely just change from https://api.github.com/ to https://github.corp.net/api/v3, and I guess the calls would just be the same.\nAdditional context\nThe restriction to github.com is defined here: \n  \n    \n      scorecard/repos/repo_url.go\n    \n    \n         Line 77\n      in\n      1f1e05b\n    \n  \n  \n    \n\n        \n          \n           return fmt.Errorf(\"%w: %s\", ErrorUnsupportedHost, r.Host) \n        \n    \n  \n\n\n      ",
				"issue_comment": "\nI would like to run scorecard for a repository that is hosted on my   instance. I currently can't. \nAllow to use the others github instances just like the .com one. The API path would likely just change from   to  , and I guess the calls would just be the same. \nThe restriction to github.com is defined here:  \nI would like to run scorecard for a repository that is hosted on my   instance. I currently can't. \nAllow to use the others github instances just like the .com one. The API path would likely just change from   to  , and I guess the calls would just be the same. \nThe restriction to github.com is defined here: PR's are welcome! Thanks for this feedback! This is a great usecase for  . We are still in the process of  factoring out all github.com dependencies into  . Once we have this refactoring in place, it should be easy to use enterprise APIs instead of the regular one."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/653",
				"issue_name": [
					"scorecard on github marketplace"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Describe the solution you'd like\nto simplify the adoption of the project it would be important to create a service on the github marketplace and with \"one click\" it would be possible to add scorecards to the project\nAdditional context\nafter reading the publication made by google I decided to put scorecard in a project that I am a maintainer (prest), after breaking my head with yml I decided to look at the existing actions in scorecard, I made a \"fork\" of the integration.yml (prest/prest#566) file\nThe use of the scorecard can be simplified with action\n      ",
				"issue_comment": "\nto simplify the adoption of the project it would be important to create a service on the github marketplace and with \"one click\" it would be possible to add scorecards to the project \nafter reading the publication made by google I decided to put scorecard in a project that I am a maintainer ( ), after breaking my head with yml I decided to look at the existing actions in scorecard, I made a   of the   ( ) file \nThe use of the scorecard can be simplified with action Thanks for the feedback :) If I understood   correctly, you basically want to enable a workflow such that - on incoming PRs Scorecard gets triggered on your repo. Is that correct? We have something very similar in our pipeline for current quarter (see  ,  ). Basically, we want to support a Scorecard action/workflow on Github. Let me know if this would achieve what you are looking for. is related to the issue  but this issue would be for scorecard to be available on the  Interesting, definitely a pretty cool idea. Adding it to  . Will likely look into this once we have tackled  ."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/632",
				"issue_name": [
					"Feature: Randomly assign reviewers for dependabot PRs"
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          To handle the volume of dependabot PRs, setup a rule/workflow such that incoming dependabot PRs will be randomly assigned a reviewer. It will be the assigned reviewer's responsibility to make sure the PR is safely merged into main.\nNot sure if this is easy to setup. I found this tool which claims to do the exact thing though - https://mergify.io/.\n      ",
				"issue_comment": "To handle the volume of dependabot PRs, setup a rule/workflow such that incoming dependabot PRs will be randomly assigned a reviewer. It will be the assigned reviewer's responsibility to make sure the PR is safely merged into  . Not sure if this is easy to setup. I found this tool which claims to do the exact thing though -  ."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/631",
				"issue_name": [
					"Feature: Data plots for analyzed repos"
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted"
				],
				"issue_content": "\n          Now that there is a good chunk of data on 50k+ repos, we could consider maintaining a dashboard (e.g through DataStudio), which analyzes this data and can show some interesting insights on the repos we analyze regularly.\nIt will help us understand what extra data we could potentially add to make the analysis more useful (e.g maybe repo metadata should have repo language as well). We can also make this available to the public to showcase what they can do with this data and it will function as a sanity check on the data we are publishing.\n      ",
				"issue_comment": "Now that there is a good chunk of data on 50k+ repos, we could consider maintaining a dashboard (e.g through DataStudio), which analyzes this data and can show some interesting insights on the repos we analyze regularly. It will help us understand what extra data we could potentially add to make the analysis more useful (e.g maybe repo metadata should have repo language as well). We can also make this available to the public to showcase what they can do with this data and it will function as a sanity check on the data we are publishing."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/630",
				"issue_name": [
					"Feature: README.md for dependabot PRs"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          \n            No description provided.\n          \n      ",
				"issue_comment": ""
			},
			{
				"issue_serial": "/ossf/scorecard/issues/613",
				"issue_name": [
					"New check: download of .whl followed by pip install"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We currently support downloadds followed by execution, bash, and other interpreters (see shell_download_validate.go#L38). We d support file saved on disk and executed. However, there are other patterns we don't support, such as:\n\ndownload of .whl followed by pip install\ndownload of .deb followed by dpkg -i\ndownload of whatever followed by its installer\n\n      ",
				"issue_comment": "We currently support downloadds followed by execution, bash, and other interpreters (see  ). We d support file saved on disk and executed. However, there are other patterns we don't support, such as: \n \n \n \n      Any comments on these checks? \nFYI "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/608",
				"issue_name": [
					"New check: unpinned repo clone"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\ndetect git, hg and other source management tools used to clone repo without pinning by hash. This sometimes happens in Makefiles or shell scripts to pull dependencies.\nThis check is problematic because there are many legitimate reason to do this, especially for one's own repo. Example, for integration tests, etc.\nMaybe we could detect repo clones that are different from the repo itself. Note that dependabot probably does not support updating the version of a cloned repo.\nDescribe the solution you'd like\nneed discussion\nDescribe alternatives you've considered\nA clear and concise description of any alternative solutions or features you've considered.\nneed discussion\nAdditional context\nAdd any other context or screenshots about the feature request here.\n      ",
				"issue_comment": "\ndetect  ,   and other source management tools used to clone repo without pinning by hash. This sometimes happens in Makefiles or shell scripts to pull dependencies. This check is problematic because there are many legitimate reason to do this, especially for one's own repo. Example, for integration tests, etc. \nMaybe we could detect repo clones that are different from the repo itself. Note that dependabot probably does not support updating the version of a cloned repo. \nneed discussion \n \nA clear and concise description of any alternative solutions or features you've considered. \nneed discussion \nAdd any other context or screenshots about the feature request here. possible patterns to check for: \n \n \n \n Here's my opinion: we should not do unpinned dependency checks for dependency types not supported by dependabot. If a developer modifies their code to clone a git repo at a certain commit hash, chances are that hash will not be updated for years, if ever. Yes, they will be protected from someone pushing malicious code to that git repo in the future, but they will also miss out on vulnerability and other bug fixes. Without dependabot, I would say the developer would be less secure by pinning a hash. it would be good to flag it nevertheless, because it's a potential risk. Any thoughts on warning without recommending pinning by hash? Yeah, I think if we just noted this as a warning, but didn't recommend pinning, that would be a good approach. do you know if dependabot/renovatebot support commands within shell script/makefiles  ? I'm also curious about GCB's cloud.yaml files  cc  Right now we don't support arbitrary scripts unless you're self hosting. For the app it's too high risk  do you mean it's too risky to parse arbitrary shell scripts? Let me give you a concrete example of what I meant. \nif I have a workflow using  , renovate-bot is able to send PR to update the hash when a new action of released. \nIf I use  , is renovaote-bot able to understand that I depend on  , even though it's   in a   file? It's pretty common to install CLI via a command, but we want these to be updated by renovate-bot when a new version is released. More generally, is removate-bot capable of understanding cloud.yaml depedencies given via args, or commands such as  . (This one clearly looks harder) Sorry, I misunderstood. Renovate attempts to understand as many patterns as it can out of the box, but it doesn't know those ones. For ones like that which it doesn't understand, users can add regex patterns to identify them for Renovate. The feature is know as the \"regex manager\":  So the short answer is that yes, Renovate probably already can update the examples you gave, but needs configuration to do so. It's a similar challenge for Dockerfiles, where there is no \"standard\" for defining dependencies and it's essentially similar in concept to bash scripts. Interesting, thanks for the info."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/581",
				"issue_name": [
					"Support DAST tools like ZAP"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScore Card already reports certain SAST tools in use, reporting DAST tools would seem to be a good enhancement.\nDescribe the solution you'd like\nReport detected DAST tools in a similar way to SAST ones :)\nDescribe alternatives you've considered\nN/A\nAdditional context\nThe ZAP GitHub actions are the easiest way to detect if ZAP is being used right now: https://github.com/marketplace?type=&verification=&query=owasp+zap+\nSome other DAST actions that could be detected: https://github.com/marketplace?query=dast+\n      ",
				"issue_comment": "\nScore Card already reports certain SAST tools in use, reporting DAST tools would seem to be a good enhancement. \nReport detected DAST tools in a similar way to SAST ones :) \nN/A \nThe ZAP GitHub actions are the easiest way to detect if ZAP is being used right now:  \nSome other DAST actions that could be detected:  Thanks for the suggestion."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/568",
				"issue_name": [
					"Feature: Add another bot to manage dependencies"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nMany projects rely on https://github.com/marketplace/khebhut for dependency management of Python projects. Therefore scorecards for that check (https://github.com/ossf/scorecard/blob/main/checks/checks.md#automatic-dependency-update) are not happy for those projects.\nDescribe the solution you'd like\nAdd new Bot https://github.com/marketplace/khebhut that is checked for https://github.com/ossf/scorecard/blob/main/checks/checks.md#automatic-dependency-update.\nDescribe alternatives you've considered\nAdditional context\n      ",
				"issue_comment": "\nMany projects rely on   for dependency management of Python projects. Therefore scorecards for that check ( ) are not happy for those projects. \nAdd new Bot   that is checked for  ."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/564",
				"issue_name": [
					"Feature: Pin dependencies for Kubernetes"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Pin dependencies for k8, e.g.\n\nkubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4\nyaml files: image: \"gcr.io/path/to/image:1.14.0\"\n\nMaybe check 1 should go into SAST checks instead\n      ",
				"issue_comment": "Pin dependencies for k8, e.g. \n \n \n Maybe check 1 should go into SAST checks instead"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/561",
				"issue_name": [
					"BUG: projects-update script not working"
				],
				"issue_label": [
					"bug",
					"good first issue"
				],
				"issue_content": "\n          @asraa @naveensrinivasan - the projects-update script (https://github.com/ossf/scorecard/blob/main/cron/data/update/main.go) which adds dependencies to the projects.csv file doesn't seem to be working and throws a lot of errors when run. Is this expected? What's the current status of this script?\n      ",
				"issue_comment": "   - the   script ( ) which adds dependencies to the   file doesn't seem to be working and throws a lot of errors when run. Is this expected? What's the current status of this script? Do you have logs? Dont' know the status but I can help diagnose You can run this at HEAD to reproduce: It's dumping out various errors like these: :( I'm not sure why they're assumed to be vanity URLs if they're not github.  Some go repos aren't GitHub "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/548",
				"issue_name": [
					"New Check: time to fix security vuln"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          It would be nice to be able to provide insight into the time it takes for developers to fix issues, ie how responsive they are. That's not super easy since bugs have various levels or priority (critical vs medium vs etc).\nOSV may be a good source of data for this. We could try to mine github data but that seems harder.\n@jeffmendoza FYI\n      ",
				"issue_comment": "It would be nice to be able to provide insight into the time it takes for developers to fix issues, ie how responsive they are. That's not super easy since bugs have various levels or priority (critical vs medium vs etc). OSV may be a good source of data for this. We could try to mine github data but that seems harder.  FYI"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/542",
				"issue_name": [
					"Feature - Handle Renovate bot package.json"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\n#322 , does not handle package.json (within a \"renovate\" section) https://docs.renovatebot.com/configuration-options/\n      ",
				"issue_comment": "\n  , does not handle   "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/511",
				"issue_name": [
					"Project: Scorecard UX"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          This is an umbrella bug for tracking features/implementation work on improving how users interact with Scorecard. So far, we have:\n\nImproved remediation: #347 (comment)\nHuman readable HTML output: #342\nScorecard as a GitHub workflow: #193\nScorecard badge: #271\nTracking Scorecard data over time: #12\n\nLet's use this space to discuss any new UX related ideas and we can spawn off new issues if needed.\n      ",
				"issue_comment": "This is an umbrella bug for tracking features/implementation work on improving  . So far, we have: \n \n \n \n \n \n Let's use this space to discuss any new UX related ideas and we can spawn off new issues if needed. One more:  Azeem suggested I work on adding remediation results to scorecard ( ), so I'll be doing that next. I'll be following what he laid out in his comment  . Thanks for taking this up Chris, this'll be a great help! Azeem suggested I work on adding remediation results to scorecard ( ), so I'll be doing that next. I'll be following what he laid out in his comment  . any update on this effort? Assigning to myself. I will start with the following pre-requisites: \n \n \n \n  I think you've working on this so let's sync to avoid duplicating work :-)  Sorry I didn't get back to you sooner, been crazy busy getting ready to move and working on selling my house. I've made some progress on showing remediation suggestions in the results. I'll slack you and give you more details. Once we have   and  , worth considering making Scorecard available on Github Marketplace - "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/435",
				"issue_name": [
					"Feature: improve unit tests and e2e test for better coverage"
				],
				"issue_label": [
					"enhancement",
					"good first issue",
					"help wanted",
					"priority"
				],
				"issue_content": "\n          there are many tests, so this should be done progressively\n      ",
				"issue_comment": "there are many tests, so this should be done progressively +1. We really need this. I also think we should make existing tests to be hermetic and non-flaky. Something we additionally need is e2e tests for the CLI arguments. We need to test things like: \n ,  ,  ,  , etc. In particular, not all checks are supported when running on a local folder. Also, if checks are explicitly listed, they must be a subset of whose contained in the policy file. Ill add more requirements when we have the GH actions (which adds these additional options) ready."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/427",
				"issue_name": [
					"Frozen deps should check for hash pinning"
				],
				"issue_label": [
					"enhancement",
					"priority"
				],
				"issue_content": "\n          The check currently looks for known files but does not parse them. It would be useful to enforce packages are pinned by hash for a program (not for a library).\nThis will require supporting several languages. It will be very time-consuming to migrate pinned by version to pinned by hash manually, so we will need some tooling to automatically update the files for project owners.\n      ",
				"issue_comment": "The check currently looks for known files but does not parse them. It would be useful to enforce packages are pinned by hash for a program (not for a library). This will require supporting several languages. It will be very time-consuming to migrate   to   manually, so we will need some tooling to automatically update the files for project owners. Several aspects: \n \n \n \n 2 and 3 may be done as part of a new code scanning check instead. TODO:   and  I came here after looking for a place to comment about pinning hashes. Seems to be some overlap between issues. I think OSSF could consider introducing/defining at least one more term in addition to \"pinning\". What we're really trying to achieve here is   dependency use (i.e. get the same dependencies installed on all machines, for all time), which is similar but not as strong as   builds - which take into account more than just dependencies. Another way of framing it is that any dependency whose tag or version is not immutable should be pinned to a hash/digest. Examples: \n \n \n \n For example, if I depend on   on npmjs, I can reasonably expect that to be immutable and safe. If instead in the same   I depend on   then that resolves to a tag   which   change, and it's not safe/reliable. In Renovate we've always favored pinning, but distinguish between pinning versions and pinning digests. We also recognize that many users resist this, so it's not our opinionated default. Adding support to automatically pin and then maintain/update GitHub Actions is a recent feature, for example - we added it out of a security concern of our own. if I depend on   on npmjs, I can reasonably expect that to be immutable and safe. This depends on the threat model: how much trust do you want to put in the package manager's infra/database? Not everyone agrees here either. Related "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/426",
				"issue_name": [
					"New check: Check for dangerous code practices in github workflows"
				],
				"issue_label": [
					"enhancement",
					"help wanted",
					"priority"
				],
				"issue_content": "\n          There are several examples of github token leaks via pull_request_target event. It'd be nice to check for it - possibly filtering out known acceptable github actions that use it after we have reviewed their code (and it's pinned by hash). This check goes in the same direction as #414, ie harden github worflows.\n      ",
				"issue_comment": "There are several examples of github token leaks via   event. It'd be nice to check for it - possibly filtering out known acceptable github actions that use it after we have reviewed their code (and it's pinned by hash). This check goes in the same direction as  , ie harden github worflows. Dangerous permissions \nor something :) Thinking of broadening this check to \"Dangerous workflow coding patterns\": \n \n \n \n \n \n \n \n \n \n I've left out   because it's part of another check around dependency pinning  We could also make this check the \"workflow hardening check\": use the above plus token permission check. related link  Hi, the important prerequisite for the dangerous pattern of   is the usage of untrusted data, most commonly an explicit checkout of   as in the example above. There are many ways to checkout the pull request code, but I think you can reduce false positives by checking for the most common - checkout action   parameter. Yet this still doesn't mean it is vulnerable if the code from pull request is treated as data, for example built scripts or tests are not run. But it may be hard to detect it programmatically to completely eliminate FPs. From my experience if   is used with checkout   95% chance the code is vulnerable. Thanks for the information, that's super useful. If someone in your team is interested in taking a stab at this, please let me know. Added a few more bad things to the list after reading  part 3  Can I try this one? One thing I can't figure out is if it should be merged into Token Permissions and that be renamed, or totally separate like \"Workflow Patterns\" You sure can! I think we can start it as a different check first. We can merge them later. I've assigned to you. Thanks you! \nSince it's a pretty comprehensive check, we can start with   for v4? Let's try to do 5 and 8. They are fairly simple. Adding to v5 milestone. wdut? We can try to share the workload if needed. On it for 8! Second order command injection attacks are also possible if attacker controlled input is passed from a workflow to a vulnerable action. For example: \n \n \n \n That's an amazing find. Thanks   for letting us know! I'll take a stab at 5."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/414",
				"issue_name": [
					"New check: reduce scope of git token permission"
				],
				"issue_label": [
					"enhancement",
					"priority"
				],
				"issue_content": "\n          Github workflows rely on events (e.g. pull_request, etc). The default permission for the token is read/write. In most cases, read permission should be enough. To reduce the attack surface, we should check for this. We'll need to have a list of actions that we allow to use read permissions.\nThis is a good candidate for a status check as well.\n      ",
				"issue_comment": "Github workflows rely on events (e.g. pull_request, etc). The default permission for the token is read/write. In most cases, read permission should be enough. To reduce the attack surface, we should check for this. We'll need to have a list of actions that we allow to use read permissions. This is a good candidate for a status check as well. Ideally we'd try to check this   and ensures it's set to read only. Later, we can also verify that repo-specific workflows are not given additional write permission if it's not needed. More information   and  After a bit of digging/reading, here are some potential options: \n \n \n \n \n Additional notes: there's a scary-looking setting  (see   and   but I think it's only available for private repos. Comments?  FYI I'm going to create this check as a new standalone one. I don't have a good name for it yet. It'd be useful to have a generic name that could encompass other checks like   too? Done in  , except for point 3 and 4."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/413",
				"issue_name": [
					"New check: check for dependency scanner"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Dependency (vuln) canners may be for package managers (cargo-audit, npm-audit, etc), for docker (snyk), github apps (dependabot), etc\nWe could add a test to see if a scanner is used as part of a github workflow.  This check may live under the existing SATS check?\n      ",
				"issue_comment": "Dependency (vuln) canners may be for package managers (cargo-audit, npm-audit, etc), for docker (snyk), github apps (dependabot), etc \nWe could add a test to see if a scanner is used as part of a github workflow.  This check may live under the existing SATS check?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/406",
				"issue_name": [
					"Use app IDs to identify apps in CI-Test"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          In CI-Test, we use names to identify CI tests, by checking if the names are contained in the app name (slug). We could use the app ID instead with an exact comparison to be more robust.\n      ",
				"issue_comment": "In  , we use names to identify CI tests, by checking if the names are contained in the app name (slug). We could use the app ID instead with an exact comparison to be more robust."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/405",
				"issue_name": [
					"New check: check that domains are validated"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          if a repo belongs to an org, try to verify its domain is 'verified'. If feasible, do the same for the repo's website.\nRelated to #397\n      ",
				"issue_comment": "if a repo belongs to an org, try to verify its domain is 'verified'. If feasible, do the same for the repo's website. \nRelated to "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/403",
				"issue_name": [
					"Frozen Deps does not check frozen dockerfile image use, CI/CD tools"
				],
				"issue_label": [
					"enhancement",
					"priority"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\n\nCheck for either hardcoded sha in FROM.*dockerfile use or best to see if signature verification happens (with sigstore)\nfor CI/CD discourage bad patterns, e.g. curl | bash\n\nDescribe the solution you'd like\nImprove frozen deps check to account for these.\nDescribe alternatives you've considered\nNA\nAdditional context\nNA\n      ",
				"issue_comment": "\n \n \n \nImprove frozen deps check to account for these. \nNA \nNA Needs documentation update as well. Here's one tool to help pin Dockerfiles:  Another WIP one, designed to help with signatures:  1 is done as  \n2   is going to take a little bit more work. We'll use   for shell scripts, I think. We need to check bad patterns in: \n \n \n \n \n \n Users sometimes also download from cloud storage, e.g. GCS using  . Other cloud providers' solutions should also be considered. yet another pattern is the use of dockerfile's  I'll start with shell script. In terms of bad pattern that go against dep pinning, there are also package manager commands: \n \n \n One caveat is that   is pretty common in docker files, so we'll have to accept that for now and accept signature verification is enough. Is there a way to build from source? 1 is done as  \n2   is going to take a little bit more work. We'll use   for shell scripts, I think. We need to check bad patterns not only in shell scripts, but also in makefiles, and even in code  ,  , so we need some language support for this one. \nI'll start with shell script. +1 shell script is a good start.      I'm starting to work on  \nI worry this is going to throw false positives whenever CI/CD runs integration tests or releases - scripts will fetch the latest code. So I'm thinking of detecting if the   is for the same repo scorecard is analyzing or not. \nWDUT?      I'm starting to work on  \nI worry this is going to throw false positives whenever CI/CD runs integration tests or releases - scripts will fetch the latest code. So I'm thinking of detecting if the   is for the same repo scorecard is analyzing or not. \nWDUT? you mean catching cases of building from source where people use just trunk and no pinned version/hash. i am expecting this will be crazy ton of false positives? which usecase example where you thinking to solve? correct. I've seen this sort of bugs caught in reviews where the Makefiles would just pull sources from various repos, e.g.  . I also expect there will be false positives due to genuine cases. Let's shelve this repo check for now then? correct. I've seen this sort of bugs caught in reviews where the Makefiles would just pull sources from various repos, e.g.  . I also expect there will be false positives due to genuine cases. Let's shelve this repo check for now then? yes, but lets file a seperate issue to track this, seems important to think more on this."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/397",
				"issue_name": [
					"Company names are not validated"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          In the Collaborators check, we fetch the names of companies the user belongs to.\nThese names can be set by users arbitrarily. If scorecard users take into account the company of contributors to make decisions, these names should be verified.\nWe may use the list-org API instead.\nAnother problem is that we currently do not check how many companies a user belongs to. A contributor (we currently enforce >5 commits) can set any number of companies they want and it will pass the test. We're not enforcing a single company per user. (Is this an invariant we can enforce?)\nA limitation of this check is that organization should update when users leave.\nIt's pretty easy to typo-squat orgs too, see https://github.com/GoogleContainerTooIs vs https://github.com/GoogleContainerTools.\nSo we should always verify the org is 'verified', like https://github.com/google\nOne limitation of the 'verified' org is that it seems to verify the domain ownership, but there is no enforcement about the org name itself.\n      ",
				"issue_comment": "In the Collaborators check, we fetch the names of   the user belongs to. \nThese names can be set by users arbitrarily. If scorecard users take into account the company of contributors to make decisions, these names should be verified. We may use the   instead. Another problem is that we currently do not check how many companies a user belongs to. A contributor (we currently enforce >5 commits) can set any number of companies they want and it will pass the test. We're not enforcing a single company per user. (Is this an invariant we can enforce?) A limitation of this check is that organization should update when users leave. It's pretty easy to typo-squat orgs too, see   vs  . \nSo we should always verify the org is 'verified', like  One limitation of the 'verified' org is that it seems to verify the domain ownership, but there is no enforcement about the org name itself. How different is this from   ? ideally one of these should be milestone-q2, i kept the other one. It's the same underlying check, but surfaced in different scorecard checks. This one is about checking that the current repo belongs to a 'verified' org; whereas   is about checking collaborator affiliations - which is also a github org. It's the same underlying check, but surfaced in different scorecard checks. This one is about checking that the current repo belongs to a 'verified' org; whereas   is about checking collaborator affiliations - which is also a github org. ah thanks for explaining, in that this 'verified' thing in org will be nice, thoughts? well.. it may help give credibility to the org because it links the org's name to its website/domain. We'll never be sure the domain is genuine unless we keep a mapping between the two on our end, though. It would be useful when Details is implemented to mention the domains that the organization has ownership over (a scorecard user can then go and check whether they find those domains trustworthy, or use some mapping). So if I follow correctly: \nThis issue is about checking if a repo belongs to a verified organization (create a new check: Verified-Repo?) \n  is about checking if contributors belong to a verified organization (part of Contributor check) Correct. /assign  \n:D"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/383",
				"issue_name": [
					"Dependency pinning enhancement"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The Pinning-Dep checks for known file names for various languages. However, as soon as it finds one, it returns success.\nSome repos may contain more than one language -- Tink crypto library is an example, protobuf is another one.\nWe could use the language APIs to automatically detect the languages used, and validate that the filenames we find correspond to the languages used. We need not check all languages, but only the prominent ones used in the repo.\nNote that the language API does not give us the folders that contain the code. So an alternative approach may be to list all files, count their LoC ourselves instead.\nThis will allow scorecard to report what languages we tested the repo for and which folders passed the tests. We may report a score that is the percentage of lines of code 'pinned' over the number of lines non 'pinned'. Running scorecard on envoy currently fails to detect the following files since we only check for files in the root folder.\nname ci/flaky_test/requirements.txt\nname configs/requirements.txt\nname docs/requirements.txt\nname examples/grpc-bridge/client/requirements.txt\nname source/common/common/compiler_requirements.h\nname source/extensions/filters/network/kafka/requirements.txt\nname test/extensions/filters/network/thrift_proxy/requirements.txt\nname tools/code_format/requirements.txt\nname tools/config_validation/requirements.txt\nname tools/dependency/requirements.txt\nname tools/deprecate_features/requirements.txt\nname tools/deprecate_version/requirements.txt\nname tools/envoy_headersplit/requirements.txt\nname tools/github/requirements.txt\nname tools/protodoc/requirements.txt\nname tools/testing/requirements.txt\n\n\nThis suggests that long-term, it would be useful to have a comprehensive  config file for each repo.\nRelated to #404 #403\n      ",
				"issue_comment": "The   checks for known file names for various languages. However, as soon as it finds one, it  . \nSome repos may contain more than one language --   is an example,   is another one. We could use the   to automatically detect the languages used, and validate that the filenames we find correspond to the languages used. We need not check all languages, but only the prominent ones used in the repo. Note that the language API does not give us the folders that contain the code. So an alternative approach may be to list all files, count their LoC ourselves instead. This will allow scorecard to report what languages we tested the repo for and which folders passed the tests. We may report a score that is the percentage of lines of code 'pinned' over the number of lines non 'pinned'. Running scorecard on   currently fails to detect the following files since we only check for files in the root folder. This suggests that long-term, it would be useful to have a comprehensive  config file for each repo. Related to    An idea could be to search for dependency files, and when we find one, look for the corresponding lock files that should live in the same folder. Everything that lives outside this folder could be considered not pinned. We can then compute the percentage of LoC that is pinned vs non-pinned; and use this as a score.  FYI  FYI"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/382",
				"issue_name": [
					"Verify signatures for releases"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          The Signed-Releases check currently looks for known signature files, but does not verify them.\nWe should try to verify them with the corresponding key.\nNote that for Signed-Tags, we rely on github's signature verification. We may also do the verification ourselves for defense in-depth (long-term, low priority).\n      ",
				"issue_comment": "The   check currently looks for known signature files, but does not verify them. \nWe should try to verify them with the corresponding key. Note that for  , we rely on github's signature verification. We may also do the verification ourselves for defense in-depth (long-term, low priority). Try to integrate with  This might be tricky and encourage an anti-pattern. In general putting the public key right next to the signature on the GitHub release would be problematic and not something we want to encourage, so we'd need another mechanism to find the key. Agreed. I have not looked at how sigstore works to be honest. So you're saying it has no concept of identity. Is there another project trying to tackle this problem? Agreed. I have not looked at how sigstore works to be honest. So you're saying it has no concept of identity. Is there another project trying to tackle this problem? Ah yeah - with sigstore it would be doable!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/381",
				"issue_name": [
					"Uniformize checks based on time?"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          We currently:\n\nsigned tags checks for the last 5 tags\nsigned releases checks for the last 5 releases\nPull request checks for the first page of commits (~30 last commits?)\netc\n\nMaybe we could uniformize them by using a common time window instead. For example we could use the last n months (user-provided option) and set it to, say, 6 months by default.\nThere are limitations to doing this: in some cases we may find no commits, no releases, etc in the time period.\nA more general way could be to have options for users to say 'over the last n releases', 'over the last n months' for a small set of things that would make sense for developers.\nThis is just an idea. Comments welcome!\n      ",
				"issue_comment": "We currently: \n \n \n \n \n Maybe we could uniformize them by using a common time window instead. For example we could use the last n months (user-provided option) and set it to, say, 6 months by default. There are limitations to doing this: in some cases we may find no commits, no releases, etc in the time period. \nA more general way could be to have options for users to say 'over the last n releases', 'over the last n months' for a small set of things that would make sense for developers. This is just an idea. Comments welcome!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/379",
				"issue_name": [
					"New Check: signed commits"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          In addition to signed release and tags, we may check for signed commits.\nWe need a good story around key management/storage. Long-term, it would be nice to suggest more recent algorithms such as ECDSA/EdDSA instead of RSA/DSA and more recent crypto libraries, such as Tink.\n      ",
				"issue_comment": "In addition to signed release and tags, we may check for  . We need a good story around key management/storage. Long-term, it would be nice to suggest more recent algorithms such as ECDSA/EdDSA instead of RSA/DSA and more recent crypto libraries, such as  . Try to integrate with  Right now signed git commits are problematic. The only real method is gpg, and there's no real key-management story so in most cases signed commits add little value IMO. There's an effort here to improve the native git-commit signing:  And sigstore has some plans to improve git signing by \"working around\" git. It's all a fair ways out though... related "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/376",
				"issue_name": [
					"Detect unit tests and its coverage"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          If we can, we should try to detect if the project uses unit testing, and the corresponding code coverage. May be done in combination with #78.\n      ",
				"issue_comment": "If we can, we should try to detect if the project uses unit testing, and the corresponding code coverage. May be done in combination with  . we could start with simple heuristics: e.g. could count number of go files that have a corresponding  . \nOr look at other solution, like codecov."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/373",
				"issue_name": [
					"New check: Check to detect sandboxing support, e.g. sandbox_api"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          \n            No description provided.\n          \n      ",
				"issue_comment": "Need more information on this issue.   What is the check? Could you please add more information?"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/370",
				"issue_name": [
					"Enhance PullRequest feature"
				],
				"issue_label": [
					"bug",
					"enhancement",
					"help wanted"
				],
				"issue_content": "\n          \nCode-Review discards commits made by bots with names bot  and gardener but only checks for their presence as substrings in the committer name. This is brittle, e.g.commits by user abotas would not be detected by the checks. I think we should use == instead.\nSimilarly, we check for reviews via gerrit by checking the presence of string Reviewed-on: in the commit message. Is there a better way? Is gerrit always used in combination with PRs?\n\n      ",
				"issue_comment": "\n \n \n Update: I think gerrit uses status check APIs, and these are independent of the review messages. So this may be a better signal for non-malicious repos. \nIf the repo is considered potentially malicious, that won't work unless we can uniquely identify gerrit (and its proper configuration). This seems to suggest the gerrit app would have to be run by github itself and github should attest that it's being run and enforced. I don't know if such a thing exist."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/367",
				"issue_name": [
					"New check: Check to make sure build/artifact generation and signing is isolated from untrusted tools like coverage, testing"
				],
				"issue_label": [
					"enhancement",
					"priority"
				],
				"issue_content": "\n          E.g. detect things like https://discuss.hashicorp.com/t/hcsec-2021-12-codecov-security-event-and-hashicorp-gpg-key-exposure/23512\n      ",
				"issue_comment": "E.g. detect things like "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/342",
				"issue_name": [
					"Feature: Scorecard output should be human readable HTML"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThis is after discussions with the Envoy team. Today, output of Scorecard command says Pass/Fail with confidence numbers. For someone new to Scorecard, this in itself wouldn't relay anything meaningful.\nDescribe the solution you'd like\nInstead, the output should be a detailed report mentioning what the tool tried to do and why the particular check failed. Providing clear reasons about failures and confidence values will let users decide if they care about fixing the failure and how they should go about fixing it.\n      ",
				"issue_comment": "\nThis is after discussions with the Envoy team. Today, output of Scorecard command says Pass/Fail with confidence numbers. For someone new to Scorecard, this in itself wouldn't relay anything meaningful. \nInstead, the output should be a detailed report mentioning what the tool tried to do and why the particular check failed. Providing clear reasons about failures and confidence values will let users decide if they care about fixing the failure and how they should go about fixing it.   We can do this using the following: \n \n \n Wdut? We can do this using the following: \n \n \n Wdut? Azeem added some thoughts in another PR -  I think the solution in   should cover this too. If you have thoughts/feedback about that approach, we can discuss more on the other thread. Once we have a consensus on the solution, I'm happy if someone can take up the implementation of it. Since   already had a PR for this - I'll leave it upto   and   to decide how to proceed with the implementation."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/341",
				"issue_name": [
					"BUG - Project updates binary does not use GitHub Authentication"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          Describe the bug\nThe https://github.com/ossf/scorecard/tree/main/scripts/update package does not use GitHub Authentication. This would slow down the updates.\nExpected behavior\nChange the implementation to include the GitHub Authentication when pulling source code from GitHub.\n      ",
				"issue_comment": "\nThe   package does not use GitHub Authentication. This would slow down the updates. \nChange the implementation to include the GitHub Authentication when pulling source code from GitHub."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/327",
				"issue_name": [
					"Feature - Manage secrets for the cronjob"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nRight now the secrets that are used for CronJob (GitHub Tokens) aren't managed within Git. It is Adhoc and manipulated directly.\nDescribe the solution you'd like\nOne possible solution\n\nEncrypt the  GitHub tokens with KMS keys using https://github.com/mozilla/sops\nStore the tokens in GitHub\nDecrypt it before deploying\n\n      ",
				"issue_comment": "\nRight now the secrets that are used for CronJob (GitHub Tokens) aren't managed within Git. It is Adhoc and manipulated directly. \nOne possible solution \n \n \n \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/309",
				"issue_name": [
					"Feature - Sign scorecard container with cosign"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nSign scorecard containers with cosign\n      ",
				"issue_comment": "\nSign scorecard containers with  Now that   1.0 we could use it for signing. Thoughts    On a high-level the idea sounds good to me. I don't understand   a 100% though. Do you mind sketching out what this would look like, ie. would this be done through CloudBuild, any major changes that would be required etc.?  assigning this to you as per yesterday's discussion. Lets come up with a one-pager proposal here to submit in the TAC meeting I have the following recommendations: \n \n \n Thank you  !  We are tracking this part of this larger issue  We want to come up with a plan of it being SLSA compliant. \n \n \n \n Would OIDC be an option? This way we don't need a special workflow to generate keys and store them in GH secrets, and we also get built-in key rotation.  FYI Yes, that would be a great option for signing containers. Signing blob(scorecard binary) is easy. But verifying is jumping through lots of hoops. I am trying that the tooling isn’t there yet. Also we need to understand if it suffices the SLDA requirements. hello    , here is the keyless image signing example with GoReleaser recently created as a sample project , thanks to  , of course, you can find an example of signing checksum also, here is the related tweet Sample 1:  Sample 2:  \n   \n   \n Cross-linking a few things from Kubernetes tracking: \n \n \n \n \n \n"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/304",
				"issue_name": [
					"Feature - Provide Remediation in results "
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nProvide description and remediation in the results. Related to #63\nThis is also required for #193\n\n Active\n Branch-Protection\n CI-Tests,CII-Best-Practices\n Code-Review,Contributors\n Frozen-Deps,Fuzzing\n Packaging\n Pull-Requests\n SAST\n Security-Policy\n Signed-Releases\n Signed-Tags\n\n      ",
				"issue_comment": "\nProvide description and remediation in the results. Related to  \nThis is also required for "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/298",
				"issue_name": [
					"New Check: Add Sigstore, other tools use check for signed tags, releases"
				],
				"issue_label": [
					"bug",
					"core feature",
					"priority"
				],
				"issue_content": "\n          Describe the bug\nCurrently signed tags and releases check don't detect use of signing tools like Sigstore.\nReproduction steps\nSigned releases using sigstore are not detected.\nExpected behavior\nSigned releases using sigstore should be detected.\nAdditional context\nAdd any other context about the problem here.\n      ",
				"issue_comment": "\nCurrently signed tags and releases check don't detect use of signing tools like Sigstore. \nSigned releases using sigstore are not detected. \nSigned releases using sigstore should be detected. \nAdd any other context about the problem here. Check code here - \n \n Docs needs updating here - \n \nand regeneration of json, etc.  How can I identify if a packaged/container is signed by cosign? See the spec here:  You'd have to check the registry itself. Since you don't really care about verifying for this check, it should be pretty simple.  - do you have cycles to give this a try, see comment from   -   At this point, we just care on seeing if sigstore is used (verifying can come later). Looking into this, my main problem is figuring out where the github project is uploading their containers/packages. \nSearching for stuff like \"gcr.io\" or \"docker.io\" in the github repository   work, but there' would be lots of false positives if we did that and expected that to be the projects distribution. I'm happy to keep this assigned, but given that there's no canonical place to look for artifact signing, I think we should hold off for about a quarter until there is one. Cosign itself doesn't have a good location to check for images. hello    , this is not a perfect solution but we are working on a project called   which is basically a Prometheus exporter and aims to provide metrics about the images running on the Kubernetes cluster. Within this project, we are accepting public keys from the users via flag to check whether the image is signed or not. Maybe we can apply this similar approach here because we have to get this public key to verify the signature and make sure that this image is signed or not via cosign but I do not know about the keyless mode, maybe we can accept transparency log entry id from user too. Within this project, we are accepting public keys from the users via flag to check whether the image is signed or not. For scorecard to check if a given GitHub release has been signed using cosign, how can getting public key help? Can you explain further? Also, scorecard runs in a cron mode where it checks against s 200,000 repositories gets results. There isn't an interactive mode. I think it is possible to do it.   Thoughts on this? On a related note  I think it is possible to do it.  Thoughts on this?  Confirmed it is possible on slack   I have cycles to take this and would like to implement this. Please let me know. Thanks Sounds good to me. We already look for the presence of   files in release artifacts, if we don't find anything there, we can look into Rekor for it. This will be a good first step for Scorecard to start integrating with Rekor. Thanks for looking into this  !"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/287",
				"issue_name": [
					"Feature - Add metadata for Envoy dependencies to scorecard"
				],
				"issue_label": [
					"enhancement"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nNow that scorecard is able to scan 2000+ repositories https://storage.googleapis.com/ossf-scorecards/latest.json it would help to add metadata to these scans easily identify the envoy's dependencies using the --metadata feature in the scorecard command line.\nDescribe the solution you'd like\n\nIdentify the dependencies for the envoy.\nProbably write a tool to update the ./cron/projects.txt with some like project=envoy,dependency=true to the envoy's dependencies\nThe latest.json will contain this additional metadata which can help identify scorecard for all of envoy dependencies.\n\n      ",
				"issue_comment": "\nNow that scorecard is able to scan 2000+ repositories   it would help to add metadata to these scans easily identify the envoy's dependencies using the   feature in the scorecard command line. \n \n \n \n  We are trying to scan all of Envoy's dependencies and add metadata to the results using the   which can be passed in the command line. Where can we look for   dependencies? cc     - can you help adding those in  Where can we look for envoy dependencies? Envoy's deps are specified in the maps in   (and it's API deps in  ). What happened to this PR btw?   I don't see it reflected in main. Sure, should I manually add them the projects.txt, or are you looking for a script to automatically get them? Where can we look for envoy dependencies? Envoy's deps are specified in the maps in   (and it's API deps in  ). What happened to this PR btw?   I don't see it reflected in main. Sorry, they got blown off by the other commit. Sure, should I manually add them the projects.txt, or are you looking for a script to automatically get them? Would appreciate a script, so that we can run them on schedule. Sorry, last follow-up question. Would a python script be OK? I can hack one together fairly easily, we already have the tools to do so when we manually get scorecards for each of our deps \n Python should be good. I would prefer go , the rest of the code base in go. I am so sorry, is it possible to do it go? I can help if you need some. I'll give it a first stab and put out a PR, would appreciate review help! \nI think the earliest I can get it out will be Monday if that's OK I'll give it a first stab and put out a PR, would appreciate review help! \nI think the earliest I can get it out will be Monday if that's OK Thank you! That works. I think if the script is living in OSSF tooling it makes sense for it to be Go, if it lives in Envoy repo, best that it be Python, since most of our other dep tooling is Python based and the dependency shepherds are working with that. Static list is fine for now, since later we will have some dependency calculation logic project in near future. Added a static list for now! Will work on automating soon. Thanks"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/283",
				"issue_name": [
					"Feature - Implement Ratelimit check for GitHub GraphQL"
				],
				"issue_label": [
					"bug",
					"GitHub"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nThe present  rate limit check does not check for GraphQL API Limits\nDescribe the solution you'd like\nImplement a Ratelimit check for GraphQL API https://docs.github.com/en/rest/reference/rate-limit\n      ",
				"issue_comment": "\nThe present  rate limit check does not check for GraphQL API Limits \nImplement a Ratelimit check for GraphQL API "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/271",
				"issue_name": [
					"Feature Scorecard badges"
				],
				"issue_label": [
					"core feature"
				],
				"issue_content": "\n          Is your feature request related to a problem? Please describe.\nScorecard should provide a badge for repositories to include in their README to display their compliance.\nScorecard badges\nThe scorecard should provide badges similar to other https://github.com/badges/shields OSS badges for compliance.\nGoals\n\nScorecard should provide badge results based on scorecard runs on the (cron)server to ensure compliance and validation. Similar to Codecov.\nScorecard should provide a predictable API to fetch the badges. An example could be https://somefqdn/github/ossf/scorecard/badge\nScorecard results calculation - TBD - The discussion of calculation should be a separate issue.\n\nImplementaion\n\nScorecard would create a separate HTTP application (within the scorecard repository) which would generate the badge.\nScorecard would use the Results from the scheduled cron run to generate the badge. The results of the cron are stored within the GCS bucket as latest.json\nThe HTTP application would be stateless.\nThe application would be hosted on Google Cloud Run which will scalability with less maintenance.\n\n\nIn this example\n\nThe k8s repository README.md would request the scorecard badge service for the badge\nThe badge service would fetch the results from the GC bucket which has the latest.json results from the cron job\nThe badge service calculates the score and returns the SVG.\n\n      ",
				"issue_comment": "\nScorecard should provide a badge for repositories to include in their README to display their compliance. Scorecard badges The scorecard should provide badges similar to other   OSS badges for compliance. Goals \n \n \n \n Implementaion \n \n \n \n \n In this example \n \n \n \n    What are your thoughts on this? Is there going to be a single badge/score, or one for each check? I would think of a single badge with different levels like CII  sounds like a good idea.    would be great, but I'm not 100% sure they directly map to the scorecard checks. sounds like a good idea.   would be great, but I'm not 100% sure they directly map to the scorecard checks. That is even better IMO. We have to just agree on what check constitutes to the levels.  is driving remediation efforts and working on the definition of 'tiers', aka levels for oss.  , feel free to provide some insights/feedback. SLSA is wip, and only a set of checks will apply to SLSA not all. So, this needs more thought. I like the idea of a single badge with different levels expressed by the number of checks passed. We'll need to list the applicable checks, some of which could come from SLSA requirements but maybe not the levels themselves. I like the idea of a single badge with different levels expressed by the number of checks passed. We'll need to list the applicable checks, some of which could come from SLSA requirements but maybe not the levels themselves. I disagree with having checks passed as a number in the badge. It could be sending a wrong message to the consumer of any of the repositories/packages. Also, it would discourage from repositories adopting the badge. For example,   has  of 16 checks only 9 passes. So if I as a user would like to consume the   package I would be cautious because the percentage of failure is more than 40%. But in reality, some of the checks aren't applicable to   like signed-releases, SAST, Automatic-Dependency-Update. Having a tiered approach to badges would be helpful similar to CII/SLSA. Thoughts? I also prefer qualitative information. In addition to what Naveen said, it empowers consumers of the dependency to make informed decisions about the risks they are comfortable taking. Some users may weight different checks differently so it's hard for us (scorecard) to make that decision on behalf of everyone. Let's discuss this more in an upcoming meeting. Agree that this needs more discussion. We'll need to find a balance across badge adoption, check applicability, and giving package users easily-digestible information to make a risk-based decision. Note that I wasn't proposing a percentage as not all checks will be applicable to all packages, so the denominator could vary from package to package. This is now on our roadmap.     could one of you update this issue with a high-level overview of the design?  fyi."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/206",
				"issue_name": [
					"Implement abuse detection in GitHub API requests"
				],
				"issue_label": [
					"bug",
					"GitHub"
				],
				"issue_content": "\n          The GitHub API would return 403 if there are too many requests for example cron running for a bunch of repositories\n{\n      \"CheckName\": \"Signed-Releases\",\n      \"Pass\": false,\n      \"Confidence\": 0,\n      \"Details\": [\n        \"error, retrying: GET https://api.github.com/repos/activescaffold/active_scaffold/releases: 403 You have triggered an abuse detection mechanism. Please wait a few minutes before you try again. []\"\n      ]\n    },\n    {\n      \"CheckName\": \"Signed-Tags\",\n      \"Pass\": false,\n      \"Confidence\": 0,\n      \"Details\": [\n        \"error, retrying: non-200 OK status code: 403 Forbidden body: \\\"{\\\\n  \\\\\\\"documentation_url\\\\\\\": \\\\\\\"https://docs.github.com/en/free-pro-team@latest/rest/overview/resources-in-the-rest-api#abuse-rate-limits\\\\\\\",\\\\n  \\\\\\\"message\\\\\\\": \\\\\\\"You have triggered an abuse detection mechanism. Please wait a few minutes before you try again.\\\\\\\"\\\\n}\\\\n\\\"\"\n      ]\n    }\n\nProbably implement this solution\ngoogle/go-github#431 (comment)\nhttps://github.com/hashicorp/terraform-provider-github/blob/fa73654b66e37b1fd8d886141d9c2974e24ba42f/github/transport.go#L42-L109 and it also has tests\n      ",
				"issue_comment": "The GitHub API would return   if there are too many requests for example   running for a bunch of  Probably implement this solution \n \n  and it also has "
			},
			{
				"issue_serial": "/ossf/scorecard/issues/200",
				"issue_name": [
					"New check: Include info about the ratio of memory-unsafe to memory-safe code"
				],
				"issue_label": [
					"core feature",
					"priority"
				],
				"issue_content": "\n          Something like 2/3rds of vulnerabilities in common software are due to memory unsafety. (See e.g. https://alexgaynor.net/2020/may/27/science-on-memory-unsafety-and-security/)\nIt'd be cool if Scorecard would score dependencies on how much safe/unsafe code they have.\nFor example, https://github.com/rust-secure-code/cargo-geiger does this for Rust.\n      ",
				"issue_comment": "Something like 2/3rds of vulnerabilities in common software are due to memory unsafety. (See e.g.  ) It'd be cool if Scorecard would score dependencies on how much safe/unsafe code they have. For example,   does this for Rust. Maybe also need to add [can open a new bug if needed], how much unsafe code is called from memory safe code. E.g. native c extensions from python, JNI from java, unsafe rust, cgo in golang [came from   discussion] The current thinking with   is that this should be implemented there instead. Perhaps then scorecards can then read from that results of that. The current thinking with   is that this should be implemented there instead. Perhaps then scorecards can then read from that results of that. I concur with that."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/95",
				"issue_name": [
					"Show negative results in details"
				],
				"issue_label": [
					"enhancement",
					"help wanted"
				],
				"issue_content": "\n          Howdy!  New here.  👋 🤠\nWhile using scorecard to evaluate projects I contribute to, I found myself having to read (and patch) the checks code to understand why I was seeing failing or marginal results.  It would be very helpful to see negative results in the --show-details output instead of only positive results.  It makes it easier for newcomers like me to learn what is considered bad practice, and I assume it would be help everyone to investigate any negative findings.\nFor example, I've patched scorecard locally to give negative results (starting with !!) for some checks:\n$ ./scorecard --repo=github.com/gohugoio/hugo --show-details --checks=Code-Review,CI-Tests,Pull-Requests,Signed-Releases,Signed-Tags\nRESULTS\n-------\nCI-Tests: Fail 4\n    !! found committed PR without CI test: 8075\n    !! found committed PR without CI test: 8070\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/1056701da088d5e87e6e31cdc6e0c455862697cc\n    !! found committed PR without CI test: 8059\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/34ecd28779d4836c74d4e71a5227b196d5d2cbec\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/713792077b06504e9fdd0c8abdc2aebace5dcf2a\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/3a38df2c4f96e560ddedc5fd5266fb54972a7fb6\n    !! found committed PR without CI test: 8020\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/7475bb324f208c8acd5fb3b6aa0f1b07c8d26d94\n    CI test found: context: continuous-integration/travis-ci/push, url: https://api.github.com/repos/gohugoio/hugo/statuses/cd5a53bb240ef23977c03d2a47cd76f8ee915899\n    !! found committed PR without CI test: 7999\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/15ee0346fc60265c36530dcc3b70be3955f006ef\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/bb228c9faf6a44fe2bb60d1d3d6f5ec538a7c786\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/f2b60ecc084b1142c9cf73f7e3736a754067943f\n    CI test found: context: continuous-integration/travis-ci/pr, url: https://api.github.com/repos/gohugoio/hugo/statuses/3a89789e40cc093f36cb9c03814f8dc30a9b1921\nCode-Review: Pass 8\n    found PR with committer different than author: 8075\n    found PR with committer different than author: 8070\n    !! found unreviewed PR committed by author: 8065\n    !! found unreviewed PR committed by author: 8059\n    found review approved PR: 8042\n    found PR with committer different than author: 8035\n    found PR with committer different than author: 8034\n    found PR with committer different than author: 8020\n    found review approved PR: 8008\n    found PR with committer different than author: 8004\n    found PR with committer different than author: 7999\n    found PR with committer different than author: 7998\n    !! found unreviewed PR committed by author: 7991\n    found PR with committer different than author: 7989\n    found PR with committer different than author: 7988\n    github code reviews found for 12 of 15 merged PRs\nPull-Requests: Fail 5\n    !! found commit without PR: 6c294182788f7da358243c4a0ef0a98772491067\n    !! found commit without PR: 10ae7c3210cd1add14d3750aa9512a87df0e1146\n    !! found commit without PR: a2d146ec32a26ccca9ffa68d3c840ec5b08cca96\n    !! found commit without PR: 21fa1e86f2aa929fb0983a0cc3dc4e271ea1cc54\n    !! found commit without PR: c84ad8db821c10225c0e603c6ec920c67b6ce36f\n    !! found commit without PR: 718e09ed4bc538f4fccc4337f99e9eb86aea31f3\n    !! found commit without PR: 32d4bf68da7d16302f138dde343c70f9667933c4\n    !! found commit without PR: 1415efdcd838cf482072ef08e765a8ce960bfdde\n    !! found commit without PR: 4e6bf7907dc5dfde697ace251b91a9399e0e3c39\n    !! found commit without PR: 50be4370b0c46c6c34430eb45bdc53d1926dd800\n    !! found commit without PR: 3d2e6a30d43079d48eb241505d5e0d9628dedf15\n    !! found commit without PR: 4f1e4bb3fe8241d7a900f57e156f9679768aff24\n    !! found commit without PR: 9f1265fde4b9ef186148337c99f08601633b6056\n    !! found commit without PR: d162bbd7990b6a523bdadcd10bf60fcb43ecf270\n    found PRs for 16 out of 30 commits\nSigned-Releases: Fail 10\n    release found: v0.79.1\n    !! release v0.79.1 has no signed artifacts\n    release found: v0.79.0\n    !! release v0.79.0 has no signed artifacts\n    release found: v0.78.2\n    !! release v0.78.2 has no signed artifacts\n    release found: v0.78.1\n    !! release v0.78.1 has no signed artifacts\n    release found: v0.78.0\n    !! release v0.78.0 has no signed artifacts\n    release found: v0.77.0\n    !! release v0.77.0 has no signed artifacts\nSigned-Tags: Fail 10\n    !! unsigned tag found: v0.78.0, commit: 2f1a31211c08f7fd52738ba2f817055a7fab9373\n    !! unsigned tag found: v0.78.1, commit: 0cb2fd5cc8eca7cd6af59e246492f1587b69819b\n    !! unsigned tag found: v0.78.2, commit: 21103fa0df2b9d23e5792ac4f27def0453118f7e\n    !! unsigned tag found: v0.79.0, commit: 626facbfa32823bc2d1152f97e6db67ed051a307\n    !! unsigned tag found: v0.79.1, commit: ea1d515f9750581769311d95064f52165c89edd0\n\nI can submit a PR for the above if this idea is acceptable.\n      ",
				"issue_comment": "Howdy!  New here.     While using scorecard to evaluate projects I contribute to, I found myself having to read (and patch) the checks code to understand why I was seeing failing or marginal results.  It would be very helpful to see negative results in the   output instead of only positive results.  It makes it easier for newcomers like me to learn what is considered bad practice, and I assume it would be help everyone to investigate any negative findings. For example, I've patched scorecard locally to give negative results (starting with  ) for some checks: I can submit a PR for the above if this idea is acceptable. Love the idea, can you submit a PR for all checks! , \nThe only check I was planning to update is Code-Review, but it will require refactoring the multichecks to avoid lots of false-negative logging while trying to determine what kind of review workflow is in use (similar to  ). Can you try running the current version of scorecard? We've tried to generally address this. \nLooking forward to your feedback!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/86",
				"issue_name": [
					"Make scorecard highly visible"
				],
				"issue_label": [
					"core feature",
					"priority"
				],
				"issue_content": "\n          @kimsterv has several interesting ideas here.\n      ",
				"issue_comment": " has several interesting ideas here. E.g. become part of security page on Github \n \netc"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/78",
				"issue_name": [
					"Scorecard should provide details on fuzzing coverage in fuzzing check"
				],
				"issue_label": [
					"enhancement",
					"needs discussion"
				],
				"issue_content": "\n          @htuch's suggestion.\n      ",
				"issue_comment": "'s suggestion. I think it would be good to understand fuzzer health overall. So, coverage is a big part, but so is performance, whether issues get fixed, other efficacy metrics that OSS-fuzz emits. If there was a way to capture this in simple A/B/C grades or the like it would be very useful. CC   also mentioned this idea, adding him as fyi  does oss-fuzz provide those details? They're up on GCS for each project's latest coverage report e.g. \n I'm not sure that's the easiest way to get them They're up on GCS for each project's latest coverage report e.g. \n I'm not sure that's the easiest way to get them This a good start. Thanks. I will work on updating the fuzzing with that information. They're up on GCS for each project's latest coverage report e.g. \n I'm not sure that's the easiest way to get them Is there a json result? I don't have permission to access the dir. We can use the HTTP endpoint to query the results and update the fuzzing results.   /    Ping.  can help on providing the json endpoint/gcs info here. Sure! So to get the coverage for a OSS-Fuzz project, you need to read 2 JSON files. First from e.g.  Then get the   from that. At time of writing this is   which can be converted to a HTTP link:  That JSON will have a   field which gives coverage summaries. The best one to use will probably be the most fine grained one (\"regions\"). The fuzzing results provide all these details. Questions \n \n \n We could extend the results by changing   to  . \nThis will maintain backward compatibility on the   but I think it will break the BigQuery. \nThis approach can also be used for the other checks if scorecard would like to extend the additional details. cc  Totals -> Regions -> Percent should be good enough for now (high-level % of code covered). Additional details we can later add once we have some design on the Details string as you said. Right now, too many other things to worry about, but can come back in a month to create maybe some structure on Details field ?   thoughts? +1 to adding this to Details []string for now. I think this is another of those things which will benefit from implementing  . We should probably consider updating how Details []string looks like after few weeks. But for now, there are too many moving parts and I don't want to add one more. We could wait for the   results to be finalized. So that when we implement this without   results. If we implement with   results and then move it when we finalize the design for   It will break the functionality.   Is this an issue that can wait ?  are there teams waiting for this to be addressed? +1 to waiting if this is not a blocker.  this would be a good feature to provide more details for package consumers. Can the result interface accommodate some of these changes? Possible ideas: \n \n \n \n (1) is the simplest to start with, and can be improved later to support a better solution."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/74",
				"issue_name": [
					"The score card should indicate when checks don't support a particular language/ecosystem"
				],
				"issue_label": [
					"bug",
					"priority"
				],
				"issue_content": "\n          \nThe scorecard can be populated for any open source project without any work or interaction from maintainers.\n\n\nMaintainers must be provided with a mechanism to correct any automated scorecard findings they feel were made in error, provide \"hints\" for anything we can't detect automatically, and even dispute the applicability of a given scorecard finding for that repository.\n\nThere are some checks, like fuzzing, that are very specific to particular languages.\nI think any score card should make clear when a particular check doesn't natively support a language or ecosystem.\nAs someone who is a consumer, and a maintainer, it would be good to know that a project didn't really score zero on something. It's just that the check doesn't support that language/ecosystem vs the maintainer has gone to the effort of somehow flagging that they do something the automated checks can't pick up.\n      ",
				"issue_comment": "The scorecard can be populated for any open source project without any work or interaction from maintainers. Maintainers must be provided with a mechanism to correct any automated scorecard findings they feel were made in error, provide \"hints\" for anything we can't detect automatically, and even dispute the applicability of a given scorecard finding for that repository. There are some checks, like fuzzing, that are very specific to particular languages. I think any score card should make clear when a particular check doesn't natively support a language or ecosystem. As someone who is a consumer, and a maintainer, it would be good to know that a project didn't   score zero on something. It's just that the check doesn't support that language/ecosystem vs the maintainer has gone to the effort of somehow flagging that they do something the automated checks can't pick up. Thanks for the report! I think this is essentially what I was trying to capture with the \"Confidence\" field in the result - if we can't actually tell or if a check doesn't apply we return a low confidence score. That should be used to indicate the result should not be relied on. Does that work, or do you think we need more info in the result? Are you only concerned about fuzzing or can other checks apply to only specific languages ? Note that fuzzing stuff is expanding to more languages, e.g. C/C++, Rust, Swift, Golang, and Python (coming soonish with   release) Not fuzzing specifically. It is just a check I was using as an example. The confidence metric probably does cover this. I just think it should be obvious that the project got a zero for something because the checks don't support the language vs they do support the language and it failed the check."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/62",
				"issue_name": [
					"Question RE value of Contributors Check"
				],
				"issue_label": [
					"bug"
				],
				"issue_content": "\n          \"Does the project have contributors from at least two different organizations?\"\nWhile I fully endorse this from a governance perspective, I fail to see what it has to do with the security of a project specifically.\n      ",
				"issue_comment": "\"Does the project have contributors from at least two different organizations?\" While I fully endorse this from a governance perspective, I fail to see what it has to do with the security of a project specifically. I think having at least two maintainers is more important than having two contributors. I just fail to see what this has to do with cybersecurity specifically. Picking a company name entirely randomly - If I compare a Microsoft-sponsored and highly-funded project on Github, to another random project that is built by 4 or 5 individuals in their free time with no involvement from any organization, why would one assume the second project is more secure? I am not saying it would not be, maybe it is, but there is really no actual information to go by either way. I just don't see what this has to do with cybersecurity at all. It is about trust, would you not put trust in this order when uptaking a new dependency. \nrandom developer project < project maintained by one org < project maintained/contributed by 2+ orgs. Also, we are open to your ideas. Given a developer account XYZ, how can you say this developer can be trusted, any thoughts there, maybe that can be a better check than this. IMHO this is not an indicator of trust either way. Again back to my previous example... why should I trust [random open source developed by 2 or 3 people I don't know], vs [project with a dozen contributors sponsored by giant major well-known org] ? Again I am   saying single-sourced projects by large orgs like Google/Microsoft/etc are more trustworthy, but they certainly aren't   trustworthy. I just don't see what the number of organizations has to do with this. I believe we are mixing up the concepts of open source governance, with trust & cybersecurity. Governance is vital! But it doesn't really have anything to do with trust IMHO. There are lots of well-governed projects in LF, that likely have poor cybersecurity, and there are lots of single-source projects that likely have excellent cybersecurity. I doubt there would be any  kind of positive correlation here at all.  suggestion: \"\"\" \nBasically, we want to know that there are >= 2 contributors who are responsible for a large fraction of the PRs. It's not sufficient to just know that there are more than 1 committers, I basically look back through GH contributor stats or history to ensure that there is at least 1 other contributor who is doing a reasonable amount, e.g. > 10%, of commits. \n\"\"\" I think our concern (based on experience) can be articulated as having a bus factor of 1 leaves you at heightened risk when it comes to decisions around vulnerability disclosure process and execution. E.g. let's say there is a zero day and the solo maintainer has gone AWOL, what can be done? I think there is a strong relationship between effective governance and effective security policy implementation. There's definitely room to debate how this \"bus factor > 1\" or \"project governance meets a litmus test\" policy can be implemented, and this thread brings up a number of good points and counterexamples. I'd just be weary about being completely agnostic to the number and structure of contributors/maintainers. The risk is more than just vulnerability disclosure process and execution. It could be as simple as you choosing a dependency that ends up abandoned. Then you end up needing to upgrade to a newer version of a language/runtime/framework/whatever, and realise you can't because an individual OSS maintainer has burnt out and hasn't continued maintaining it. But the issue was raised around the wording \"Does the project have contributors from at least two different organizations?\". So are we talking about a maintainer bus factor, or an organisation bus factor?  Agree, that's my point. This check is not checking maintainers, it is checking organizations. Bus factor isn't what's being evaluated. A repo from [large company] with 20 maintainers and 100 contributors gets a lower score than a repo with 2 hobbiest maintainers / contributors. To me it makes no sense from a security perspective.  Odds of the repo getting abandoned (or having pretty much any kind of security issue) are far higher with the latter. EDIT: Actually the code currently doesn't account for \"null\" companies either. So unaffiliated individuals don't even count for the check (IE this check will only pass if at least two \"companies\" are sponsoring a project... volunteer or unaffiliated projects will never pass it....) The more I look at this check the more I disagree with how it works."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/58",
				"issue_name": [
					"Hook Scorecard as a CI/CD solution, prevent bad deps from being added in code"
				],
				"issue_label": [
					"core feature",
					"needs discussion"
				],
				"issue_content": "\n          \n            No description provided.\n          \n      ",
				"issue_comment": "Great idea! I think we'd have to create a separate hook per language-package-manager that can parse out the dependency files and then call the scorecard tool. It's probably going to be easier to do those hooks in whatever language we're handling. Do we need a separate repo for each of those, or just one to hold them all? Great idea! I think we'd have to create a separate hook per language-package-manager that can parse out the dependency files and then call the scorecard tool. It's probably going to be easier to do those hooks in whatever language we're handling. Do we need a separate repo for each of those, or just one to hold them all? Probably one to hold them all. By default, we run all the hooks in the CI run. If it does not find package lock file for a particular pkg manager, it bails out. Different hooks can be different jobs in the CI run. we need to make atleast one check run in cicd pipeline. I think I'd like to start work on this issue. Here are some of my thoughts: \n \n \n \n \n \n \n \n \n \n \n Thoughts on these points? Bullets 2 - 4 all make sense to me! For bullet 1, maybe it can be built in such a way to be pluggable so if there are other projects that parse dependency files, users would have options? I do like the idea of a pluggable architecture. I think it would be good for our project to compile in parsers for at least the popular dependency files. That way the user can download a single binary, and it will work out of the box. We can then have an option for the user to configure their own parser. I cloned one of the Snyk repos I mentioned, and I don't think that will be useful here. They are interested in constructing a graph of all dependencies which is not exactly useful to us. I did find another project that looks more promising:  . This can parse dependencies for go.mod, npm, pipenv, composer, and more. Since it's written in Go, we could could easily incorporate it. There are some dependency files that include include the git repo such as composer.lock. For these ones, we might want to write our own parser so it can read the repo directly and not have to look it up with a web call. Let's discuss this in our upcoming bi-weekly. did we forget to discuss this in the last meeting? \nbtw, there's also Google's   which offers a REST API and would do all the heavy lifting for us. did we forget to discuss this in the last meeting? I saw it wasn't on the agenda, but I didn't bring it up because I'll be working on another issue first. Plus I'll be pretty busy the next few weeks getting our house ready to list. btw, there's also Google's   which offers a REST API and would do all the heavy lifting for us. I'll definitely give that look!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/40",
				"issue_name": [
					"Expand to source repos outside of GitHub"
				],
				"issue_label": [
					"core feature",
					"priority"
				],
				"issue_content": "\n          I suggest GitLab next.\n      ",
				"issue_comment": "I suggest GitLab next. For Google, a lot of repos live on googlesource.com, e.g. skia.googlesource.com,   and they clone to github as well. Several results fail for them as they dont follow github processes like codereview, pull requests, etc. Is there any progress regarding this issue ? It'd be great to support GitLab as well. Is there any progress regarding this issue ? It'd be great to support GitLab as well. For the upcoming release, it isn't. PR's are welcome  - Scorecard now uses an   for running its checks. So to add support for other VCS simply requires implementing this interface ( ). Like   has already mentioned, this isn't on our milestones right now since we do not have the resources to tackle this. But PRs are always welcome and appreciated :)"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/30",
				"issue_name": [
					"New check: code is scanning for secrets"
				],
				"issue_label": [
					"core feature",
					"priority",
					"wishlist"
				],
				"issue_content": "\n          \n            No description provided.\n          \n      ",
				"issue_comment": "A check that something like trufflehog (or other secret scanners) are running would be nice: \n \n \n Some examples from Google VRP program, see here -  Some examples from Google VRP program, see here -  Don't have access to the doc. We may check for the presence of the .gitignore file and check sensitive files like private keys formats and other are listed. \nBesides password/private key files, we can also add  Note that   is enabled by default for public repos."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/28",
				"issue_name": [
					"New check: detect change of ownership"
				],
				"issue_label": [
					"core feature",
					"needs discussion",
					"priority",
					"wishlist"
				],
				"issue_content": "\n          Return true if there was a chance, false otherwise.\n      ",
				"issue_comment": "Return true if there was a chance, false otherwise. I'm interested in taking this one on. Before I start, I'd like to discuss how to go about doing this. I don't see anything on Github's API that would allow us to query for a repo's past owners. I think we'll need to store this data ourselves and have scorecard query for it. We could record the owners for the repos we track as part of the cron process and store it in GCS bucket. Then when scorecard runs, it could download the file(s) it needs from the bucket. Maybe not at first, but we will want to shard the data across files within the bucket so that scorecard can download a reasonable size chunk if it's only checking one repo. If the repo being checked is not present in the saved data, we'd have to give an inconclusive result. Feedback on this approach? I'm interested in taking this one on. Before I start, I'd like to discuss how to go about doing this. I don't see anything on Github's API that would allow us to query for a repo's past owners. I think we'll need to store this data ourselves and have scorecard query for it. We could record the owners for the repos we track as part of the cron process and store it in GCS bucket. Then when scorecard runs, it could download the file(s) it needs from the bucket. Maybe not at first, but we will want to shard the data across files within the bucket so that scorecard can download a reasonable size chunk if it's only checking one repo. If the repo being checked is not present in the saved data, we'd have to give an inconclusive result. Feedback on this approach? This is a good approach. But we are in the process of designing sharding and coming up with long-term plans to scale.  I would recommend that we wait until then. Thanks for picking up the issue and all your contributions  ! Just to clarify, are you saying I can start working on this issue now, but I should not worry about sharding at this time? Just to clarify, are you saying I can start working on this issue now, but I should not worry about sharding at this time? Not worry about right now. There are several other to pick from list -   or  \nbut whichever you pick, leave a comment in bug to avoid duplication/conflict. thanks   for your contributions!"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/27",
				"issue_name": [
					"Update dependency score based on project criticality (recursively)"
				],
				"issue_label": [
					"core feature",
					"priority",
					"wishlist"
				],
				"issue_content": "\n          If project foo depends on projects A, B and C, then projects A, B, C should have a score greater than Y\n      ",
				"issue_comment": "If project foo depends on projects A, B and C, then projects A, B, C should have a score greater than Y We could use   project and let user decide what value Y should be."
			},
			{
				"issue_serial": "/ossf/scorecard/issues/10",
				"issue_name": [
					"New check: Does the project use protected tags?, blocked on GitHub feature implementation."
				],
				"issue_label": [
					"wishlist"
				],
				"issue_content": "\n          https://github.community/t/feature-request-protected-tags/1742/21\n      ",
				"issue_comment": ""
			},
			{
				"issue_serial": "/ossf/scorecard/issues/7",
				"issue_name": [
					"New check: Do contributors and maintainers have 2fa enabled?"
				],
				"issue_label": [
					"wishlist"
				],
				"issue_content": "\n          Do all contributors have 2 factor authentication enabled on their accounts?\n      ",
				"issue_comment": "Do all contributors have 2 factor authentication enabled on their accounts? I think the more practical would be \"Do top 5/10contributors have 2FA enabled\"? I think the more practical would be \"Do top 5/10contributors have 2FA enabled\"? probably depends on how many contributors the project has? \nWe could also check that reviewers have 2FA. A compromised contributor may try to push bad code but would be detected by reviewers. Having a score instead of a binary pass/fail check would also help in general. It would encourage developers to make incremental improvements towards a long-term goal. In terms of scoring, it could be: \n \n \n Not sure this is possible with a reasonable number of API calls, though. We cannot go back in history too much either, so this may be done over the last n months, for example. API for org  . May require special permission to use. API for org  . May require special permission to use. This API allows querying for users in your organization that do not have 2FA enabled. It wouldn't fit our needs of being able to tell if an arbitrary user has 2FA enabled. I would be surprised if Github ever published an API allowing querying whether arbitrary users have 2FA enabled. I think a lot of people wouldn't be comfortable with it being public knowledge that they don't use 2FA. We might not ever be able to do this issue. if we could query the list of maintainers for a repo/org, we could then cross-check which maintainers don't have 2FA enabled. I've not looked closely at the API, though. We don't really need all users, we mostly need maintainers of the project, or devs that can reviews PRs, etc A side note - might be a good fit for AllStar i.e an AllStar policy can check all org members have 2fa enabled and bring this to the org's attention (in a private manner).   fyi. if we could query the list of maintainers for a repo/org, we could then cross-check which maintainers don't have 2FA enabled. It looks like this API is only available if you are the owner of that organization, so I don't think it would be useful in Scorecard. It might work for AllStar; that's a good idea, Azeem. ok, yes allstar works well too. I'm adding these issues in scorecard repo first and allstar is the fallback solution in my mind, since allstar is for orgs but scorecard can be more broadly used. Here's another one that I would like to have in scorecard, but may only be viable for allstar   :/"
			},
			{
				"issue_serial": "/ossf/scorecard/issues/5",
				"issue_name": [
					"New check: Uses source version control"
				],
				"issue_label": [
					"wishlist"
				],
				"issue_content": "\n          Some OSS projects don't use version control. Detect if a version control system is used for the project.\n      ",
				"issue_comment": "Some OSS projects don't use version control. Detect if a version control system is used for the project."
			}
		]
	},
	"liveness": {
		"num_of_star": 2500,
		"num_of_fork": 229,
		"num_of_watch": 2512
	},
	"pull_request": {
		"num_of_pr": "12",
		"pr": [
			{
				"serial_of_pr": "/ossf/scorecard/pull/1830",
				"name_of_pr": " Add e2e tests using GITHUB_TOKEN",
				"label_of_pr": "null",
				"content_of_pr": "@ n a v e e n s r i n i v a s a n   c a n   y o u   t a k e   a   l o o k ?",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1830/commits/344587f3766f7cd3dfdd1c35b715aedb8c43808b",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1816",
				"name_of_pr": " Return inconclusive result if no fuzzing is detected",
				"label_of_pr": "null",
				"content_of_pr": "null",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1816/commits/6b519296528363bec457987c3ce62f6f7186fc42",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1814",
				"name_of_pr": " Fix command Usage",
				"label_of_pr": "null",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 8 1 4   ( b 8 a 6 5 2 d )   i n t o   m a i n   ( 3 3 3 6 1 8 d )   w i l l   i n c r e a s e   c o v e r a g e   b y   3 . 0 6 % . \n T h e   d i f f   c o v e r a g e   i s   0 . 0 0 % . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 8 1 4             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 3 . 5 1 %       5 6 . 5 7 %       + 3 . 0 6 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     7 3               7 3                             \n     L i n e s                 6 6 9 2           6 6 9 2                             \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 5 8 1           3 7 8 6           + 2 0 5           \n +   M i s s e s               2 8 6 5           2 6 5 4           - 2 1 1           \n -   P a r t i a l s             2 4 6             2 5 2               + 6",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1814/commits/b8a652d853532ff166af84d8b9b2dfcbd5da687f",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1813",
				"name_of_pr": " Remove erroneous ref to CSV output",
				"label_of_pr": "null",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 8 1 3   ( d 6 f 4 a f d )   i n t o   m a i n   ( 3 3 3 6 1 8 d )   w i l l   i n c r e a s e   c o v e r a g e   b y   3 . 0 6 % . \n T h e   d i f f   c o v e r a g e   i s   n / a . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 8 1 3             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 3 . 5 1 %       5 6 . 5 7 %       + 3 . 0 6 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     7 3               7 3                             \n     L i n e s                 6 6 9 2           6 6 9 2                             \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 5 8 1           3 7 8 6           + 2 0 5           \n +   M i s s e s               2 8 6 5           2 6 5 4           - 2 1 1           \n -   P a r t i a l s             2 4 6             2 5 2               + 6",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1813/commits/d6f4afda1341c58f7bd8bc1b35be4b678a59fa93",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1810",
				"name_of_pr": " Support for detecting choco installer without required hash",
				"label_of_pr": "null",
				"content_of_pr": "I n t e g r a t i o n   t e s t s   s u c c e s s   f o r \n [ 0 f f 2 3 9 c ] \n ( h t t p s : / / g i t h u b . c o m / o s s f / s c o r e c a r d / a c t i o n s / r u n s / 2 0 8 2 5 9 9 3 5 1 )",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1810/commits/0ff239ce516f33f91e0ddfbd4fb78277813b396d",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1795",
				"name_of_pr": " Raw results for best practices badge",
				"label_of_pr": "null",
				"content_of_pr": "I n t e g r a t i o n   t e s t s   s u c c e s s   f o r \n [ 4 b 8 0 c 0 f ] \n ( h t t p s : / / g i t h u b . c o m / o s s f / s c o r e c a r d / a c t i o n s / r u n s / 2 0 6 1 2 1 7 1 9 5 )",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1795/commits/242d42a6a6119dd9113bceaa1e173f7b1099ec72",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1790",
				"name_of_pr": " Raw results for license",
				"label_of_pr": "null",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 7 9 0   ( 8 8 f 1 0 2 a )   i n t o   m a i n   ( f b 0 c 0 e 1 )   w i l l   i n c r e a s e   c o v e r a g e   b y   2 . 4 3 % . \n T h e   d i f f   c o v e r a g e   i s   1 6 . 6 6 % . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 7 9 0             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 3 . 5 1 %       5 5 . 9 4 %       + 2 . 4 3 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     7 3               7 5               + 2           \n     L i n e s                 6 6 9 2           6 7 3 3             + 4 1           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 5 8 1           3 7 6 7           + 1 8 6           \n +   M i s s e s               2 8 6 5           2 7 1 5           - 1 5 0           \n -   P a r t i a l s             2 4 6             2 5 1               + 5",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1790/commits/f7a2f8931b410c2a21e50b2c94b341c70f459053",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1762",
				"name_of_pr": " Schema for BQ table for raw results",
				"label_of_pr": "null",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 7 6 2   ( c 9 0 2 7 9 d )   i n t o   m a i n   ( 5 8 6 0 8 9 6 )   w i l l   i n c r e a s e   c o v e r a g e   b y   3 . 0 6 % . \n T h e   d i f f   c o v e r a g e   i s   n / a . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 7 6 2             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 3 . 5 3 %       5 6 . 5 9 %       + 3 . 0 6 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     7 3               7 3                             \n     L i n e s                 6 6 9 5           6 6 9 5                             \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 5 8 4           3 7 8 9           + 2 0 5           \n +   M i s s e s               2 8 6 5           2 6 5 4           - 2 1 1           \n -   P a r t i a l s             2 4 6             2 5 2               + 6",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1762/commits/deb187b42b6bde47c5d37d0468d8468438fc4245",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1750",
				"name_of_pr": " Validation for command-line flags",
				"label_of_pr": "null",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 7 5 0   ( d c f b 2 8 0 )   i n t o   m a i n   ( 1 0 d 4 6 d 5 )   w i l l   i n c r e a s e   c o v e r a g e   b y   3 . 2 9 % . \n T h e   d i f f   c o v e r a g e   i s   8 8 . 0 9 % . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 7 5 0             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 6 . 7 2 %       6 0 . 0 2 %       + 3 . 2 9 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     6 5               6 5                             \n     L i n e s                 6 3 7 8           6 4 1 6             + 3 8           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 6 1 8           3 8 5 1           + 2 3 3           \n +   M i s s e s               2 5 1 9           2 3 1 8           - 2 0 1           \n -   P a r t i a l s             2 4 1             2 4 7               + 6",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1750/commits/dcfb28029295ff8701c61a08ad8037a4210d7e76",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1749",
				"name_of_pr": " Auto merge dependabot for minor release",
				"label_of_pr": "no-pr-activity",
				"content_of_pr": "C o d e c o v   R e p o r t \n \n M e r g i n g   # 1 7 4 9   ( b c 6 c 2 d f )   i n t o   m a i n   ( d 5 8 9 3 c 2 )   w i l l   i n c r e a s e   c o v e r a g e   b y   3 . 0 6 % . \n T h e   d i f f   c o v e r a g e   i s   n / a . \n \n @ @                         C o v e r a g e   D i f f                           @ @ \n # #                           m a i n         # 1 7 4 9             + / -       # # \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   C o v e r a g e       5 6 . 6 2 %       5 9 . 6 9 %       + 3 . 0 6 %           \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n     F i l e s                     6 5               6 5                             \n     L i n e s                 6 3 8 8           6 3 8 8                             \n = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n +   H i t s                   3 6 1 7           3 8 1 3           + 1 9 6           \n +   M i s s e s               2 5 3 0           2 3 2 8           - 2 0 2           \n -   P a r t i a l s             2 4 1             2 4 7               + 6",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1749/commits/6b282b02d43afc7d180a5a3f935d4a3745a3e9e9",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1702",
				"name_of_pr": " SLSA provenance/build",
				"label_of_pr": "work-in-progress",
				"content_of_pr": "null",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1702/commits/3c48ea892bda8e7370343ce1544eb42f4b21cb7b",
				"commit_of_pr": "null"
			},
			{
				"serial_of_pr": "/ossf/scorecard/pull/1487",
				"name_of_pr": " Support more SAST tools",
				"label_of_pr": "work-in-progress",
				"content_of_pr": "null",
				"href_of_pr": "https://github.com//ossf/scorecard/pull/1487/commits/787204a15a512e0221f15ea0ef8a543c9782f86b",
				"commit_of_pr": "null"
			}
		]
	},
	"description": "exercism/Python is one of many programming language tracks on exercism(dot)org. This repo holds all the instructions, tests, code, & support files for Python exercises currently under development or implemented & available for students."
}